{
	"title": "Huggingface TextGen Inference | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/integrations/llms/huggingface_textgen_inference",
	"html": "ComponentsLLMsHuggingface TextGen Inference\nHuggingface TextGen Inference\n\nText Generation Inference is a Rust, Python and gRPC server for text generation inference. Used in production at HuggingFace to power LLMs api-inference widgets.\n\nThis notebooks goes over how to use a self hosted LLM using Text Generation Inference.\n\nTo use, you should have the text_generation python package installed.\n\n# !pip3 install text_generation\n\nfrom langchain.llms import HuggingFaceTextGenInference\n\nllm = HuggingFaceTextGenInference(\n    inference_server_url=\"http://localhost:8010/\",\n    max_new_tokens=512,\n    top_k=10,\n    top_p=0.95,\n    typical_p=0.95,\n    temperature=0.01,\n    repetition_penalty=1.03,\n)\nllm(\"What did foo say about bar?\")\n\nStreaming‚Äã\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\nfrom langchain.llms import HuggingFaceTextGenInference\n\nllm = HuggingFaceTextGenInference(\n    inference_server_url=\"http://localhost:8010/\",\n    max_new_tokens=512,\n    top_k=10,\n    top_p=0.95,\n    typical_p=0.95,\n    temperature=0.01,\n    repetition_penalty=1.03,\n    streaming=True,\n)\nllm(\"What did foo say about bar?\", callbacks=[StreamingStdOutCallbackHandler()])\n\nPrevious\nHugging Face Local Pipelines\nNext\nJavelin AI Gateway Tutorial"
}