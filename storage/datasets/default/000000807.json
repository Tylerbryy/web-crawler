{
	"title": "Pairwise embedding distance | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/guides/evaluation/comparison/pairwise_embedding_distance",
	"html": "EvaluationComparison EvaluatorsPairwise embedding distance\nPairwise embedding distance\n\nOne way to measure the similarity (or dissimilarity) between two predictions on a shared or similar input is to embed the predictions and compute a vector distance between the two embeddings.[1]\n\nYou can load the pairwise_embedding_distance evaluator to do this.\n\nNote: This returns a distance score, meaning that the lower the number, the more similar the outputs are, according to their embedded representation.\n\nCheck out the reference docs for the PairwiseEmbeddingDistanceEvalChain for more info.\n\nfrom langchain.evaluation import load_evaluator\n\nevaluator = load_evaluator(\"pairwise_embedding_distance\")\n\nevaluator.evaluate_string_pairs(\n    prediction=\"Seattle is hot in June\", prediction_b=\"Seattle is cool in June.\"\n)\n\n    {'score': 0.0966466944859925}\n\nevaluator.evaluate_string_pairs(\n    prediction=\"Seattle is warm in June\", prediction_b=\"Seattle is cool in June.\"\n)\n\n    {'score': 0.03761174337464557}\n\nSelect the Distance Metric‚Äã\n\nBy default, the evaluator uses cosine distance. You can choose a different distance metric if you'd like.\n\nfrom langchain.evaluation import EmbeddingDistance\n\nlist(EmbeddingDistance)\n\n    [<EmbeddingDistance.COSINE: 'cosine'>,\n     <EmbeddingDistance.EUCLIDEAN: 'euclidean'>,\n     <EmbeddingDistance.MANHATTAN: 'manhattan'>,\n     <EmbeddingDistance.CHEBYSHEV: 'chebyshev'>,\n     <EmbeddingDistance.HAMMING: 'hamming'>]\n\nevaluator = load_evaluator(\n    \"pairwise_embedding_distance\", distance_metric=EmbeddingDistance.EUCLIDEAN\n)\n\nSelect Embeddings to Use‚Äã\n\nThe constructor uses OpenAI embeddings by default, but you can configure this however you want. Below, use huggingface local embeddings\n\nfrom langchain.embeddings import HuggingFaceEmbeddings\n\nembedding_model = HuggingFaceEmbeddings()\nhf_evaluator = load_evaluator(\"pairwise_embedding_distance\", embeddings=embedding_model)\n\nhf_evaluator.evaluate_string_pairs(\n    prediction=\"Seattle is hot in June\", prediction_b=\"Seattle is cool in June.\"\n)\n\n    {'score': 0.5486443280477362}\n\nhf_evaluator.evaluate_string_pairs(\n    prediction=\"Seattle is warm in June\", prediction_b=\"Seattle is cool in June.\"\n)\n\n    {'score': 0.21018880025138598}\n\n1. Note: When it comes to semantic similarity, this often gives better results than older string distance metrics (such as those in the `PairwiseStringDistanceEvalChain`), though it tends to be less reliable than evaluators that use the LLM directly (such as the `PairwiseStringEvalChain`)\nPrevious\nPairwise string comparison\nNext\nCustom pairwise evaluator"
}