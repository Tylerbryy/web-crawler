{
	"title": "Pinecone Hybrid Search | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/integrations/retrievers/pinecone_hybrid_search",
	"html": "ComponentsRetrieversPinecone Hybrid Search\nPinecone Hybrid Search\n\nPinecone is a vector database with broad functionality.\n\nThis notebook goes over how to use a retriever that under the hood uses Pinecone and Hybrid Search.\n\nThe logic of this retriever is taken from this documentation\n\nTo use Pinecone, you must have an API key and an Environment. Here are the installation instructions.\n\n#!pip install pinecone-client pinecone-text\n\nimport getpass\nimport os\n\nos.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Pinecone API Key:\")\n\nfrom langchain.retrievers import PineconeHybridSearchRetriever\n\nos.environ[\"PINECONE_ENVIRONMENT\"] = getpass.getpass(\"Pinecone Environment:\")\n\n\nWe want to use OpenAIEmbeddings so we have to get the OpenAI API Key.\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n\nSetup Pinecone‚Äã\n\nYou should only have to do this part once.\n\nNote: it's important to make sure that the \"context\" field that holds the document text in the metadata is not indexed. Currently you need to specify explicitly the fields you do want to index. For more information checkout Pinecone's docs.\n\nimport os\n\nimport pinecone\n\napi_key = os.getenv(\"PINECONE_API_KEY\") or \"PINECONE_API_KEY\"\n# find environment next to your API key in the Pinecone console\nenv = os.getenv(\"PINECONE_ENVIRONMENT\") or \"PINECONE_ENVIRONMENT\"\n\nindex_name = \"langchain-pinecone-hybrid-search\"\n\npinecone.init(api_key=api_key, environment=env)\npinecone.whoami()\n\n    WhoAmIResponse(username='load', user_label='label', projectname='load-test')\n\n# create the index\npinecone.create_index(\n    name=index_name,\n    dimension=1536,  # dimensionality of dense model\n    metric=\"dotproduct\",  # sparse values supported only for dotproduct\n    pod_type=\"s1\",\n    metadata_config={\"indexed\": []},  # see explanation above\n)\n\n\nNow that its created, we can use it\n\nindex = pinecone.Index(index_name)\n\nGet embeddings and sparse encoders‚Äã\n\nEmbeddings are used for the dense vectors, tokenizer is used for the sparse vector\n\nfrom langchain.embeddings import OpenAIEmbeddings\n\nembeddings = OpenAIEmbeddings()\n\n\nTo encode the text to sparse values you can either choose SPLADE or BM25. For out of domain tasks we recommend using BM25.\n\nFor more information about the sparse encoders you can checkout pinecone-text library docs.\n\nfrom pinecone_text.sparse import BM25Encoder\n\n# or from pinecone_text.sparse import SpladeEncoder if you wish to work with SPLADE\n\n# use default tf-idf values\nbm25_encoder = BM25Encoder().default()\n\n\nThe above code is using default tfids values. It's highly recommended to fit the tf-idf values to your own corpus. You can do it as follow:\n\ncorpus = [\"foo\", \"bar\", \"world\", \"hello\"]\n\n# fit tf-idf values on your corpus\nbm25_encoder.fit(corpus)\n\n# store the values to a json file\nbm25_encoder.dump(\"bm25_values.json\")\n\n# load to your BM25Encoder object\nbm25_encoder = BM25Encoder().load(\"bm25_values.json\")\n\nLoad Retriever‚Äã\n\nWe can now construct the retriever!\n\nretriever = PineconeHybridSearchRetriever(\n    embeddings=embeddings, sparse_encoder=bm25_encoder, index=index\n)\n\nAdd texts (if necessary)‚Äã\n\nWe can optionally add texts to the retriever (if they aren't already in there)\n\nretriever.add_texts([\"foo\", \"bar\", \"world\", \"hello\"])\n\n    100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.27s/it]\n\nUse Retriever‚Äã\n\nWe can now use the retriever!\n\nresult = retriever.get_relevant_documents(\"foo\")\n\nresult[0]\n\n    Document(page_content='foo', metadata={})\n\nPrevious\nMetal\nNext\nPubMed"
}