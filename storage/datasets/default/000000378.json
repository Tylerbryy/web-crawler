{
	"title": "Fireworks | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/integrations/chat/fireworks",
	"html": "ComponentsChat modelsFireworks\nFireworks\n\nFireworks accelerates product development on generative AI by creating an innovative AI experiment and production platform.\n\nThis example goes over how to use LangChain to interact with ChatFireworks models.\n\nimport os\n\nfrom langchain.chat_models.fireworks import ChatFireworks\nfrom langchain.schema import HumanMessage, SystemMessage\n\nSetup\nMake sure the fireworks-ai package is installed in your environment.\nSign in to Fireworks AI for the an API Key to access our models, and make sure it is set as the FIREWORKS_API_KEY environment variable.\nSet up your model using a model id. If the model is not set, the default model is fireworks-llama-v2-7b-chat. See the full, most up-to-date model list on app.fireworks.ai.\nimport getpass\nimport os\n\nif \"FIREWORKS_API_KEY\" not in os.environ:\n    os.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass(\"Fireworks API Key:\")\n\n# Initialize a Fireworks chat model\nchat = ChatFireworks(model=\"accounts/fireworks/models/llama-v2-13b-chat\")\n\nCalling the Model Directly\n\nYou can call the model directly with a system and human message to get answers.\n\n# ChatFireworks Wrapper\nsystem_message = SystemMessage(content=\"You are to chat with the user.\")\nhuman_message = HumanMessage(content=\"Who are you?\")\n\nchat([system_message, human_message])\n\n    AIMessage(content=\"Hello! My name is LLaMA, I'm a large language model trained by a team of researcher at Meta AI. My primary function is to assist and converse with users like you, answering questions and engaging in discussion to the best of my ability. I'm here to help and provide information on a wide range of topics, so feel free to ask me anything!\", additional_kwargs={}, example=False)\n\n# Setting additional parameters: temperature, max_tokens, top_p\nchat = ChatFireworks(\n    model=\"accounts/fireworks/models/llama-v2-13b-chat\",\n    model_kwargs={\"temperature\": 1, \"max_tokens\": 20, \"top_p\": 1},\n)\nsystem_message = SystemMessage(content=\"You are to chat with the user.\")\nhuman_message = HumanMessage(content=\"How's the weather today?\")\nchat([system_message, human_message])\n\n    AIMessage(content=\"Oh hello there! *giggle* It's such a beautiful day today, isn\", additional_kwargs={}, example=False)\n\nSimple Chat Chain\n\nYou can use chat models on fireworks, with system prompts and memory.\n\nfrom langchain.chat_models import ChatFireworks\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain.schema.runnable import RunnablePassthrough\n\nllm = ChatFireworks(\n    model=\"accounts/fireworks/models/llama-v2-13b-chat\",\n    model_kwargs={\"temperature\": 0, \"max_tokens\": 64, \"top_p\": 1.0},\n)\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are a helpful chatbot that speaks like a pirate.\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{input}\"),\n    ]\n)\n\n\nInitially, there is no chat memory\n\nmemory = ConversationBufferMemory(return_messages=True)\nmemory.load_memory_variables({})\n\n    {'history': []}\n\n\nCreate a simple chain with memory\n\nchain = (\n    RunnablePassthrough.assign(\n        history=memory.load_memory_variables | (lambda x: x[\"history\"])\n    )\n    | prompt\n    | llm.bind(stop=[\"\\n\\n\"])\n)\n\n\nRun the chain with a simple question, expecting an answer aligned with the system message provided.\n\ninputs = {\"input\": \"hi im bob\"}\nresponse = chain.invoke(inputs)\nresponse\n\n    AIMessage(content=\"Ahoy there, me hearty! Yer a fine lookin' swashbuckler, I can see that! *adjusts eye patch* What be bringin' ye to these waters? Are ye here to plunder some booty or just to enjoy the sea breeze?\", additional_kwargs={}, example=False)\n\n\nSave the memory context, then read it back to inspect contents\n\nmemory.save_context(inputs, {\"output\": response.content})\nmemory.load_memory_variables({})\n\n    {'history': [HumanMessage(content='hi im bob', additional_kwargs={}, example=False),\n      AIMessage(content=\"Ahoy there, me hearty! Yer a fine lookin' swashbuckler, I can see that! *adjusts eye patch* What be bringin' ye to these waters? Are ye here to plunder some booty or just to enjoy the sea breeze?\", additional_kwargs={}, example=False)]}\n\n\nNow as another question that requires use of the memory.\n\ninputs = {\"input\": \"whats my name\"}\nchain.invoke(inputs)\n\n    AIMessage(content=\"Arrrr, ye be askin' about yer name, eh? Well, me matey, I be knowin' ye as Bob, the scurvy dog! *winks* But if ye want me to call ye somethin' else, just let me know, and I\", additional_kwargs={}, example=False)\n\nPrevious\nEverlyAI\nNext\nGigaChat"
}