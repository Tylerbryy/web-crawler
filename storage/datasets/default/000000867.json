{
	"title": "Voyage AI | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/integrations/text_embedding/voyageai",
	"html": "ComponentsText embedding modelsVoyage AI\nVoyage AI\n\nVoyage AI provides cutting-edge embedding/vectorizations models.\n\nLet's load the Voyage Embedding class.\n\nfrom langchain.embeddings import VoyageEmbeddings\n\n\nVoyage AI utilizes API keys to monitor usage and manage permissions. To obtain your key, create an account on our homepage. Then, create a VoyageEmbeddings model with your API key.\n\nembeddings = VoyageEmbeddings(voyage_api_key=\"[ Your Voyage API key ]\")\n\n\nPrepare the documents and use embed_documents to get their embeddings.\n\ndocuments = [\n    \"Caching embeddings enables the storage or temporary caching of embeddings, eliminating the necessity to recompute them each time.\",\n    \"An LLMChain is a chain that composes basic LLM functionality. It consists of a PromptTemplate and a language model (either an LLM or chat model). It formats the prompt template using the input key values provided (and also memory key values, if available), passes the formatted string to LLM and returns the LLM output.\",\n    \"A Runnable represents a generic unit of work that can be invoked, batched, streamed, and/or transformed.\",\n]\n\ndocuments_embds = embeddings.embed_documents(documents)\n\ndocuments_embds[0][:5]\n\n    [0.0562174916267395,\n     0.018221192061901093,\n     0.0025736060924828053,\n     -0.009720131754875183,\n     0.04108370840549469]\n\n\nSimilarly, use embed_query to embed the query.\n\nquery = \"What's an LLMChain?\"\n\nquery_embd = embeddings.embed_query(query)\n\nquery_embd[:5]\n\n    [-0.0052348352037370205,\n     -0.040072452276945114,\n     0.0033957737032324076,\n     0.01763271726667881,\n     -0.019235141575336456]\n\nA minimalist retrieval system‚Äã\n\nThe main feature of the embeddings is that the cosine similarity between two embeddings captures the semantic relatedness of the corresponding original passages. This allows us to use the embeddings to do semantic retrieval / search.\n\nWe can find a few closest embeddings in the documents embeddings based on the cosine similarity, and retrieve the corresponding document using the KNNRetriever class from LangChain.\n\nfrom langchain.retrievers import KNNRetriever\n\nretriever = KNNRetriever.from_texts(documents, embeddings)\n\n# retrieve the most relevant documents\nresult = retriever.get_relevant_documents(query)\ntop1_retrieved_doc = result[0].page_content  # return the top1 retrieved result\n\nprint(top1_retrieved_doc)\n\n    An LLMChain is a chain that composes basic LLM functionality. It consists of a PromptTemplate and a language model (either an LLM or chat model). It formats the prompt template using the input key values provided (and also memory key values, if available), passes the formatted string to LLM and returns the LLM output.\n\nPrevious\nTensorFlow Hub\nNext\nXorbits inference (Xinference)"
}