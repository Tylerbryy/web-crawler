{
	"title": "Pinecone | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/integrations/vectorstores/pinecone",
	"html": "ComponentsVector storesPinecone\nPinecone\n\nPinecone is a vector database with broad functionality.\n\nThis notebook shows how to use functionality related to the Pinecone vector database.\n\nTo use Pinecone, you must have an API key. Here are the installation instructions.\n\npip install pinecone-client openai tiktoken langchain\n\nimport getpass\nimport os\n\nos.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Pinecone API Key:\")\n\nos.environ[\"PINECONE_ENV\"] = getpass.getpass(\"Pinecone Environment:\")\n\n\nWe want to use OpenAIEmbeddings so we have to get the OpenAI API Key.\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n\nfrom langchain.document_loaders import TextLoader\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.vectorstores import Pinecone\n\nfrom langchain.document_loaders import TextLoader\n\nloader = TextLoader(\"../../modules/state_of_the_union.txt\")\ndocuments = loader.load()\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\ndocs = text_splitter.split_documents(documents)\n\nembeddings = OpenAIEmbeddings()\n\nimport pinecone\n\n# initialize pinecone\npinecone.init(\n    api_key=os.getenv(\"PINECONE_API_KEY\"),  # find at app.pinecone.io\n    environment=os.getenv(\"PINECONE_ENV\"),  # next to api key in console\n)\n\nindex_name = \"langchain-demo\"\n\n# First, check if our index already exists. If it doesn't, we create it\nif index_name not in pinecone.list_indexes():\n    # we create a new index\n    pinecone.create_index(name=index_name, metric=\"cosine\", dimension=1536)\n# The OpenAI embedding model `text-embedding-ada-002 uses 1536 dimensions`\ndocsearch = Pinecone.from_documents(docs, embeddings, index_name=index_name)\n\n# if you already have an index, you can load it like this\n# docsearch = Pinecone.from_existing_index(index_name, embeddings)\n\nquery = \"What did the president say about Ketanji Brown Jackson\"\ndocs = docsearch.similarity_search(query)\n\nprint(docs[0].page_content)\n\nAdding More Text to an Existing Index‚Äã\n\nMore text can embedded and upserted to an existing Pinecone index using the add_texts function\n\nindex = pinecone.Index(\"langchain-demo\")\nvectorstore = Pinecone(index, embeddings.embed_query, \"text\")\n\nvectorstore.add_texts(\"More text!\")\n\nMaximal Marginal Relevance Searches‚Äã\n\nIn addition to using similarity search in the retriever object, you can also use mmr as retriever.\n\nretriever = docsearch.as_retriever(search_type=\"mmr\")\nmatched_docs = retriever.get_relevant_documents(query)\nfor i, d in enumerate(matched_docs):\n    print(f\"\\n## Document {i}\\n\")\n    print(d.page_content)\n\n\nOr use max_marginal_relevance_search directly:\n\nfound_docs = docsearch.max_marginal_relevance_search(query, k=2, fetch_k=10)\nfor i, doc in enumerate(found_docs):\n    print(f\"{i + 1}.\", doc.page_content, \"\\n\")\n\nPrevious\nPGVector\nNext\nQdrant"
}