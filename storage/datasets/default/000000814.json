{
	"title": "Different call methods | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/modules/chains/how_to/call_methods",
	"html": "ModulesMoreChainsHow toDifferent call methods\nDifferent call methods\n\nAll classes inherited from Chain offer a few ways of running chain logic. The most direct one is by using __call__:\n\nchat = ChatOpenAI(temperature=0)\nprompt_template = \"Tell me a {adjective} joke\"\nllm_chain = LLMChain(llm=chat, prompt=PromptTemplate.from_template(prompt_template))\n\nllm_chain(inputs={\"adjective\": \"corny\"})\n\n    {'adjective': 'corny',\n     'text': 'Why did the tomato turn red? Because it saw the salad dressing!'}\n\n\nBy default, __call__ returns both the input and output key values. You can configure it to only return output key values by setting return_only_outputs to True.\n\nllm_chain(\"corny\", return_only_outputs=True)\n\n    {'text': 'Why did the tomato turn red? Because it saw the salad dressing!'}\n\n\nIf the Chain only outputs one output key (i.e. only has one element in its output_keys), you can use run method. Note that run outputs a string instead of a dictionary.\n\n# llm_chain only has one output key, so we can use run\nllm_chain.output_keys\n\n    ['text']\n\nllm_chain.run({\"adjective\": \"corny\"})\n\n    'Why did the tomato turn red? Because it saw the salad dressing!'\n\n\nIn the case of one input key, you can input the string directly without specifying the input mapping.\n\n# These two are equivalent\nllm_chain.run({\"adjective\": \"corny\"})\nllm_chain.run(\"corny\")\n\n# These two are also equivalent\nllm_chain(\"corny\")\nllm_chain({\"adjective\": \"corny\"})\n\n    {'adjective': 'corny',\n     'text': 'Why did the tomato turn red? Because it saw the salad dressing!'}\n\n\nTips: You can easily integrate a Chain object as a Tool in your Agent via its run method. See an example here.\n\nPrevious\nAsync API\nNext\nCustom chain"
}