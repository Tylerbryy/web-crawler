{
	"title": "Custom chain | ðŸ¦œï¸ðŸ”— Langchain",
	"url": "https://python.langchain.com/docs/modules/chains/how_to/custom_chain",
	"html": "ModulesMoreChainsHow toCustom chain\nCustom chain\n\nTo implement your own custom chain you can subclass Chain and implement the following methods:\n\nfrom __future__ import annotations\n\nfrom typing import Any, Dict, List, Optional\n\nfrom langchain.callbacks.manager import (\n    AsyncCallbackManagerForChainRun,\n    CallbackManagerForChainRun,\n)\nfrom langchain.chains.base import Chain\nfrom langchain.prompts.base import BasePromptTemplate\nfrom langchain.schema.language_model import BaseLanguageModel\nfrom pydantic import Extra\n\n\nclass MyCustomChain(Chain):\n    \"\"\"\n    An example of a custom chain.\n    \"\"\"\n\n    prompt: BasePromptTemplate\n    \"\"\"Prompt object to use.\"\"\"\n    llm: BaseLanguageModel\n    output_key: str = \"text\"  #: :meta private:\n\n    class Config:\n        \"\"\"Configuration for this pydantic object.\"\"\"\n\n        extra = Extra.forbid\n        arbitrary_types_allowed = True\n\n    @property\n    def input_keys(self) -> List[str]:\n        \"\"\"Will be whatever keys the prompt expects.\n\n        :meta private:\n        \"\"\"\n        return self.prompt.input_variables\n\n    @property\n    def output_keys(self) -> List[str]:\n        \"\"\"Will always return text key.\n\n        :meta private:\n        \"\"\"\n        return [self.output_key]\n\n    def _call(\n        self,\n        inputs: Dict[str, Any],\n        run_manager: Optional[CallbackManagerForChainRun] = None,\n    ) -> Dict[str, str]:\n        # Your custom chain logic goes here\n        # This is just an example that mimics LLMChain\n        prompt_value = self.prompt.format_prompt(**inputs)\n\n        # Whenever you call a language model, or another chain, you should pass\n        # a callback manager to it. This allows the inner run to be tracked by\n        # any callbacks that are registered on the outer run.\n        # You can always obtain a callback manager for this by calling\n        # `run_manager.get_child()` as shown below.\n        response = self.llm.generate_prompt(\n            [prompt_value], callbacks=run_manager.get_child() if run_manager else None\n        )\n\n        # If you want to log something about this run, you can do so by calling\n        # methods on the `run_manager`, as shown below. This will trigger any\n        # callbacks that are registered for that event.\n        if run_manager:\n            run_manager.on_text(\"Log something about this run\")\n\n        return {self.output_key: response.generations[0][0].text}\n\n    async def _acall(\n        self,\n        inputs: Dict[str, Any],\n        run_manager: Optional[AsyncCallbackManagerForChainRun] = None,\n    ) -> Dict[str, str]:\n        # Your custom chain logic goes here\n        # This is just an example that mimics LLMChain\n        prompt_value = self.prompt.format_prompt(**inputs)\n\n        # Whenever you call a language model, or another chain, you should pass\n        # a callback manager to it. This allows the inner run to be tracked by\n        # any callbacks that are registered on the outer run.\n        # You can always obtain a callback manager for this by calling\n        # `run_manager.get_child()` as shown below.\n        response = await self.llm.agenerate_prompt(\n            [prompt_value], callbacks=run_manager.get_child() if run_manager else None\n        )\n\n        # If you want to log something about this run, you can do so by calling\n        # methods on the `run_manager`, as shown below. This will trigger any\n        # callbacks that are registered for that event.\n        if run_manager:\n            await run_manager.on_text(\"Log something about this run\")\n\n        return {self.output_key: response.generations[0][0].text}\n\n    @property\n    def _chain_type(self) -> str:\n        return \"my_custom_chain\"\n\nfrom langchain.callbacks.stdout import StdOutCallbackHandler\nfrom langchain.chat_models.openai import ChatOpenAI\nfrom langchain.prompts.prompt import PromptTemplate\n\nchain = MyCustomChain(\n    prompt=PromptTemplate.from_template(\"tell us a joke about {topic}\"),\n    llm=ChatOpenAI(),\n)\n\nchain.run({\"topic\": \"callbacks\"}, callbacks=[StdOutCallbackHandler()])\n\n    \n    \n    > Entering new MyCustomChain chain...\n    Log something about this run\n    > Finished chain.\n\n\n\n\n\n    'Why did the callback function feel lonely? Because it was always waiting for someone to call it back!'\n\nPrevious\nDifferent call methods\nNext\nAdding memory (state)"
}