{
	"title": "Astra DB | 🦜️🔗 Langchain",
	"url": "https://python.langchain.com/docs/integrations/vectorstores/astradb",
	"html": "ComponentsVector storesAstra DB\nAstra DB\n\nThis page provides a quickstart for using Astra DB and Apache Cassandra® as a Vector Store.\n\nNote: in addition to access to the database, an OpenAI API Key is required to run the full example.\n\nSetup and general dependencies​\n\nUse of the integration requires the following Python package.\n\npip install --quiet \"astrapy>=0.5.3\"\n\n\nNote: depending on your LangChain setup, you may need to install/upgrade other dependencies needed for this demo (specifically, recent versions of datasets, openai, pypdf and tiktoken are required).\n\nimport os\nfrom getpass import getpass\n\nfrom datasets import (\n    load_dataset,\n)\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.document_loaders import PyPDFLoader\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.schema import Document\nfrom langchain.schema.output_parser import StrOutputParser\nfrom langchain.schema.runnable import RunnablePassthrough\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\nos.environ[\"OPENAI_API_KEY\"] = getpass(\"OPENAI_API_KEY = \")\n\nembe = OpenAIEmbeddings()\n\n\nKeep reading to connect with Astra DB. For usage with Apache Cassandra and Astra DB through CQL, scroll to the section below.\n\nAstra DB​\n\nDataStax Astra DB is a serverless vector-capable database built on Cassandra and made conveniently available through an easy-to-use JSON API.\n\nfrom langchain.vectorstores import AstraDB\n\nAstra DB connection parameters​\nthe API Endpoint looks like https://01234567-89ab-cdef-0123-456789abcdef-us-east1.apps.astra.datastax.com\nthe Token looks like AstraCS:6gBhNmsk135....\nASTRA_DB_API_ENDPOINT = input(\"ASTRA_DB_API_ENDPOINT = \")\nASTRA_DB_APPLICATION_TOKEN = getpass(\"ASTRA_DB_APPLICATION_TOKEN = \")\n\nvstore = AstraDB(\n    embedding=embe,\n    collection_name=\"astra_vector_demo\",\n    api_endpoint=ASTRA_DB_API_ENDPOINT,\n    token=ASTRA_DB_APPLICATION_TOKEN,\n)\n\nLoad a dataset​\n\nConvert each entry in the source dataset into a Document, then write them into the vector store:\n\nphilo_dataset = load_dataset(\"datastax/philosopher-quotes\")[\"train\"]\n\ndocs = []\nfor entry in philo_dataset:\n    metadata = {\"author\": entry[\"author\"]}\n    doc = Document(page_content=entry[\"quote\"], metadata=metadata)\n    docs.append(doc)\n\ninserted_ids = vstore.add_documents(docs)\nprint(f\"\\nInserted {len(inserted_ids)} documents.\")\n\n\nIn the above, metadata dictionaries are created from the source data and are part of the Document.\n\nNote: check the Astra DB API Docs for the valid metadata field names: some characters are reserved and cannot be used.\n\nAdd some more entries, this time with add_texts:\n\ntexts = [\"I think, therefore I am.\", \"To the things themselves!\"]\nmetadatas = [{\"author\": \"descartes\"}, {\"author\": \"husserl\"}]\nids = [\"desc_01\", \"huss_xy\"]\n\ninserted_ids_2 = vstore.add_texts(texts=texts, metadatas=metadatas, ids=ids)\nprint(f\"\\nInserted {len(inserted_ids_2)} documents.\")\n\n\nNote: you may want to speed up the execution of add_texts and add_documents by increasing the concurrency level for these bulk operations - check out the *_concurrency parameters in the class constructor and the add_texts docstrings for more details. Depending on the network and the client machine specifications, your best-performing choice of parameters may vary.\n\nRun simple searches​\n\nThis section demonstrates metadata filtering and getting the similarity scores back:\n\nresults = vstore.similarity_search(\"Our life is what we make of it\", k=3)\nfor res in results:\n    print(f\"* {res.page_content} [{res.metadata}]\")\n\nresults_filtered = vstore.similarity_search(\n    \"Our life is what we make of it\",\n    k=3,\n    filter={\"author\": \"plato\"},\n)\nfor res in results_filtered:\n    print(f\"* {res.page_content} [{res.metadata}]\")\n\nresults = vstore.similarity_search_with_score(\"Our life is what we make of it\", k=3)\nfor res, score in results:\n    print(f\"* [SIM={score:3f}] {res.page_content} [{res.metadata}]\")\n\nMMR (Maximal-marginal-relevance) search​\nresults = vstore.max_marginal_relevance_search(\n    \"Our life is what we make of it\",\n    k=3,\n    filter={\"author\": \"aristotle\"},\n)\nfor res in results:\n    print(f\"* {res.page_content} [{res.metadata}]\")\n\nDeleting stored documents​\ndelete_1 = vstore.delete(inserted_ids[:3])\nprint(f\"all_succeed={delete_1}\")  # True, all documents deleted\n\ndelete_2 = vstore.delete(inserted_ids[2:5])\nprint(f\"some_succeeds={delete_2}\")  # True, though some IDs were gone already\n\nA minimal RAG chain​\n\nThe next cells will implement a simple RAG pipeline:\n\ndownload a sample PDF file and load it onto the store;\ncreate a RAG chain with LCEL (LangChain Expression Language), with the vector store at its heart;\nrun the question-answering chain.\ncurl -L \\\n    \"https://github.com/awesome-astra/datasets/blob/main/demo-resources/what-is-philosophy/what-is-philosophy.pdf?raw=true\" \\\n    -o \"what-is-philosophy.pdf\"\n\npdf_loader = PyPDFLoader(\"what-is-philosophy.pdf\")\nsplitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=64)\ndocs_from_pdf = pdf_loader.load_and_split(text_splitter=splitter)\n\nprint(f\"Documents from PDF: {len(docs_from_pdf)}.\")\ninserted_ids_from_pdf = vstore.add_documents(docs_from_pdf)\nprint(f\"Inserted {len(inserted_ids_from_pdf)} documents.\")\n\nretriever = vstore.as_retriever(search_kwargs={\"k\": 3})\n\nphilo_template = \"\"\"\nYou are a philosopher that draws inspiration from great thinkers of the past\nto craft well-thought answers to user questions. Use the provided context as the basis\nfor your answers and do not make up new reasoning paths - just mix-and-match what you are given.\nYour answers must be concise and to the point, and refrain from answering about other topics than philosophy.\n\nCONTEXT:\n{context}\n\nQUESTION: {question}\n\nYOUR ANSWER:\"\"\"\n\nphilo_prompt = ChatPromptTemplate.from_template(philo_template)\n\nllm = ChatOpenAI()\n\nchain = (\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\n    | philo_prompt\n    | llm\n    | StrOutputParser()\n)\n\nchain.invoke(\"How does Russel elaborate on Peirce's idea of the security blanket?\")\n\n\nFor more, check out a complete RAG template using Astra DB here.\n\nCleanup​\n\nIf you want to completely delete the collection from your Astra DB instance, run this.\n\n(You will lose the data you stored in it.)\n\nvstore.delete_collection()\n\nApache Cassandra and Astra DB through CQL​\n\nCassandra is a NoSQL, row-oriented, highly scalable and highly available database.Starting with version 5.0, the database ships with vector search capabilities.\n\nDataStax Astra DB through CQL is a managed serverless database built on Cassandra, offering the same interface and strengths.\n\nWhat sets this case apart from \"Astra DB\" above?​\n\nThanks to LangChain having a standardized VectorStore interface, most of the \"Astra DB\" section above applies to this case as well. However, this time the database uses the CQL protocol, which means you'll use a different class this time and instantiate it in another way.\n\nThe cells below show how you should get your vstore object in this case and how you can clean up the database resources at the end: for the rest, i.e. the actual usage of the vector store, you will be able to run the very code that was shown above.\n\nIn other words, running this demo in full with Cassandra or Astra DB through CQL means:\n\ninitialization as shown below\n\"Load a dataset\", see above section\n\"Run simple searches\", see above section\n\"MMR search\", see above section\n\"Deleting stored documents\", see above section\n\"A minimal RAG chain\", see above section\ncleanup as shown below\nInitialization​\n\nThe class to use is the following:\n\nfrom langchain.vectorstores import Cassandra\n\n\nNow, depending on whether you connect to a Cassandra cluster or to Astra DB through CQL, you will provide different parameters when creating the vector store object.\n\nInitialization (Cassandra cluster)​\n\nIn this case, you first need to create a cassandra.cluster.Session object, as described in the Cassandra driver documentation. The details vary (e.g. with network settings and authentication), but this might be something like:\n\nfrom cassandra.cluster import Cluster\n\ncluster = Cluster([\"127.0.0.1\"])\nsession = cluster.connect()\n\n\nYou can now set the session, along with your desired keyspace name, as a global CassIO parameter:\n\nimport cassio\n\nCASSANDRA_KEYSPACE = input(\"CASSANDRA_KEYSPACE = \")\n\ncassio.init(session=session, keyspace=CASSANDRA_KEYSPACE)\n\n\nNow you can create the vector store:\n\nvstore = Cassandra(\n    embedding=embe,\n    table_name=\"cassandra_vector_demo\",\n    # session=None, keyspace=None  # Uncomment on older versions of LangChain\n)\n\nInitialization (Astra DB through CQL)​\n\nIn this case you initialize CassIO with the following connection parameters:\n\nthe Database ID, e.g. 01234567-89ab-cdef-0123-456789abcdef\nthe Token, e.g. AstraCS:6gBhNmsk135.... (it must be a \"Database Administrator\" token)\nOptionally a Keyspace name (if omitted, the default one for the database will be used)\nASTRA_DB_ID = input(\"ASTRA_DB_ID = \")\nASTRA_DB_APPLICATION_TOKEN = getpass(\"ASTRA_DB_APPLICATION_TOKEN = \")\n\ndesired_keyspace = input(\"ASTRA_DB_KEYSPACE (optional, can be left empty) = \")\nif desired_keyspace:\n    ASTRA_DB_KEYSPACE = desired_keyspace\nelse:\n    ASTRA_DB_KEYSPACE = None\n\nimport cassio\n\ncassio.init(\n    database_id=ASTRA_DB_ID,\n    token=ASTRA_DB_APPLICATION_TOKEN,\n    keyspace=ASTRA_DB_KEYSPACE,\n)\n\n\nNow you can create the vector store:\n\nvstore = Cassandra(\n    embedding=embe,\n    table_name=\"cassandra_vector_demo\",\n    # session=None, keyspace=None  # Uncomment on older versions of LangChain\n)\n\nUsage of the vector store​\n\nSee the sections \"Load a dataset\" through \"A minimal RAG chain\" above.\n\nSpeaking of the latter, you can check out a full RAG template for Astra DB through CQL here.\n\nCleanup​\n\nthe following essentially retrieves the Session object from CassIO and runs a CQL DROP TABLE statement with it:\n\ncassio.config.resolve_session().execute(\n    f\"DROP TABLE {cassio.config.resolve_keyspace()}.cassandra_vector_demo;\"\n)\n\nLearn more​\n\nFor more information, extended quickstarts and additional usage examples, please visit the CassIO documentation for more on using the LangChain Cassandra vector store.\n\nPrevious\nAnnoy\nNext\nAtlas"
}