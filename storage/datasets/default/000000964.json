{
	"title": "Recursively split by character | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter",
	"html": "ModulesRetrievalDocument transformersText splittersRecursively split by character\nRecursively split by character\n\nThis text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"]. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.\n\nHow the text is split: by list of characters.\nHow the chunk size is measured: by number of characters.\n# This is a long document we can split up.\nwith open('../../../state_of_the_union.txt') as f:\n    state_of_the_union = f.read()\n\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    # Set a really small chunk size, just to show.\n    chunk_size = 100,\n    chunk_overlap  = 20,\n    length_function = len,\n    is_separator_regex = False,\n)\n\ntexts = text_splitter.create_documents([state_of_the_union])\nprint(texts[0])\nprint(texts[1])\n\n    page_content='Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and' lookup_str='' metadata={} lookup_index=0\n    page_content='of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.' lookup_str='' metadata={} lookup_index=0\n\ntext_splitter.split_text(state_of_the_union)[:2]\n\n    ['Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and',\n     'of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.']\n\nPrevious\nMarkdownHeaderTextSplitter\nNext\nSplit by tokens"
}