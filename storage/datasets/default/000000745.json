{
	"title": "PromptLayer | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/integrations/providers/promptlayer",
	"html": "ProvidersMorePromptLayer\nPromptLayer\n\nThis page covers how to use PromptLayer within LangChain. It is broken into two parts: installation and setup, and then references to specific PromptLayer wrappers.\n\nInstallation and Setup‚Äã\n\nIf you want to work with PromptLayer:\n\nInstall the promptlayer python library pip install promptlayer\nCreate a PromptLayer account\nCreate an api token and set it as an environment variable (PROMPTLAYER_API_KEY)\nWrappers‚Äã\nLLM‚Äã\n\nThere exists an PromptLayer OpenAI LLM wrapper, which you can access with\n\nfrom langchain.llms import PromptLayerOpenAI\n\n\nTo tag your requests, use the argument pl_tags when initializing the LLM\n\nfrom langchain.llms import PromptLayerOpenAI\nllm = PromptLayerOpenAI(pl_tags=[\"langchain-requests\", \"chatbot\"])\n\n\nTo get the PromptLayer request id, use the argument return_pl_id when initializing the LLM\n\nfrom langchain.llms import PromptLayerOpenAI\nllm = PromptLayerOpenAI(return_pl_id=True)\n\n\nThis will add the PromptLayer request ID in the generation_info field of the Generation returned when using .generate or .agenerate\n\nFor example:\n\nllm_results = llm.generate([\"hello world\"])\nfor res in llm_results.generations:\n    print(\"pl request id: \", res[0].generation_info[\"pl_request_id\"])\n\n\nYou can use the PromptLayer request ID to add a prompt, score, or other metadata to your request. Read more about it here.\n\nThis LLM is identical to the OpenAI LLM, except that\n\nall your requests will be logged to your PromptLayer account\nyou can add pl_tags when instantiating to tag your requests on PromptLayer\nyou can add return_pl_id when instantiating to return a PromptLayer request id to use while tracking requests.\n\nPromptLayer also provides native wrappers for PromptLayerChatOpenAI and PromptLayerOpenAIChat\n\nPrevious\nPrediction Guard\nNext\nSemaDB"
}