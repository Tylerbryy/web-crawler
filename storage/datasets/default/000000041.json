{
	"title": "Run LLMs locally | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/guides/local_llms",
	"html": "Run LLMs locally\nRun LLMs locally\nUse case‚Äã\n\nThe popularity of projects like PrivateGPT, llama.cpp, and GPT4All underscore the demand to run LLMs locally (on your own device).\n\nThis has at least two important benefits:\n\nPrivacy: Your data is not sent to a third party, and it is not subject to the terms of service of a commercial service\nCost: There is no inference fee, which is important for token-intensive applications (e.g., long-running simulations, summarization)\nOverview‚Äã\n\nRunning an LLM locally requires a few things:\n\nOpen-source LLM: An open-source LLM that can be freely modified and shared\nInference: Ability to run this LLM on your device w/ acceptable latency\nOpen-source LLMs‚Äã\n\nUsers can now gain access to a rapidly growing set of open-source LLMs.\n\nThese LLMs can be assessed across at least two dimensions (see figure):\n\nBase model: What is the base-model and how was it trained?\nFine-tuning approach: Was the base-model fine-tuned and, if so, what set of instructions was used?\n\nThe relative performance of these models can be assessed using several leaderboards, including:\n\nLmSys\nGPT4All\nHuggingFace\nInference‚Äã\n\nA few frameworks for this have emerged to support inference of open-source LLMs on various devices:\n\nllama.cpp: C++ implementation of llama inference code with weight optimization / quantization\ngpt4all: Optimized C backend for inference\nOllama: Bundles model weights and environment into an app that runs on device and serves the LLM\n\nIn general, these frameworks will do a few things:\n\nQuantization: Reduce the memory footprint of the raw model weights\nEfficient implementation for inference: Support inference on consumer hardware (e.g., CPU or laptop GPU)\n\nIn particular, see this excellent post on the importance of quantization.\n\nWith less precision, we radically decrease the memory needed to store the LLM in memory.\n\nIn addition, we can see the importance of GPU memory bandwidth sheet!\n\nA Mac M2 Max is 5-6x faster than a M1 for inference due to the larger GPU memory bandwidth.\n\nQuickstart‚Äã\n\nOllama is one way to easily run inference on macOS.\n\nThe instructions here provide details, which we summarize:\n\nDownload and run the app\nFrom command line, fetch a model from this list of options: e.g., ollama pull llama2\nWhen the app is running, all models are automatically served on localhost:11434\nfrom langchain.llms import Ollama\n\nllm = Ollama(model=\"llama2\")\nllm(\"The first man on the moon was ...\")\n\n    ' The first man on the moon was Neil Armstrong, who landed on the moon on July 20, 1969 as part of the Apollo 11 mission. obviously.'\n\n\nStream tokens as they are being generated.\n\nfrom langchain.callbacks.manager import CallbackManager\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n\nllm = Ollama(\n    model=\"llama2\", callback_manager=CallbackManager([StreamingStdOutCallbackHandler()])\n)\nllm(\"The first man on the moon was ...\")\n\n     The first man to walk on the moon was Neil Armstrong, an American astronaut who was part of the Apollo 11 mission in 1969. —Ñ–µ–≤—Ä—É–∞—Ä–∏ 20, 1969, Armstrong stepped out of the lunar module Eagle and onto the moon's surface, famously declaring \"That's one small step for man, one giant leap for mankind\" as he took his first steps. He was followed by fellow astronaut Edwin \"Buzz\" Aldrin, who also walked on the moon during the mission.\n\n\n\n\n    ' The first man to walk on the moon was Neil Armstrong, an American astronaut who was part of the Apollo 11 mission in 1969. —Ñ–µ–≤—Ä—É–∞—Ä–∏ 20, 1969, Armstrong stepped out of the lunar module Eagle and onto the moon\\'s surface, famously declaring \"That\\'s one small step for man, one giant leap for mankind\" as he took his first steps. He was followed by fellow astronaut Edwin \"Buzz\" Aldrin, who also walked on the moon during the mission.'\n\nEnvironment‚Äã\n\nInference speed is a challenge when running models locally (see above).\n\nTo minimize latency, it is desirable to run models locally on GPU, which ships with many consumer laptops e.g., Apple devices.\n\nAnd even with GPU, the available GPU memory bandwidth (as noted above) is important.\n\nRunning Apple silicon GPU‚Äã\n\nOllama will automatically utilize the GPU on Apple devices.\n\nOther frameworks require the user to set up the environment to utilize the Apple GPU.\n\nFor example, llama.cpp python bindings can be configured to use the GPU via Metal.\n\nMetal is a graphics and compute API created by Apple providing near-direct access to the GPU.\n\nSee the llama.cpp setup here to enable this.\n\nIn particular, ensure that conda is using the correct virtual environment that you created (miniforge3).\n\nE.g., for me:\n\nconda activate /Users/rlm/miniforge3/envs/llama\n\n\nWith the above confirmed, then:\n\nCMAKE_ARGS=\"-DLLAMA_METAL=on\" FORCE_CMAKE=1 pip install -U llama-cpp-python --no-cache-dir\n\nLLMs‚Äã\n\nThere are various ways to gain access to quantized model weights.\n\nHuggingFace - Many quantized model are available for download and can be run with framework such as llama.cpp\ngpt4all - The model explorer offers a leaderboard of metrics and associated quantized models available for download\nOllama - Several models can be accessed directly via pull\nOllama‚Äã\n\nWith Ollama, fetch a model via ollama pull <model family>:<tag>:\n\nE.g., for Llama-7b: ollama pull llama2 will download the most basic version of the model (e.g., smallest # parameters and 4 bit quantization)\nWe can also specify a particular version from the model list, e.g., ollama pull llama2:13b\nSee the full set of parameters on the API reference page\nfrom langchain.llms import Ollama\n\nllm = Ollama(model=\"llama2:13b\")\nllm(\"The first man on the moon was ... think step by step\")\n\n    ' Sure! Here\\'s the answer, broken down step by step:\\n\\nThe first man on the moon was... Neil Armstrong.\\n\\nHere\\'s how I arrived at that answer:\\n\\n1. The first manned mission to land on the moon was Apollo 11.\\n2. The mission included three astronauts: Neil Armstrong, Edwin \"Buzz\" Aldrin, and Michael Collins.\\n3. Neil Armstrong was the mission commander and the first person to set foot on the moon.\\n4. On July 20, 1969, Armstrong stepped out of the lunar module Eagle and onto the moon\\'s surface, famously declaring \"That\\'s one small step for man, one giant leap for mankind.\"\\n\\nSo, the first man on the moon was Neil Armstrong!'\n\nLlama.cpp‚Äã\n\nLlama.cpp is compatible with a broad set of models.\n\nFor example, below we run inference on llama2-13b with 4 bit quantization downloaded from HuggingFace.\n\nAs noted above, see the API reference for the full set of parameters.\n\nFrom the llama.cpp docs, a few are worth commenting on:\n\nn_gpu_layers: number of layers to be loaded into GPU memory\n\nValue: 1\nMeaning: Only one layer of the model will be loaded into GPU memory (1 is often sufficient).\n\nn_batch: number of tokens the model should process in parallel\n\nValue: n_batch\nMeaning: It's recommended to choose a value between 1 and n_ctx (which in this case is set to 2048)\n\nn_ctx: Token context window .\n\nValue: 2048\nMeaning: The model will consider a window of 2048 tokens at a time\n\nf16_kv: whether the model should use half-precision for the key/value cache\n\nValue: True\nMeaning: The model will use half-precision, which can be more memory efficient; Metal only supports True.\n%pip install -U llama-cpp-python --no-cache-dirclear`\n\nfrom langchain.llms import LlamaCpp\n\nllm = LlamaCpp(\n    model_path=\"/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin\",\n    n_gpu_layers=1,\n    n_batch=512,\n    n_ctx=2048,\n    f16_kv=True,\n    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n    verbose=True,\n)\n\n\nThe console log will show the below to indicate Metal was enabled properly from steps above:\n\nggml_metal_init: allocating\nggml_metal_init: using MPS\n\nllm(\"The first man on the moon was ... Let's think step by step\")\n\n    Llama.generate: prefix-match hit\n\n\n     and use logical reasoning to figure out who the first man on the moon was.\n    \n    Here are some clues:\n    \n    1. The first man on the moon was an American.\n    2. He was part of the Apollo 11 mission.\n    3. He stepped out of the lunar module and became the first person to set foot on the moon's surface.\n    4. His last name is Armstrong.\n    \n    Now, let's use our reasoning skills to figure out who the first man on the moon was. Based on clue #1, we know that the first man on the moon was an American. Clue #2 tells us that he was part of the Apollo 11 mission. Clue #3 reveals that he was the first person to set foot on the moon's surface. And finally, clue #4 gives us his last name: Armstrong.\n    Therefore, the first man on the moon was Neil Armstrong!\n\n    \n    llama_print_timings:        load time =  9623.21 ms\n    llama_print_timings:      sample time =   143.77 ms /   203 runs   (    0.71 ms per token,  1412.01 tokens per second)\n    llama_print_timings: prompt eval time =   485.94 ms /     7 tokens (   69.42 ms per token,    14.40 tokens per second)\n    llama_print_timings:        eval time =  6385.16 ms /   202 runs   (   31.61 ms per token,    31.64 tokens per second)\n    llama_print_timings:       total time =  7279.28 ms\n\n\n\n\n\n    \" and use logical reasoning to figure out who the first man on the moon was.\\n\\nHere are some clues:\\n\\n1. The first man on the moon was an American.\\n2. He was part of the Apollo 11 mission.\\n3. He stepped out of the lunar module and became the first person to set foot on the moon's surface.\\n4. His last name is Armstrong.\\n\\nNow, let's use our reasoning skills to figure out who the first man on the moon was. Based on clue #1, we know that the first man on the moon was an American. Clue #2 tells us that he was part of the Apollo 11 mission. Clue #3 reveals that he was the first person to set foot on the moon's surface. And finally, clue #4 gives us his last name: Armstrong.\\nTherefore, the first man on the moon was Neil Armstrong!\"\n\nGPT4All‚Äã\n\nWe can use model weights downloaded from GPT4All model explorer.\n\nSimilar to what is shown above, we can run inference and use the API reference to set parameters of interest.\n\npip install gpt4all\n\nfrom langchain.llms import GPT4All\n\nllm = GPT4All(\n    model=\"/Users/rlm/Desktop/Code/gpt4all/models/nous-hermes-13b.ggmlv3.q4_0.bin\"\n)\n\nllm(\"The first man on the moon was ... Let's think step by step\")\n\n    \".\\n1) The United States decides to send a manned mission to the moon.2) They choose their best astronauts and train them for this specific mission.3) They build a spacecraft that can take humans to the moon, called the Lunar Module (LM).4) They also create a larger spacecraft, called the Saturn V rocket, which will launch both the LM and the Command Service Module (CSM), which will carry the astronauts into orbit.5) The mission is planned down to the smallest detail: from the trajectory of the rockets to the exact movements of the astronauts during their moon landing.6) On July 16, 1969, the Saturn V rocket launches from Kennedy Space Center in Florida, carrying the Apollo 11 mission crew into space.7) After one and a half orbits around the Earth, the LM separates from the CSM and begins its descent to the moon's surface.8) On July 20, 1969, at 2:56 pm EDT (GMT-4), Neil Armstrong becomes the first man on the moon. He speaks these\"\n\nPrompts‚Äã\n\nSome LLMs will benefit from specific prompts.\n\nFor example, LLaMA will use special tokens.\n\nWe can use ConditionalPromptSelector to set prompt based on the model type.\n\n# Set our LLM\nllm = LlamaCpp(\n    model_path=\"/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin\",\n    n_gpu_layers=1,\n    n_batch=512,\n    n_ctx=2048,\n    f16_kv=True,\n    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n    verbose=True,\n)\n\n\nSet the associated prompt based upon the model version.\n\nfrom langchain.chains import LLMChain\nfrom langchain.chains.prompt_selector import ConditionalPromptSelector\nfrom langchain.prompts import PromptTemplate\n\nDEFAULT_LLAMA_SEARCH_PROMPT = PromptTemplate(\n    input_variables=[\"question\"],\n    template=\"\"\"<<SYS>> \\n You are an assistant tasked with improving Google search \\\nresults. \\n <</SYS>> \\n\\n [INST] Generate THREE Google search queries that \\\nare similar to this question. The output should be a numbered list of questions \\\nand each should have a question mark at the end: \\n\\n {question} [/INST]\"\"\",\n)\n\nDEFAULT_SEARCH_PROMPT = PromptTemplate(\n    input_variables=[\"question\"],\n    template=\"\"\"You are an assistant tasked with improving Google search \\\nresults. Generate THREE Google search queries that are similar to \\\nthis question. The output should be a numbered list of questions and each \\\nshould have a question mark at the end: {question}\"\"\",\n)\n\nQUESTION_PROMPT_SELECTOR = ConditionalPromptSelector(\n    default_prompt=DEFAULT_SEARCH_PROMPT,\n    conditionals=[(lambda llm: isinstance(llm, LlamaCpp), DEFAULT_LLAMA_SEARCH_PROMPT)],\n)\n\nprompt = QUESTION_PROMPT_SELECTOR.get_prompt(llm)\nprompt\n\n    PromptTemplate(input_variables=['question'], output_parser=None, partial_variables={}, template='<<SYS>> \\n You are an assistant tasked with improving Google search results. \\n <</SYS>> \\n\\n [INST] Generate THREE Google search queries that are similar to this question. The output should be a numbered list of questions and each should have a question mark at the end: \\n\\n {question} [/INST]', template_format='f-string', validate_template=True)\n\n# Chain\nllm_chain = LLMChain(prompt=prompt, llm=llm)\nquestion = \"What NFL team won the Super Bowl in the year that Justin Bieber was born?\"\nllm_chain.run({\"question\": question})\n\n      Sure! Here are three similar search queries with a question mark at the end:\n    \n    1. Which NBA team did LeBron James lead to a championship in the year he was drafted?\n    2. Who won the Grammy Awards for Best New Artist and Best Female Pop Vocal Performance in the same year that Lady Gaga was born?\n    3. What MLB team did Babe Ruth play for when he hit 60 home runs in a single season?\n\n    \n    llama_print_timings:        load time = 14943.19 ms\n    llama_print_timings:      sample time =    72.93 ms /   101 runs   (    0.72 ms per token,  1384.87 tokens per second)\n    llama_print_timings: prompt eval time = 14942.95 ms /    93 tokens (  160.68 ms per token,     6.22 tokens per second)\n    llama_print_timings:        eval time =  3430.85 ms /   100 runs   (   34.31 ms per token,    29.15 tokens per second)\n    llama_print_timings:       total time = 18578.26 ms\n\n\n\n\n\n    '  Sure! Here are three similar search queries with a question mark at the end:\\n\\n1. Which NBA team did LeBron James lead to a championship in the year he was drafted?\\n2. Who won the Grammy Awards for Best New Artist and Best Female Pop Vocal Performance in the same year that Lady Gaga was born?\\n3. What MLB team did Babe Ruth play for when he hit 60 home runs in a single season?'\n\n\nWe also can use the LangChain Prompt Hub to fetch and / or store prompts that are model specific.\n\nThis will work with your LangSmith API key.\n\nFor example, here is a prompt for RAG with LLaMA-specific tokens.\n\nUse cases‚Äã\n\nGiven an llm created from one of the models above, you can use it for many use cases.\n\nFor example, here is a guide to RAG with local LLMs.\n\nIn general, use cases for local LLMs can be driven by at least two factors:\n\nPrivacy: private data (e.g., journals, etc) that a user does not want to share\nCost: text preprocessing (extraction/tagging), summarization, and agent simulations are token-use-intensive tasks\n\nIn addition, here is an overview on fine-tuning, which can utilize open-source LLMs.\n\nPrevious\nFallbacks\nNext\nModel comparison"
}