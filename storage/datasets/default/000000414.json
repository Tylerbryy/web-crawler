{
	"title": "Async API | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/modules/model_io/llms/async_llm",
	"html": "ModulesModel I/OLLMsAsync API\nAsync API\n\nAll LLMs implement the Runnable interface, which comes with default implementations of all methods, ie. ainvoke, batch, abatch, stream, astream. This gives all LLMs basic support for asynchronous calls.\n\nAsync support defaults to calling the LLM's respective sync method in asyncio's default thread pool executor. This lets other async functions in your application make progress while the LLM is being executed, by moving this call to a background thread. Where LLMs providers have native implementations for async, that is used instead of the default LLM implementation.\n\nSee which integrations provide native async support here.\n\nimport asyncio\nimport time\n\nfrom langchain.llms import OpenAI\n\nllm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0.9)\n\n\ndef invoke_serially():\n    for _ in range(10):\n        resp = llm.invoke(\"Hello, how are you?\")\n\n\nasync def async_invoke(llm):\n    resp = await llm.ainvoke(\"Hello, how are you?\")\n\n\nasync def invoke_concurrently():\n    tasks = [async_invoke(llm) for _ in range(10)]\n    await asyncio.gather(*tasks)\n\n\ns = time.perf_counter()\n# If running this outside of Jupyter, use asyncio.run(generate_concurrently())\nawait invoke_concurrently()\nelapsed = time.perf_counter() - s\nprint(\"\\033[1m\" + f\"Concurrent executed in {elapsed:0.2f} seconds.\" + \"\\033[0m\")\n\ns = time.perf_counter()\ninvoke_serially()\nelapsed = time.perf_counter() - s\nprint(\"\\033[1m\" + f\"Serial executed in {elapsed:0.2f} seconds.\" + \"\\033[0m\")\n\n    Concurrent executed in 1.03 seconds.\n    Serial executed in 6.80 seconds.\n\n\nTo simplify things we could also just use abatch to run a batch concurrently:\n\ns = time.perf_counter()\n# If running this outside of Jupyter, use asyncio.run(generate_concurrently())\nawait llm.abatch([\"Hello, how are you?\"] * 10)\nelapsed = time.perf_counter() - s\nprint(\"\\033[1m\" + f\"Batch executed in {elapsed:0.2f} seconds.\" + \"\\033[0m\")\n\n    Batch executed in 1.31 seconds.\n\nPrevious\nLLMs\nNext\nCustom LLM"
}