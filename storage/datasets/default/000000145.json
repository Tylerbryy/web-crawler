{
	"title": "Text embedding models | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/modules/data_connection/text_embedding/",
	"html": "ModulesRetrievalText embedding models\nText embedding models\nINFO\n\nHead to Integrations for documentation on built-in integrations with text embedding model providers.\n\nThe Embeddings class is a class designed for interfacing with text embedding models. There are lots of embedding model providers (OpenAI, Cohere, Hugging Face, etc) - this class is designed to provide a standard interface for all of them.\n\nEmbeddings create a vector representation of a piece of text. This is useful because it means we can think about text in the vector space, and do things like semantic search where we look for pieces of text that are most similar in the vector space.\n\nThe base Embeddings class in LangChain provides two methods: one for embedding documents and one for embedding a query. The former takes as input multiple texts, while the latter takes a single text. The reason for having these as two separate methods is that some embedding providers have different embedding methods for documents (to be searched over) vs queries (the search query itself).\n\nGet started‚Äã\nSetup‚Äã\n\nTo start we'll need to install the OpenAI Python package:\n\npip install openai\n\n\nAccessing the API requires an API key, which you can get by creating an account and heading here. Once we have a key we'll want to set it as an environment variable by running:\n\nexport OPENAI_API_KEY=\"...\"\n\n\nIf you'd prefer not to set an environment variable you can pass the key in directly via the openai_api_key named parameter when initiating the OpenAI LLM class:\n\nfrom langchain.embeddings import OpenAIEmbeddings\n\nembeddings_model = OpenAIEmbeddings(openai_api_key=\"...\")\n\n\nOtherwise you can initialize without any params:\n\nfrom langchain.embeddings import OpenAIEmbeddings\n\nembeddings_model = OpenAIEmbeddings()\n\nembed_documents‚Äã\nEmbed list of texts‚Äã\nembeddings = embeddings_model.embed_documents(\n    [\n        \"Hi there!\",\n        \"Oh, hello!\",\n        \"What's your name?\",\n        \"My friends call me World\",\n        \"Hello World!\"\n    ]\n)\nlen(embeddings), len(embeddings[0])\n\n(5, 1536)\n\nembed_query‚Äã\nEmbed single query‚Äã\n\nEmbed a single piece of text for the purpose of comparing to other embedded pieces of texts.\n\nembedded_query = embeddings_model.embed_query(\"What was the name mentioned in the conversation?\")\nembedded_query[:5]\n\n[0.0053587136790156364,\n -0.0004999046213924885,\n 0.038883671164512634,\n -0.003001077566295862,\n -0.00900818221271038]\n\nPrevious\nRetrieval\nNext\nCaching"
}