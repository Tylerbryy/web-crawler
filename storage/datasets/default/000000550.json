{
	"title": "Hugging Face Local Pipelines | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/integrations/llms/huggingface_pipelines",
	"html": "ComponentsLLMsHugging Face Local Pipelines\nHugging Face Local Pipelines\n\nHugging Face models can be run locally through the HuggingFacePipeline class.\n\nThe Hugging Face Model Hub hosts over 120k models, 20k datasets, and 50k demo apps (Spaces), all open source and publicly available, in an online platform where people can easily collaborate and build ML together.\n\nThese can be called from LangChain either through this local pipeline wrapper or by calling their hosted inference endpoints through the HuggingFaceHub class. For more information on the hosted pipelines, see the HuggingFaceHub notebook.\n\nTo use, you should have the transformers python package installed, as well as pytorch. You can also install xformer for a more memory-efficient attention implementation.\n\n%pip install transformers --quiet\n\nModel Loading‚Äã\n\nModels can be loaded by specifying the model parameters using the from_model_id method.\n\nfrom langchain.llms.huggingface_pipeline import HuggingFacePipeline\n\nhf = HuggingFacePipeline.from_model_id(\n    model_id=\"gpt2\",\n    task=\"text-generation\",\n    pipeline_kwargs={\"max_new_tokens\": 10},\n)\n\n\nThey can also be loaded by passing in an existing transformers pipeline directly\n\nfrom langchain.llms.huggingface_pipeline import HuggingFacePipeline\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\nmodel_id = \"gpt2\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id)\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=10)\nhf = HuggingFacePipeline(pipeline=pipe)\n\nCreate Chain‚Äã\n\nWith the model loaded into memory, you can compose it with a prompt to form a chain.\n\nfrom langchain.prompts import PromptTemplate\n\ntemplate = \"\"\"Question: {question}\n\nAnswer: Let's think step by step.\"\"\"\nprompt = PromptTemplate.from_template(template)\n\nchain = prompt | hf\n\nquestion = \"What is electroencephalography?\"\n\nprint(chain.invoke({\"question\": question}))\n\nGPU Inference‚Äã\n\nWhen running on a machine with GPU, you can specify the device=n parameter to put the model on the specified device. Defaults to -1 for CPU inference.\n\nIf you have multiple-GPUs and/or the model is too large for a single GPU, you can specify device_map=\"auto\", which requires and uses the Accelerate library to automatically determine how to load the model weights.\n\nNote: both device and device_map should not be specified together and can lead to unexpected behavior.\n\ngpu_llm = HuggingFacePipeline.from_model_id(\n    model_id=\"gpt2\",\n    task=\"text-generation\",\n    device=0,  # replace with device_map=\"auto\" to use the accelerate library.\n    pipeline_kwargs={\"max_new_tokens\": 10},\n)\n\ngpu_chain = prompt | gpu_llm\n\nquestion = \"What is electroencephalography?\"\n\nprint(gpu_chain.invoke({\"question\": question}))\n\nBatch GPU Inference‚Äã\n\nIf running on a device with GPU, you can also run inference on the GPU in batch mode.\n\ngpu_llm = HuggingFacePipeline.from_model_id(\n    model_id=\"bigscience/bloom-1b7\",\n    task=\"text-generation\",\n    device=0,  # -1 for CPU\n    batch_size=2,  # adjust as needed based on GPU map and model size.\n    model_kwargs={\"temperature\": 0, \"max_length\": 64},\n)\n\ngpu_chain = prompt | gpu_llm.bind(stop=[\"\\n\\n\"])\n\nquestions = []\nfor i in range(4):\n    questions.append({\"question\": f\"What is the number {i} in french?\"})\n\nanswers = gpu_chain.batch(questions)\nfor answer in answers:\n    print(answer)\n\nPrevious\nHugging Face Hub\nNext\nHuggingface TextGen Inference"
}