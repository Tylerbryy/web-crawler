{
	"title": "Facebook Messenger | ðŸ¦œï¸ðŸ”— Langchain",
	"url": "https://python.langchain.com/docs/integrations/chat_loaders/facebook",
	"html": "ComponentsChat loadersFacebook Messenger\nFacebook Messenger\n\nThis notebook shows how to load data from Facebook in a format you can fine-tune on. The overall steps are:\n\nDownload your messenger data to disk.\nCreate the Chat Loader and call loader.load() (or loader.lazy_load()) to perform the conversion.\nOptionally use merge_chat_runs to combine message from the same sender in sequence, and/or map_ai_messages to convert messages from the specified sender to the \"AIMessage\" class. Once you've done this, call convert_messages_for_finetuning to prepare your data for fine-tuning.\n\nOnce this has been done, you can fine-tune your model. To do so you would complete the following steps:\n\nUpload your messages to OpenAI and run a fine-tuning job.\nUse the resulting model in your LangChain app!\n\nLet's begin.\n\n1. Download Dataâ€‹\n\nTo download your own messenger data, following instructions here. IMPORTANT - make sure to download them in JSON format (not HTML).\n\nWe are hosting an example dump at this google drive link that we will use in this walkthrough.\n\n# This uses some example data\nimport zipfile\n\nimport requests\n\n\ndef download_and_unzip(url: str, output_path: str = \"file.zip\") -> None:\n    file_id = url.split(\"/\")[-2]\n    download_url = f\"https://drive.google.com/uc?export=download&id={file_id}\"\n\n    response = requests.get(download_url)\n    if response.status_code != 200:\n        print(\"Failed to download the file.\")\n        return\n\n    with open(output_path, \"wb\") as file:\n        file.write(response.content)\n        print(f\"File {output_path} downloaded.\")\n\n    with zipfile.ZipFile(output_path, \"r\") as zip_ref:\n        zip_ref.extractall()\n        print(f\"File {output_path} has been unzipped.\")\n\n\n# URL of the file to download\nurl = (\n    \"https://drive.google.com/file/d/1rh1s1o2i7B-Sk1v9o8KNgivLVGwJ-osV/view?usp=sharing\"\n)\n\n# Download and unzip\ndownload_and_unzip(url)\n\n    File file.zip downloaded.\n    File file.zip has been unzipped.\n\n2. Create Chat Loaderâ€‹\n\nWe have 2 different FacebookMessengerChatLoader classes, one for an entire directory of chats, and one to load individual files. We\n\ndirectory_path = \"./hogwarts\"\n\nfrom langchain.chat_loaders.facebook_messenger import (\n    FolderFacebookMessengerChatLoader,\n    SingleFileFacebookMessengerChatLoader,\n)\n\nloader = SingleFileFacebookMessengerChatLoader(\n    path=\"./hogwarts/inbox/HermioneGranger/messages_Hermione_Granger.json\",\n)\n\nchat_session = loader.load()[0]\nchat_session[\"messages\"][:3]\n\n    [HumanMessage(content=\"Hi Hermione! How's your summer going so far?\", additional_kwargs={'sender': 'Harry Potter'}, example=False),\n     HumanMessage(content=\"Harry! Lovely to hear from you. My summer is going well, though I do miss everyone. I'm spending most of my time going through my books and researching fascinating new topics. How about you?\", additional_kwargs={'sender': 'Hermione Granger'}, example=False),\n     HumanMessage(content=\"I miss you all too. The Dursleys are being their usual unpleasant selves but I'm getting by. At least I can practice some spells in my room without them knowing. Let me know if you find anything good in your researching!\", additional_kwargs={'sender': 'Harry Potter'}, example=False)]\n\nloader = FolderFacebookMessengerChatLoader(\n    path=\"./hogwarts\",\n)\n\nchat_sessions = loader.load()\nlen(chat_sessions)\n\n    9\n\n3. Prepare for fine-tuningâ€‹\n\nCalling load() returns all the chat messages we could extract as human messages. When conversing with chat bots, conversations typically follow a more strict alternating dialogue pattern relative to real conversations.\n\nYou can choose to merge message \"runs\" (consecutive messages from the same sender) and select a sender to represent the \"AI\". The fine-tuned LLM will learn to generate these AI messages.\n\nfrom langchain.chat_loaders.utils import (\n    map_ai_messages,\n    merge_chat_runs,\n)\n\nmerged_sessions = merge_chat_runs(chat_sessions)\nalternating_sessions = list(map_ai_messages(merged_sessions, \"Harry Potter\"))\n\n# Now all of Harry Potter's messages will take the AI message class\n# which maps to the 'assistant' role in OpenAI's training format\nalternating_sessions[0][\"messages\"][:3]\n\n    [AIMessage(content=\"Professor Snape, I was hoping I could speak with you for a moment about something that's been concerning me lately.\", additional_kwargs={'sender': 'Harry Potter'}, example=False),\n     HumanMessage(content=\"What is it, Potter? I'm quite busy at the moment.\", additional_kwargs={'sender': 'Severus Snape'}, example=False),\n     AIMessage(content=\"I apologize for the interruption, sir. I'll be brief. I've noticed some strange activity around the school grounds at night. I saw a cloaked figure lurking near the Forbidden Forest last night. I'm worried someone may be plotting something sinister.\", additional_kwargs={'sender': 'Harry Potter'}, example=False)]\n\nNow we can convert to OpenAI format dictionariesâ€‹\nfrom langchain.adapters.openai import convert_messages_for_finetuning\n\ntraining_data = convert_messages_for_finetuning(alternating_sessions)\nprint(f\"Prepared {len(training_data)} dialogues for training\")\n\n    Prepared 9 dialogues for training\n\ntraining_data[0][:3]\n\n    [{'role': 'assistant',\n      'content': \"Professor Snape, I was hoping I could speak with you for a moment about something that's been concerning me lately.\"},\n     {'role': 'user',\n      'content': \"What is it, Potter? I'm quite busy at the moment.\"},\n     {'role': 'assistant',\n      'content': \"I apologize for the interruption, sir. I'll be brief. I've noticed some strange activity around the school grounds at night. I saw a cloaked figure lurking near the Forbidden Forest last night. I'm worried someone may be plotting something sinister.\"}]\n\n\nOpenAI currently requires at least 10 training examples for a fine-tuning job, though they recommend between 50-100 for most tasks. Since we only have 9 chat sessions, we can subdivide them (optionally with some overlap) so that each training example is comprised of a portion of a whole conversation.\n\nFacebook chat sessions (1 per person) often span multiple days and conversations, so the long-range dependencies may not be that important to model anyhow.\n\n# Our chat is alternating, we will make each datapoint a group of 8 messages,\n# with 2 messages overlapping\nchunk_size = 8\noverlap = 2\n\ntraining_examples = [\n    conversation_messages[i : i + chunk_size]\n    for conversation_messages in training_data\n    for i in range(0, len(conversation_messages) - chunk_size + 1, chunk_size - overlap)\n]\n\nlen(training_examples)\n\n    100\n\n4. Fine-tune the modelâ€‹\n\nIt's time to fine-tune the model. Make sure you have openai installed and have set your OPENAI_API_KEY appropriately\n\n# %pip install -U openai --quiet\n\nimport json\nimport time\nfrom io import BytesIO\n\nimport openai\n\n# We will write the jsonl file in memory\nmy_file = BytesIO()\nfor m in training_examples:\n    my_file.write((json.dumps({\"messages\": m}) + \"\\n\").encode(\"utf-8\"))\n\nmy_file.seek(0)\ntraining_file = openai.File.create(file=my_file, purpose=\"fine-tune\")\n\n# OpenAI audits each training file for compliance reasons.\n# This make take a few minutes\nstatus = openai.File.retrieve(training_file.id).status\nstart_time = time.time()\nwhile status != \"processed\":\n    print(f\"Status=[{status}]... {time.time() - start_time:.2f}s\", end=\"\\r\", flush=True)\n    time.sleep(5)\n    status = openai.File.retrieve(training_file.id).status\nprint(f\"File {training_file.id} ready after {time.time() - start_time:.2f} seconds.\")\n\n    File file-zCyNBeg4snpbBL7VkvsuhCz8 ready afer 30.55 seconds.\n\n\nWith the file ready, it's time to kick off a training job.\n\njob = openai.FineTuningJob.create(\n    training_file=training_file.id,\n    model=\"gpt-3.5-turbo\",\n)\n\n\nGrab a cup of tea while your model is being prepared. This may take some time!\n\nstatus = openai.FineTuningJob.retrieve(job.id).status\nstart_time = time.time()\nwhile status != \"succeeded\":\n    print(f\"Status=[{status}]... {time.time() - start_time:.2f}s\", end=\"\\r\", flush=True)\n    time.sleep(5)\n    job = openai.FineTuningJob.retrieve(job.id)\n    status = job.status\n\n    Status=[running]... 908.87s\n\nprint(job.fine_tuned_model)\n\n    ft:gpt-3.5-turbo-0613:personal::7rDwkaOq\n\n5. Use in LangChainâ€‹\n\nYou can use the resulting model ID directly the ChatOpenAI model class.\n\nfrom langchain.chat_models import ChatOpenAI\n\nmodel = ChatOpenAI(\n    model=job.fine_tuned_model,\n    temperature=1,\n)\n\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.schema.output_parser import StrOutputParser\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"human\", \"{input}\"),\n    ]\n)\n\nchain = prompt | model | StrOutputParser()\n\nfor tok in chain.stream({\"input\": \"What classes are you taking?\"}):\n    print(tok, end=\"\", flush=True)\n\n    The usual - Potions, Transfiguration, Defense Against the Dark Arts. What about you?\n\nPrevious\nDiscord\nNext\nGMail"
}