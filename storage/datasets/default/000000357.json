{
	"title": "Ray Serve | ðŸ¦œï¸ðŸ”— Langchain",
	"url": "https://python.langchain.com/docs/integrations/providers/ray_serve",
	"html": "ProvidersMoreRay Serve\nRay Serve\n\nRay Serve is a scalable model serving library for building online inference APIs. Serve is particularly well suited for system composition, enabling you to build a complex inference service consisting of multiple chains and business logic all in Python code.\n\nGoal of this notebookâ€‹\n\nThis notebook shows a simple example of how to deploy an OpenAI chain into production. You can extend it to deploy your own self-hosted models where you can easily define amount of hardware resources (GPUs and CPUs) needed to run your model in production efficiently. Read more about available options including autoscaling in the Ray Serve documentation.\n\nSetup Ray Serveâ€‹\n\nInstall ray with pip install ray[serve].\n\nGeneral Skeletonâ€‹\n\nThe general skeleton for deploying a service is the following:\n\n# 0: Import ray serve and request from starlette\nfrom ray import serve\nfrom starlette.requests import Request\n\n\n# 1: Define a Ray Serve deployment.\n@serve.deployment\nclass LLMServe:\n    def __init__(self) -> None:\n        # All the initialization code goes here\n        pass\n\n    async def __call__(self, request: Request) -> str:\n        # You can parse the request here\n        # and return a response\n        return \"Hello World\"\n\n\n# 2: Bind the model to deployment\ndeployment = LLMServe.bind()\n\n# 3: Run the deployment\nserve.api.run(deployment)\n\n# Shutdown the deployment\nserve.api.shutdown()\n\nExample of deploying and OpenAI chain with custom promptsâ€‹\n\nGet an OpenAI API key from here. By running the following code, you will be asked to provide your API key.\n\nfrom langchain.chains import LLMChain\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\n\nfrom getpass import getpass\n\nOPENAI_API_KEY = getpass()\n\n@serve.deployment\nclass DeployLLM:\n    def __init__(self):\n        # We initialize the LLM, template and the chain here\n        llm = OpenAI(openai_api_key=OPENAI_API_KEY)\n        template = \"Question: {question}\\n\\nAnswer: Let's think step by step.\"\n        prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n        self.chain = LLMChain(llm=llm, prompt=prompt)\n\n    def _run_chain(self, text: str):\n        return self.chain(text)\n\n    async def __call__(self, request: Request):\n        # 1. Parse the request\n        text = request.query_params[\"text\"]\n        # 2. Run the chain\n        resp = self._run_chain(text)\n        # 3. Return the response\n        return resp[\"text\"]\n\n\nNow we can bind the deployment.\n\n# Bind the model to deployment\ndeployment = DeployLLM.bind()\n\n\nWe can assign the port number and host when we want to run the deployment.\n\n# Example port number\nPORT_NUMBER = 8282\n# Run the deployment\nserve.api.run(deployment, port=PORT_NUMBER)\n\n\nNow that service is deployed on port localhost:8282 we can send a post request to get the results back.\n\nimport requests\n\ntext = \"What NFL team won the Super Bowl in the year Justin Beiber was born?\"\nresponse = requests.post(f\"http://localhost:{PORT_NUMBER}/?text={text}\")\nprint(response.content.decode())\n\nPrevious\nQdrant\nNext\nRebuff"
}