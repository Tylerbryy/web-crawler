{
	"title": "Custom LLM Agent | 🦜️🔗 Langchain",
	"url": "https://python.langchain.com/docs/modules/agents/how_to/custom_llm_agent",
	"html": "ModulesAgentsHow-toCustom LLM Agent\nCustom LLM Agent\n\nThis notebook goes through how to create your own custom LLM agent.\n\nAn LLM agent consists of three parts:\n\nPromptTemplate: This is the prompt template that can be used to instruct the language model on what to do\nLLM: This is the language model that powers the agent\nstop sequence: Instructs the LLM to stop generating as soon as this string is found\nOutputParser: This determines how to parse the LLM output into an AgentAction or AgentFinish object\n\nThe LLM Agent is used in an AgentExecutor. This AgentExecutor can largely be thought of as a loop that:\n\nPasses user input and any previous steps to the Agent (in this case, the LLM Agent)\nIf the Agent returns an AgentFinish, then return that directly to the user\nIf the Agent returns an AgentAction, then use that to call a tool and get an Observation\nRepeat, passing the AgentAction and Observation back to the Agent until an AgentFinish is emitted.\n\nAgentAction is a response that consists of action and action_input. action refers to which tool to use, and action_input refers to the input to that tool. log can also be provided as more context (that can be used for logging, tracing, etc).\n\nAgentFinish is a response that contains the final message to be sent back to the user. This should be used to end an agent run.\n\nIn this notebook we walk through how to create a custom LLM agent.\n\nSet up environment​\n\nDo necessary imports, etc.\n\nfrom langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent, AgentOutputParser\nfrom langchain.prompts import StringPromptTemplate\nfrom langchain.llms import OpenAI\nfrom langchain.utilities import SerpAPIWrapper\nfrom langchain.chains import LLMChain\nfrom typing import List, Union\nfrom langchain.schema import AgentAction, AgentFinish, OutputParserException\nimport re\n\nSet up tool​\n\nSet up any tools the agent may want to use. This may be necessary to put in the prompt (so that the agent knows to use these tools).\n\n# Define which tools the agent can use to answer user queries\nsearch = SerpAPIWrapper()\ntools = [\n    Tool(\n        name=\"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\"\n    )\n]\n\nPrompt template​\n\nThis instructs the agent on what to do. Generally, the template should incorporate:\n\ntools: which tools the agent has access and how and when to call them.\nintermediate_steps: These are tuples of previous (AgentAction, Observation) pairs. These are generally not passed directly to the model, but the prompt template formats them in a specific way.\ninput: generic user input\n# Set up the base template\ntemplate = \"\"\"Answer the following questions as best you can, but speaking as a pirate might speak. You have access to the following tools:\n\n{tools}\n\nUse the following format:\n\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction: the action to take, should be one of [{tool_names}]\nAction Input: the input to the action\nObservation: the result of the action\n... (this Thought/Action/Action Input/Observation can repeat N times)\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question\n\nBegin! Remember to speak as a pirate when giving your final answer. Use lots of \"Arg\"s\n\nQuestion: {input}\n{agent_scratchpad}\"\"\"\n\n# Set up a prompt template\nclass CustomPromptTemplate(StringPromptTemplate):\n    # The template to use\n    template: str\n    # The list of tools available\n    tools: List[Tool]\n\n    def format(self, **kwargs) -> str:\n        # Get the intermediate steps (AgentAction, Observation tuples)\n        # Format them in a particular way\n        intermediate_steps = kwargs.pop(\"intermediate_steps\")\n        thoughts = \"\"\n        for action, observation in intermediate_steps:\n            thoughts += action.log\n            thoughts += f\"\\nObservation: {observation}\\nThought: \"\n        # Set the agent_scratchpad variable to that value\n        kwargs[\"agent_scratchpad\"] = thoughts\n        # Create a tools variable from the list of tools provided\n        kwargs[\"tools\"] = \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in self.tools])\n        # Create a list of tool names for the tools provided\n        kwargs[\"tool_names\"] = \", \".join([tool.name for tool in self.tools])\n        return self.template.format(**kwargs)\n\nprompt = CustomPromptTemplate(\n    template=template,\n    tools=tools,\n    # This omits the `agent_scratchpad`, `tools`, and `tool_names` variables because those are generated dynamically\n    # This includes the `intermediate_steps` variable because that is needed\n    input_variables=[\"input\", \"intermediate_steps\"]\n)\n\nOutput parser​\n\nThe output parser is responsible for parsing the LLM output into AgentAction and AgentFinish. This usually depends heavily on the prompt used.\n\nThis is where you can change the parsing to do retries, handle whitespace, etc.\n\nclass CustomOutputParser(AgentOutputParser):\n\n    def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:\n        # Check if agent should finish\n        if \"Final Answer:\" in llm_output:\n            return AgentFinish(\n                # Return values is generally always a dictionary with a single `output` key\n                # It is not recommended to try anything else at the moment :)\n                return_values={\"output\": llm_output.split(\"Final Answer:\")[-1].strip()},\n                log=llm_output,\n            )\n        # Parse out the action and action input\n        regex = r\"Action\\s*\\d*\\s*:(.*?)\\nAction\\s*\\d*\\s*Input\\s*\\d*\\s*:[\\s]*(.*)\"\n        match = re.search(regex, llm_output, re.DOTALL)\n        if not match:\n            raise OutputParserException(f\"Could not parse LLM output: `{llm_output}`\")\n        action = match.group(1).strip()\n        action_input = match.group(2)\n        # Return the action and action input\n        return AgentAction(tool=action, tool_input=action_input.strip(\" \").strip('\"'), log=llm_output)\n\noutput_parser = CustomOutputParser()\n\nSet up LLM​\n\nChoose the LLM you want to use!\n\nllm = OpenAI(temperature=0)\n\nDefine the stop sequence​\n\nThis is important because it tells the LLM when to stop generation.\n\nThis depends heavily on the prompt and model you are using. Generally, you want this to be whatever token you use in the prompt to denote the start of an Observation (otherwise, the LLM may hallucinate an observation for you).\n\nSet up the Agent​\n\nWe can now combine everything to set up our agent:\n\n# LLM chain consisting of the LLM and a prompt\nllm_chain = LLMChain(llm=llm, prompt=prompt)\n\ntool_names = [tool.name for tool in tools]\nagent = LLMSingleActionAgent(\n    llm_chain=llm_chain,\n    output_parser=output_parser,\n    stop=[\"\\nObservation:\"],\n    allowed_tools=tool_names\n)\n\nUse the Agent​\n\nNow we can use it!\n\nagent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True)\n\nagent_executor.run(\"How many people live in canada as of 2023?\")\n\n\n\n    > Entering new AgentExecutor chain...\n    Thought: I need to find out the population of Canada in 2023\n    Action: Search\n    Action Input: Population of Canada in 2023\n\n    Observation:The current population of Canada is 38,658,314 as of Wednesday, April 12, 2023, based on Worldometer elaboration of the latest United Nations data. I now know the final answer\n    Final Answer: Arrr, there be 38,658,314 people livin' in Canada as of 2023!\n\n    > Finished chain.\n\n\n\n\n\n    \"Arrr, there be 38,658,314 people livin' in Canada as of 2023!\"\n\nAdding Memory​\n\nIf you want to add memory to the agent, you'll need to:\n\nAdd a place in the custom prompt for the chat_history\nAdd a memory object to the agent executor.\n# Set up the base template\ntemplate_with_history = \"\"\"Answer the following questions as best you can, but speaking as a pirate might speak. You have access to the following tools:\n\n{tools}\n\nUse the following format:\n\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction: the action to take, should be one of [{tool_names}]\nAction Input: the input to the action\nObservation: the result of the action\n... (this Thought/Action/Action Input/Observation can repeat N times)\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question\n\nBegin! Remember to speak as a pirate when giving your final answer. Use lots of \"Arg\"s\n\nPrevious conversation history:\n{history}\n\nNew question: {input}\n{agent_scratchpad}\"\"\"\n\nprompt_with_history = CustomPromptTemplate(\n    template=template_with_history,\n    tools=tools,\n    # This omits the `agent_scratchpad`, `tools`, and `tool_names` variables because those are generated dynamically\n    # This includes the `intermediate_steps` variable because that is needed\n    input_variables=[\"input\", \"intermediate_steps\", \"history\"]\n)\n\nllm_chain = LLMChain(llm=llm, prompt=prompt_with_history)\n\ntool_names = [tool.name for tool in tools]\nagent = LLMSingleActionAgent(\n    llm_chain=llm_chain,\n    output_parser=output_parser,\n    stop=[\"\\nObservation:\"],\n    allowed_tools=tool_names\n)\n\nfrom langchain.memory import ConversationBufferWindowMemory\n\nmemory=ConversationBufferWindowMemory(k=2)\n\nagent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True, memory=memory)\n\nagent_executor.run(\"How many people live in canada as of 2023?\")\n\n\n\n    > Entering new AgentExecutor chain...\n    Thought: I need to find out the population of Canada in 2023\n    Action: Search\n    Action Input: Population of Canada in 2023\n\n    Observation:The current population of Canada is 38,658,314 as of Wednesday, April 12, 2023, based on Worldometer elaboration of the latest United Nations data. I now know the final answer\n    Final Answer: Arrr, there be 38,658,314 people livin' in Canada as of 2023!\n\n    > Finished chain.\n\n\n\n\n\n    \"Arrr, there be 38,658,314 people livin' in Canada as of 2023!\"\n\nagent_executor.run(\"how about in mexico?\")\n\n\n\n    > Entering new AgentExecutor chain...\n    Thought: I need to find out how many people live in Mexico.\n    Action: Search\n    Action Input: How many people live in Mexico as of 2023?\n\n    Observation:The current population of Mexico is 132,679,922 as of Tuesday, April 11, 2023, based on Worldometer elaboration of the latest United Nations data. Mexico 2020 ... I now know the final answer.\n    Final Answer: Arrr, there be 132,679,922 people livin' in Mexico as of 2023!\n\n    > Finished chain.\n\n\n\n\n\n    \"Arrr, there be 132,679,922 people livin' in Mexico as of 2023!\"\n\nPrevious\nCustom agent with tool retrieval\nNext\nCustom LLM Chat Agent"
}