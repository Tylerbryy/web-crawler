{
	"title": "GPT4All | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/integrations/providers/gpt4all",
	"html": "ProvidersMoreGPT4All\nGPT4All\n\nThis page covers how to use the GPT4All wrapper within LangChain. The tutorial is divided into two parts: installation and setup, followed by usage with an example.\n\nInstallation and Setup‚Äã\nInstall the Python package with pip install pyllamacpp\nDownload a GPT4All model and place it in your desired directory\nUsage‚Äã\nGPT4All‚Äã\n\nTo use the GPT4All wrapper, you need to provide the path to the pre-trained model file and the model's configuration.\n\nfrom langchain.llms import GPT4All\n\n# Instantiate the model. Callbacks support token-wise streaming\nmodel = GPT4All(model=\"./models/gpt4all-model.bin\", n_ctx=512, n_threads=8)\n\n# Generate text\nresponse = model(\"Once upon a time, \")\n\n\nYou can also customize the generation parameters, such as n_predict, temp, top_p, top_k, and others.\n\nTo stream the model's predictions, add in a CallbackManager.\n\nfrom langchain.llms import GPT4All\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n\n# There are many CallbackHandlers supported, such as\n# from langchain.callbacks.streamlit import StreamlitCallbackHandler\n\ncallbacks = [StreamingStdOutCallbackHandler()]\nmodel = GPT4All(model=\"./models/gpt4all-model.bin\", n_ctx=512, n_threads=8)\n\n# Generate text. Tokens are streamed through the callback manager.\nmodel(\"Once upon a time, \", callbacks=callbacks)\n\nModel File‚Äã\n\nYou can find links to model file downloads in the pyllamacpp repository.\n\nFor a more detailed walkthrough of this, see this notebook\n\nPrevious\nGooseAI\nNext\nGradient"
}