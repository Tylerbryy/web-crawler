{
	"title": "TextGen | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/integrations/llms/textgen",
	"html": "ComponentsLLMsTextGen\nTextGen\n\nGitHub:oobabooga/text-generation-webui A gradio web UI for running Large Language Models like LLaMA, llama.cpp, GPT-J, Pythia, OPT, and GALACTICA.\n\nThis example goes over how to use LangChain to interact with LLM models via the text-generation-webui API integration.\n\nPlease ensure that you have text-generation-webui configured and an LLM installed. Recommended installation via the one-click installer appropriate for your OS.\n\nOnce text-generation-webui is installed and confirmed working via the web interface, please enable the api option either through the web model configuration tab, or by adding the run-time arg --api to your start command.\n\nSet model_url and run the example‚Äã\nmodel_url = \"http://localhost:5000\"\n\nfrom langchain.chains import LLMChain\nfrom langchain.globals import set_debug\nfrom langchain.llms import TextGen\nfrom langchain.prompts import PromptTemplate\n\nset_debug(True)\n\ntemplate = \"\"\"Question: {question}\n\nAnswer: Let's think step by step.\"\"\"\n\n\nprompt = PromptTemplate(template=template, input_variables=[\"question\"])\nllm = TextGen(model_url=model_url)\nllm_chain = LLMChain(prompt=prompt, llm=llm)\nquestion = \"What NFL team won the Super Bowl in the year Justin Bieber was born?\"\n\nllm_chain.run(question)\n\nStreaming Version‚Äã\n\nYou should install websocket-client to use this feature. pip install websocket-client\n\nmodel_url = \"ws://localhost:5005\"\n\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\nfrom langchain.chains import LLMChain\nfrom langchain.globals import set_debug\nfrom langchain.llms import TextGen\nfrom langchain.prompts import PromptTemplate\n\nset_debug(True)\n\ntemplate = \"\"\"Question: {question}\n\nAnswer: Let's think step by step.\"\"\"\n\n\nprompt = PromptTemplate(template=template, input_variables=[\"question\"])\nllm = TextGen(\n    model_url=model_url, streaming=True, callbacks=[StreamingStdOutCallbackHandler()]\n)\nllm_chain = LLMChain(prompt=prompt, llm=llm)\nquestion = \"What NFL team won the Super Bowl in the year Justin Bieber was born?\"\n\nllm_chain.run(question)\n\nllm = TextGen(model_url=model_url, streaming=True)\nfor chunk in llm.stream(\"Ask 'Hi, how are you?' like a pirate:'\", stop=[\"'\", \"\\n\"]):\n    print(chunk, end=\"\", flush=True)\n\nPrevious\nNebula (Symbl.ai)\nNext\nTitan Takeoff"
}