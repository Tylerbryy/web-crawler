{
	"title": "Shale Protocol | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/integrations/providers/shaleprotocol",
	"html": "ProvidersMoreShale Protocol\nShale Protocol\n\nShale Protocol provides production-ready inference APIs for open LLMs. It's a Plug & Play API as it's hosted on a highly scalable GPU cloud infrastructure.\n\nOur free tier supports up to 1K daily requests per key as we want to eliminate the barrier for anyone to start building genAI apps with LLMs.\n\nWith Shale Protocol, developers/researchers can create apps and explore the capabilities of open LLMs at no cost.\n\nThis page covers how Shale-Serve API can be incorporated with LangChain.\n\nAs of June 2023, the API supports Vicuna-13B by default. We are going to support more LLMs such as Falcon-40B in future releases.\n\nHow to‚Äã\n1. Find the link to our Discord on https://shaleprotocol.com. Generate an API key through the \"Shale Bot\" on our Discord. No credit card is required and no free trials. It's a forever free tier with 1K limit per day per API key.‚Äã\n2. Use https://shale.live/v1 as OpenAI API drop-in replacement‚Äã\n\nFor example\n\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains import LLMChain\n\nimport os\nos.environ['OPENAI_API_BASE'] = \"https://shale.live/v1\"\nos.environ['OPENAI_API_KEY'] = \"ENTER YOUR API KEY\"\n\nllm = OpenAI()\n\ntemplate = \"\"\"Question: {question}\n\n# Answer: Let's think step by step.\"\"\"\n\nprompt = PromptTemplate(template=template, input_variables=[\"question\"])\n\nllm_chain = LLMChain(prompt=prompt, llm=llm)\n\nquestion = \"What NFL team won the Super Bowl in the year Justin Beiber was born?\"\n\nllm_chain.run(question)\n\n\nPrevious\nSerpAPI\nNext\nSingleStoreDB"
}