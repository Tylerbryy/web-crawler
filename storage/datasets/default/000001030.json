{
	"title": "LangSmith Chat Datasets | ðŸ¦œï¸ðŸ”— Langchain",
	"url": "https://python.langchain.com/docs/integrations/chat_loaders/langsmith_dataset",
	"html": "ComponentsChat loadersLangSmith Chat Datasets\nLangSmith Chat Datasets\n\nThis notebook demonstrates an easy way to load a LangSmith chat dataset fine-tune a model on that data. The process is simple and comprises 3 steps.\n\nCreate the chat dataset.\nUse the LangSmithDatasetChatLoader to load examples.\nFine-tune your model.\n\nThen you can use the fine-tuned model in your LangChain app.\n\nBefore diving in, let's install our prerequisites.\n\nPrerequisitesâ€‹\n\nEnsure you've installed langchain >= 0.0.311 and have configured your environment with your LangSmith API key.\n\n%pip install -U langchain openai\n\nimport os\nimport uuid\n\nuid = uuid.uuid4().hex[:6]\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\nos.environ[\"LANGCHAIN_API_KEY\"] = \"YOUR API KEY\"\n\n1. Select a datasetâ€‹\n\nThis notebook fine-tunes a model directly on selecting which runs to fine-tune on. You will often curate these from traced runs. You can learn more about LangSmith datasets in the docs docs.\n\nFor the sake of this tutorial, we will upload an existing dataset here that you can use.\n\nfrom langsmith.client import Client\n\nclient = Client()\n\nimport requests\n\nurl = \"https://raw.githubusercontent.com/langchain-ai/langchain/master/docs/docs/integrations/chat_loaders/example_data/langsmith_chat_dataset.json\"\nresponse = requests.get(url)\nresponse.raise_for_status()\ndata = response.json()\n\ndataset_name = f\"Extraction Fine-tuning Dataset {uid}\"\nds = client.create_dataset(dataset_name=dataset_name, data_type=\"chat\")\n\n_ = client.create_examples(\n    inputs=[e[\"inputs\"] for e in data],\n    outputs=[e[\"outputs\"] for e in data],\n    dataset_id=ds.id,\n)\n\n2. Prepare Dataâ€‹\n\nNow we can create an instance of LangSmithRunChatLoader and load the chat sessions using its lazy_load() method.\n\nfrom langchain.chat_loaders.langsmith import LangSmithDatasetChatLoader\n\nloader = LangSmithDatasetChatLoader(dataset_name=dataset_name)\n\nchat_sessions = loader.lazy_load()\n\nWith the chat sessions loaded, convert them into a format suitable for fine-tuning.â€‹\nfrom langchain.adapters.openai import convert_messages_for_finetuning\n\ntraining_data = convert_messages_for_finetuning(chat_sessions)\n\n3. Fine-tune the Modelâ€‹\n\nNow, initiate the fine-tuning process using the OpenAI library.\n\nimport json\nimport time\nfrom io import BytesIO\n\nimport openai\n\nmy_file = BytesIO()\nfor dialog in training_data:\n    my_file.write((json.dumps({\"messages\": dialog}) + \"\\n\").encode(\"utf-8\"))\n\nmy_file.seek(0)\ntraining_file = openai.File.create(file=my_file, purpose=\"fine-tune\")\n\njob = openai.FineTuningJob.create(\n    training_file=training_file.id,\n    model=\"gpt-3.5-turbo\",\n)\n\n# Wait for the fine-tuning to complete (this may take some time)\nstatus = openai.FineTuningJob.retrieve(job.id).status\nstart_time = time.time()\nwhile status != \"succeeded\":\n    print(f\"Status=[{status}]... {time.time() - start_time:.2f}s\", end=\"\\r\", flush=True)\n    time.sleep(5)\n    status = openai.FineTuningJob.retrieve(job.id).status\n\n# Now your model is fine-tuned!\n\n    Status=[running]... 302.42s. 143.85s\n\n4. Use in LangChainâ€‹\n\nAfter fine-tuning, use the resulting model ID with the ChatOpenAI model class in your LangChain app.\n\n# Get the fine-tuned model ID\njob = openai.FineTuningJob.retrieve(job.id)\nmodel_id = job.fine_tuned_model\n\n# Use the fine-tuned model in LangChain\nmodel = ChatOpenAI(\n    model=model_id,\n    temperature=1,\n)\n\nmodel.invoke(\"There were three ravens sat on a tree.\")\n\n\nNow you have successfully fine-tuned a model using data from LangSmith LLM runs!\n\nPrevious\niMessage\nNext\nLangSmith LLM Runs"
}