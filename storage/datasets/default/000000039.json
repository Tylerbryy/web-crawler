{
	"title": "Evaluation | 🦜️🔗 Langchain",
	"url": "https://python.langchain.com/docs/guides/evaluation/",
	"html": "Evaluation\nEvaluation\n\nBuilding applications with language models involves many moving parts. One of the most critical components is ensuring that the outcomes produced by your models are reliable and useful across a broad array of inputs, and that they work well with your application's other software components. Ensuring reliability usually boils down to some combination of application design, testing & evaluation, and runtime checks.\n\nThe guides in this section review the APIs and functionality LangChain provides to help you better evaluate your applications. Evaluation and testing are both critical when thinking about deploying LLM applications, since production environments require repeatable and useful outcomes.\n\nLangChain offers various types of evaluators to help you measure performance and integrity on diverse data, and we hope to encourage the community to create and share other useful evaluators so everyone can improve. These docs will introduce the evaluator types, how to use them, and provide some examples of their use in real-world scenarios.\n\nEach evaluator type in LangChain comes with ready-to-use implementations and an extensible API that allows for customization according to your unique requirements. Here are some of the types of evaluators we offer:\n\nString Evaluators: These evaluators assess the predicted string for a given input, usually comparing it against a reference string.\nTrajectory Evaluators: These are used to evaluate the entire trajectory of agent actions.\nComparison Evaluators: These evaluators are designed to compare predictions from two runs on a common input.\n\nThese evaluators can be used across various scenarios and can be applied to different chain and LLM implementations in the LangChain library.\n\nWe also are working to share guides and cookbooks that demonstrate how to use these evaluators in real-world scenarios, such as:\n\nChain Comparisons: This example uses a comparison evaluator to predict the preferred output. It reviews ways to measure confidence intervals to select statistically significant differences in aggregate preference scores across different models or prompts.\nReference Docs​\n\nFor detailed information on the available evaluators, including how to instantiate, configure, and customize them, check out the reference documentation directly.\n\n🗃️ String Evaluators\n\n8 items\n\n🗃️ Comparison Evaluators\n\n3 items\n\n🗃️ Trajectory Evaluators\n\n2 items\n\n🗃️ Examples\n\n1 items\n\nPrevious\nLangChain Templates\nNext\nString Evaluators"
}