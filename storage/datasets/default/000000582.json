{
	"title": "vLLM | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/integrations/llms/vllm",
	"html": "ComponentsLLMsvLLM\nvLLM\n\nvLLM is a fast and easy-to-use library for LLM inference and serving, offering:\n\nState-of-the-art serving throughput\nEfficient management of attention key and value memory with PagedAttention\nContinuous batching of incoming requests\nOptimized CUDA kernels\n\nThis notebooks goes over how to use a LLM with langchain and vLLM.\n\nTo use, you should have the vllm python package installed.\n\n#!pip install vllm -q\n\nfrom langchain.llms import VLLM\n\nllm = VLLM(\n    model=\"mosaicml/mpt-7b\",\n    trust_remote_code=True,  # mandatory for hf models\n    max_new_tokens=128,\n    top_k=10,\n    top_p=0.95,\n    temperature=0.8,\n)\n\nprint(llm(\"What is the capital of France ?\"))\n\n    INFO 08-06 11:37:33 llm_engine.py:70] Initializing an LLM engine with config: model='mosaicml/mpt-7b', tokenizer='mosaicml/mpt-7b', tokenizer_mode=auto, trust_remote_code=True, dtype=torch.bfloat16, use_dummy_weights=False, download_dir=None, use_np_weights=False, tensor_parallel_size=1, seed=0)\n    INFO 08-06 11:37:41 llm_engine.py:196] # GPU blocks: 861, # CPU blocks: 512\n\n\n    Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.00it/s]\n\n    \n    What is the capital of France ? The capital of France is Paris.\n\n\n    \n\nIntegrate the model in an LLMChain‚Äã\nfrom langchain.chains import LLMChain\nfrom langchain.prompts import PromptTemplate\n\ntemplate = \"\"\"Question: {question}\n\nAnswer: Let's think step by step.\"\"\"\nprompt = PromptTemplate(template=template, input_variables=[\"question\"])\n\nllm_chain = LLMChain(prompt=prompt, llm=llm)\n\nquestion = \"Who was the US president in the year the first Pokemon game was released?\"\n\nprint(llm_chain.run(question))\n\n    Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.34s/it]\n\n    \n    \n    1. The first Pokemon game was released in 1996.\n    2. The president was Bill Clinton.\n    3. Clinton was president from 1993 to 2001.\n    4. The answer is Clinton.\n    \n\n\n    \n\nDistributed Inference‚Äã\n\nvLLM supports distributed tensor-parallel inference and serving.\n\nTo run multi-GPU inference with the LLM class, set the tensor_parallel_size argument to the number of GPUs you want to use. For example, to run inference on 4 GPUs\n\nfrom langchain.llms import VLLM\n\nllm = VLLM(\n    model=\"mosaicml/mpt-30b\",\n    tensor_parallel_size=4,\n    trust_remote_code=True,  # mandatory for hf models\n)\n\nllm(\"What is the future of AI?\")\n\nOpenAI-Compatible Server‚Äã\n\nvLLM can be deployed as a server that mimics the OpenAI API protocol. This allows vLLM to be used as a drop-in replacement for applications using OpenAI API.\n\nThis server can be queried in the same format as OpenAI API.\n\nOpenAI-Compatible Completion‚Äã\nfrom langchain.llms import VLLMOpenAI\n\nllm = VLLMOpenAI(\n    openai_api_key=\"EMPTY\",\n    openai_api_base=\"http://localhost:8000/v1\",\n    model_name=\"tiiuae/falcon-7b\",\n    model_kwargs={\"stop\": [\".\"]},\n)\nprint(llm(\"Rome is\"))\n\n     a city that is filled with history, ancient buildings, and art around every corner\n\nPrevious\nTongyi Qwen\nNext\nWriter"
}