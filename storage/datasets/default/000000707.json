{
	"title": "Log10 | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/integrations/providers/log10",
	"html": "ProvidersMoreLog10\nLog10\n\nThis page covers how to use the Log10 within LangChain.\n\nWhat is Log10?‚Äã\n\nLog10 is an open-source proxiless LLM data management and application development platform that lets you log, debug and tag your Langchain calls.\n\nQuick start‚Äã\nCreate your free account at log10.io\nAdd your LOG10_TOKEN and LOG10_ORG_ID from the Settings and Organization tabs respectively as environment variables.\nAlso add LOG10_URL=https://log10.io and your usual LLM API key: for e.g. OPENAI_API_KEY or ANTHROPIC_API_KEY to your environment\nHow to enable Log10 data management for Langchain‚Äã\n\nIntegration with log10 is a simple one-line log10_callback integration as shown below:\n\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.schema import HumanMessage\n\nfrom log10.langchain import Log10Callback\nfrom log10.llm import Log10Config\n\nlog10_callback = Log10Callback(log10_config=Log10Config())\n\nmessages = [\n    HumanMessage(content=\"You are a ping pong machine\"),\n    HumanMessage(content=\"Ping?\"),\n]\n\nllm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", callbacks=[log10_callback])\n\n\nLog10 + Langchain + Logs docs\n\nMore details + screenshots including instructions for self-hosting logs\n\nHow to use tags with Log10‚Äã\nfrom langchain.llms import OpenAI\nfrom langchain.chat_models import ChatAnthropic\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.schema import HumanMessage\n\nfrom log10.langchain import Log10Callback\nfrom log10.llm import Log10Config\n\nlog10_callback = Log10Callback(log10_config=Log10Config())\n\nmessages = [\n    HumanMessage(content=\"You are a ping pong machine\"),\n    HumanMessage(content=\"Ping?\"),\n]\n\nllm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", callbacks=[log10_callback], temperature=0.5, tags=[\"test\"])\ncompletion = llm.predict_messages(messages, tags=[\"foobar\"])\nprint(completion)\n\nllm = ChatAnthropic(model=\"claude-2\", callbacks=[log10_callback], temperature=0.7, tags=[\"baz\"])\nllm.predict_messages(messages)\nprint(completion)\n\nllm = OpenAI(model_name=\"text-davinci-003\", callbacks=[log10_callback], temperature=0.5)\ncompletion = llm.predict(\"You are a ping pong machine.\\nPing?\\n\")\nprint(completion)\n\n\nYou can also intermix direct OpenAI calls and Langchain LLM calls:\n\nimport os\nfrom log10.load import log10, log10_session\nimport openai\nfrom langchain.llms import OpenAI\n\nlog10(openai)\n\nwith log10_session(tags=[\"foo\", \"bar\"]):\n    # Log a direct OpenAI call\n    response = openai.Completion.create(\n        model=\"text-ada-001\",\n        prompt=\"Where is the Eiffel Tower?\",\n        temperature=0,\n        max_tokens=1024,\n        top_p=1,\n        frequency_penalty=0,\n        presence_penalty=0,\n    )\n    print(response)\n\n    # Log a call via Langchain\n    llm = OpenAI(model_name=\"text-ada-001\", temperature=0.5)\n    response = llm.predict(\"You are a ping pong machine.\\nPing?\\n\")\n    print(response)\n\nHow to debug Langchain calls‚Äã\n\nExample of debugging\n\nMore Langchain examples\n\nPrevious\nLlama.cpp\nNext\nMarqo"
}