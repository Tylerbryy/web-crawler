{
	"title": "FastEmbed by Qdrant | 🦜️🔗 Langchain",
	"url": "https://python.langchain.com/docs/integrations/text_embedding/fastembed",
	"html": "ComponentsText embedding modelsFastEmbed by Qdrant\nFastEmbed by Qdrant\n\nFastEmbed from Qdrant is a lightweight, fast, Python library built for embedding generation.\n\nQuantized model weights\nONNX Runtime, no PyTorch dependency\nCPU-first design\nData-parallelism for encoding of large datasets.\nDependencies​\n\nTo use FastEmbed with LangChain, install the fastembed Python package.\n\n%pip install fastembed\n\nImports​\nfrom langchain.embeddings.fastembed import FastEmbedEmbeddings\n\nInstantiating FastEmbed​\nParameters​\n\nmodel_name: str (default: \"BAAI/bge-small-en-v1.5\")\n\nName of the FastEmbedding model to use. You can find the list of supported models here.\n\nmax_length: int (default: 512)\n\nThe maximum number of tokens. Unknown behavior for values > 512.\n\ncache_dir: Optional[str]\n\nThe path to the cache directory. Defaults to local_cache in the parent directory.\n\nthreads: Optional[int]\n\nThe number of threads a single onnxruntime session can use. Defaults to None.\n\ndoc_embed_type: Literal[\"default\", \"passage\"] (default: \"default\")\n\n\"default\": Uses FastEmbed's default embedding method.\n\n\"passage\": Prefixes the text with \"passage\" before embedding.\n\nembeddings = FastEmbedEmbeddings()\n\nUsage​\nGenerating document embeddings​\ndocument_embeddings = embeddings.embed_documents(\n    [\"This is a document\", \"This is some other document\"]\n)\n\nGenerating query embeddings​\nquery_embeddings = embeddings.embed_query(\"This is a query\")\n\nPrevious\nFake Embeddings\nNext\nGoogle Vertex AI PaLM"
}