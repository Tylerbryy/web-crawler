{
	"title": "Beam | 🦜️🔗 Langchain",
	"url": "https://python.langchain.com/docs/integrations/providers/beam",
	"html": "ProvidersMoreBeam\nBeam\n\nThis page covers how to use Beam within LangChain. It is broken into two parts: installation and setup, and then references to specific Beam wrappers.\n\nInstallation and Setup​\nCreate an account\nInstall the Beam CLI with curl https://raw.githubusercontent.com/slai-labs/get-beam/main/get-beam.sh -sSfL | sh\nRegister API keys with beam configure\nSet environment variables (BEAM_CLIENT_ID) and (BEAM_CLIENT_SECRET)\nInstall the Beam SDK pip install beam-sdk\nWrappers​\nLLM​\n\nThere exists a Beam LLM wrapper, which you can access with\n\nfrom langchain.llms.beam import Beam\n\nDefine your Beam app.​\n\nThis is the environment you’ll be developing against once you start the app. It's also used to define the maximum response length from the model.\n\nllm = Beam(model_name=\"gpt2\",\n           name=\"langchain-gpt2-test\",\n           cpu=8,\n           memory=\"32Gi\",\n           gpu=\"A10G\",\n           python_version=\"python3.8\",\n           python_packages=[\n               \"diffusers[torch]>=0.10\",\n               \"transformers\",\n               \"torch\",\n               \"pillow\",\n               \"accelerate\",\n               \"safetensors\",\n               \"xformers\",],\n           max_length=\"50\",\n           verbose=False)\n\nDeploy your Beam app​\n\nOnce defined, you can deploy your Beam app by calling your model's _deploy() method.\n\nllm._deploy()\n\nCall your Beam app​\n\nOnce a beam model is deployed, it can be called by callying your model's _call() method. This returns the GPT2 text response to your prompt.\n\nresponse = llm._call(\"Running machine learning on a remote GPU\")\n\n\nAn example script which deploys the model and calls it would be:\n\nfrom langchain.llms.beam import Beam\nimport time\n\nllm = Beam(model_name=\"gpt2\",\n           name=\"langchain-gpt2-test\",\n           cpu=8,\n           memory=\"32Gi\",\n           gpu=\"A10G\",\n           python_version=\"python3.8\",\n           python_packages=[\n               \"diffusers[torch]>=0.10\",\n               \"transformers\",\n               \"torch\",\n               \"pillow\",\n               \"accelerate\",\n               \"safetensors\",\n               \"xformers\",],\n           max_length=\"50\",\n           verbose=False)\n\nllm._deploy()\n\nresponse = llm._call(\"Running machine learning on a remote GPU\")\n\nprint(response)\n\nPrevious\nBaseten\nNext\nBeautiful Soup"
}