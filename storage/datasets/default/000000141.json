{
	"title": "Callbacks | 🦜️🔗 Langchain",
	"url": "https://python.langchain.com/docs/modules/callbacks/",
	"html": "ModulesMoreCallbacks\nCallbacks\nINFO\n\nHead to Integrations for documentation on built-in callbacks integrations with 3rd-party tools.\n\nLangChain provides a callbacks system that allows you to hook into the various stages of your LLM application. This is useful for logging, monitoring, streaming, and other tasks.\n\nYou can subscribe to these events by using the callbacks argument available throughout the API. This argument is list of handler objects, which are expected to implement one or more of the methods described below in more detail.\n\nCallback handlers​\n\nCallbackHandlers are objects that implement the CallbackHandler interface, which has a method for each event that can be subscribed to. The CallbackManager will call the appropriate method on each handler when the event is triggered.\n\nclass BaseCallbackHandler:\n    \"\"\"Base callback handler that can be used to handle callbacks from langchain.\"\"\"\n\n    def on_llm_start(\n        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any\n    ) -> Any:\n        \"\"\"Run when LLM starts running.\"\"\"\n\n    def on_chat_model_start(\n        self, serialized: Dict[str, Any], messages: List[List[BaseMessage]], **kwargs: Any\n    ) -> Any:\n        \"\"\"Run when Chat Model starts running.\"\"\"\n\n    def on_llm_new_token(self, token: str, **kwargs: Any) -> Any:\n        \"\"\"Run on new LLM token. Only available when streaming is enabled.\"\"\"\n\n    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> Any:\n        \"\"\"Run when LLM ends running.\"\"\"\n\n    def on_llm_error(\n        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n    ) -> Any:\n        \"\"\"Run when LLM errors.\"\"\"\n\n    def on_chain_start(\n        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any\n    ) -> Any:\n        \"\"\"Run when chain starts running.\"\"\"\n\n    def on_chain_end(self, outputs: Dict[str, Any], **kwargs: Any) -> Any:\n        \"\"\"Run when chain ends running.\"\"\"\n\n    def on_chain_error(\n        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n    ) -> Any:\n        \"\"\"Run when chain errors.\"\"\"\n\n    def on_tool_start(\n        self, serialized: Dict[str, Any], input_str: str, **kwargs: Any\n    ) -> Any:\n        \"\"\"Run when tool starts running.\"\"\"\n\n    def on_tool_end(self, output: str, **kwargs: Any) -> Any:\n        \"\"\"Run when tool ends running.\"\"\"\n\n    def on_tool_error(\n        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n    ) -> Any:\n        \"\"\"Run when tool errors.\"\"\"\n\n    def on_text(self, text: str, **kwargs: Any) -> Any:\n        \"\"\"Run on arbitrary text.\"\"\"\n\n    def on_agent_action(self, action: AgentAction, **kwargs: Any) -> Any:\n        \"\"\"Run on agent action.\"\"\"\n\n    def on_agent_finish(self, finish: AgentFinish, **kwargs: Any) -> Any:\n        \"\"\"Run on agent end.\"\"\"\n\nGet started​\n\nLangChain provides a few built-in handlers that you can use to get started. These are available in the langchain/callbacks module. The most basic handler is the StdOutCallbackHandler, which simply logs all events to stdout.\n\nNote: when the verbose flag on the object is set to true, the StdOutCallbackHandler will be invoked even without being explicitly passed in.\n\nfrom langchain.callbacks import StdOutCallbackHandler\nfrom langchain.chains import LLMChain\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\n\nhandler = StdOutCallbackHandler()\nllm = OpenAI()\nprompt = PromptTemplate.from_template(\"1 + {number} = \")\n\n# Constructor callback: First, let's explicitly set the StdOutCallbackHandler when initializing our chain\nchain = LLMChain(llm=llm, prompt=prompt, callbacks=[handler])\nchain.run(number=2)\n\n# Use verbose flag: Then, let's use the `verbose` flag to achieve the same result\nchain = LLMChain(llm=llm, prompt=prompt, verbose=True)\nchain.run(number=2)\n\n# Request callbacks: Finally, let's use the request `callbacks` to achieve the same result\nchain = LLMChain(llm=llm, prompt=prompt)\nchain.run(number=2, callbacks=[handler])\n\n    > Entering new LLMChain chain...\n    Prompt after formatting:\n    1 + 2 =\n\n    > Finished chain.\n\n\n    > Entering new LLMChain chain...\n    Prompt after formatting:\n    1 + 2 =\n\n    > Finished chain.\n\n\n    > Entering new LLMChain chain...\n    Prompt after formatting:\n    1 + 2 =\n\n    > Finished chain.\n\n\n    '\\n\\n3'\n\nWhere to pass in callbacks​\n\nThe callbacks argument is available on most objects throughout the API (Chains, Models, Tools, Agents, etc.) in two different places:\n\nConstructor callbacks: defined in the constructor, e.g. LLMChain(callbacks=[handler], tags=['a-tag']), which will be used for all calls made on that object, and will be scoped to that object only, e.g. if you pass a handler to the LLMChain constructor, it will not be used by the Model attached to that chain.\nRequest callbacks: defined in the run()/apply() methods used for issuing a request, e.g. chain.run(input, callbacks=[handler]), which will be used for that specific request only, and all sub-requests that it contains (e.g. a call to an LLMChain triggers a call to a Model, which uses the same handler passed in the call() method).\n\nThe verbose argument is available on most objects throughout the API (Chains, Models, Tools, Agents, etc.) as a constructor argument, e.g. LLMChain(verbose=True), and it is equivalent to passing a ConsoleCallbackHandler to the callbacks argument of that object and all child objects. This is useful for debugging, as it will log all events to the console.\n\nWhen do you want to use each of these?​\nConstructor callbacks are most useful for use cases such as logging, monitoring, etc., which are not specific to a single request, but rather to the entire chain. For example, if you want to log all the requests made to an LLMChain, you would pass a handler to the constructor.\nRequest callbacks are most useful for use cases such as streaming, where you want to stream the output of a single request to a specific websocket connection, or other similar use cases. For example, if you want to stream the output of a single request to a websocket, you would pass a handler to the call() method\nPrevious\nMultiple Memory classes\nNext\nCallbacks"
}