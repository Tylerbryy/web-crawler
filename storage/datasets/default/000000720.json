{
	"title": "Custom String Evaluator | 🦜️🔗 Langchain",
	"url": "https://python.langchain.com/docs/guides/evaluation/string/custom",
	"html": "EvaluationString EvaluatorsCustom String Evaluator\nCustom String Evaluator\n\nYou can make your own custom string evaluators by inheriting from the StringEvaluator class and implementing the _evaluate_strings (and _aevaluate_strings for async support) methods.\n\nIn this example, you will create a perplexity evaluator using the HuggingFace evaluate library. Perplexity is a measure of how well the generated text would be predicted by the model used to compute the metric.\n\n# %pip install evaluate > /dev/null\n\nfrom typing import Any, Optional\n\nfrom evaluate import load\nfrom langchain.evaluation import StringEvaluator\n\n\nclass PerplexityEvaluator(StringEvaluator):\n    \"\"\"Evaluate the perplexity of a predicted string.\"\"\"\n\n    def __init__(self, model_id: str = \"gpt2\"):\n        self.model_id = model_id\n        self.metric_fn = load(\n            \"perplexity\", module_type=\"metric\", model_id=self.model_id, pad_token=0\n        )\n\n    def _evaluate_strings(\n        self,\n        *,\n        prediction: str,\n        reference: Optional[str] = None,\n        input: Optional[str] = None,\n        **kwargs: Any,\n    ) -> dict:\n        results = self.metric_fn.compute(\n            predictions=[prediction], model_id=self.model_id\n        )\n        ppl = results[\"perplexities\"][0]\n        return {\"score\": ppl}\n\nevaluator = PerplexityEvaluator()\n\nevaluator.evaluate_strings(prediction=\"The rains in Spain fall mainly on the plain.\")\n\n    Using pad_token, but it is not set yet.\n\n\n    huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n    To disable this warning, you can either:\n        - Avoid using `tokenizers` before the fork if possible\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\n\n\n      0%|          | 0/1 [00:00<?, ?it/s]\n\n\n\n\n\n    {'score': 190.3675537109375}\n\n# The perplexity is much higher since LangChain was introduced after 'gpt-2' was released and because it is never used in the following context.\nevaluator.evaluate_strings(prediction=\"The rains in Spain fall mainly on LangChain.\")\n\n    Using pad_token, but it is not set yet.\n\n\n\n      0%|          | 0/1 [00:00<?, ?it/s]\n\n\n\n\n\n    {'score': 1982.0709228515625}\n\nPrevious\nCriteria Evaluation\nNext\nEmbedding Distance"
}