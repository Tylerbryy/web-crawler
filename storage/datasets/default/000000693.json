{
	"title": "Hugging Face | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/integrations/providers/huggingface",
	"html": "ProvidersMoreHugging Face\nHugging Face\n\nThis page covers how to use the Hugging Face ecosystem (including the Hugging Face Hub) within LangChain. It is broken into two parts: installation and setup, and then references to specific Hugging Face wrappers.\n\nInstallation and Setup‚Äã\n\nIf you want to work with the Hugging Face Hub:\n\nInstall the Hub client library with pip install huggingface_hub\nCreate a Hugging Face account (it's free!)\nCreate an access token and set it as an environment variable (HUGGINGFACEHUB_API_TOKEN)\n\nIf you want work with the Hugging Face Python libraries:\n\nInstall pip install transformers for working with models and tokenizers\nInstall pip install datasets for working with datasets\nWrappers‚Äã\nLLM‚Äã\n\nThere exists two Hugging Face LLM wrappers, one for a local pipeline and one for a model hosted on Hugging Face Hub. Note that these wrappers only work for models that support the following tasks: text2text-generation, text-generation\n\nTo use the local pipeline wrapper:\n\nfrom langchain.llms import HuggingFacePipeline\n\n\nTo use a the wrapper for a model hosted on Hugging Face Hub:\n\nfrom langchain.llms import HuggingFaceHub\n\n\nFor a more detailed walkthrough of the Hugging Face Hub wrapper, see this notebook\n\nEmbeddings‚Äã\n\nThere exists two Hugging Face Embeddings wrappers, one for a local model and one for a model hosted on Hugging Face Hub. Note that these wrappers only work for sentence-transformers models.\n\nTo use the local pipeline wrapper:\n\nfrom langchain.embeddings import HuggingFaceEmbeddings\n\n\nTo use a the wrapper for a model hosted on Hugging Face Hub:\n\nfrom langchain.embeddings import HuggingFaceHubEmbeddings\n\n\nFor a more detailed walkthrough of this, see this notebook\n\nTokenizer‚Äã\n\nThere are several places you can use tokenizers available through the transformers package. By default, it is used to count tokens for all LLMs.\n\nYou can also use it to count tokens when splitting documents with\n\nfrom langchain.text_splitter import CharacterTextSplitter\nCharacterTextSplitter.from_huggingface_tokenizer(...)\n\n\nFor a more detailed walkthrough of this, see this notebook\n\nDatasets‚Äã\n\nThe Hugging Face Hub has lots of great datasets that can be used to evaluate your LLM chains.\n\nFor a detailed walkthrough of how to use them to do so, see this notebook\n\nPrevious\nHTML to text\nNext\niFixit"
}