{
	"title": "Streaming final agent output | ðŸ¦œï¸ðŸ”— Langchain",
	"url": "https://python.langchain.com/docs/modules/agents/how_to/streaming_stdout_final_only",
	"html": "ModulesAgentsHow-toStreaming final agent output\nStreaming final agent output\n\nIf you only want the final output of an agent to be streamed, you can use the callback FinalStreamingStdOutCallbackHandler. For this, the underlying LLM has to support streaming as well.\n\nfrom langchain.agents import AgentType, initialize_agent, load_tools\nfrom langchain.callbacks.streaming_stdout_final_only import (\n    FinalStreamingStdOutCallbackHandler,\n)\nfrom langchain.llms import OpenAI\n\n\nLet's create the underlying LLM with streaming = True and pass a new instance of FinalStreamingStdOutCallbackHandler.\n\nllm = OpenAI(\n    streaming=True, callbacks=[FinalStreamingStdOutCallbackHandler()], temperature=0\n)\n\ntools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm)\nagent = initialize_agent(\n    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=False\n)\nagent.run(\n    \"It's 2023 now. How many years ago did Konrad Adenauer become Chancellor of Germany.\"\n)\n\n     Konrad Adenauer became Chancellor of Germany in 1949, 74 years ago in 2023.\n\n\n\n\n    'Konrad Adenauer became Chancellor of Germany in 1949, 74 years ago in 2023.'\n\nHandling custom answer prefixesâ€‹\n\nBy default, we assume that the token sequence \"Final\", \"Answer\", \":\" indicates that the agent has reached an answers. We can, however, also pass a custom sequence to use as answer prefix.\n\nllm = OpenAI(\n    streaming=True,\n    callbacks=[\n        FinalStreamingStdOutCallbackHandler(answer_prefix_tokens=[\"The\", \"answer\", \":\"])\n    ],\n    temperature=0,\n)\n\n\nFor convenience, the callback automatically strips whitespaces and new line characters when comparing to answer_prefix_tokens. I.e., if answer_prefix_tokens = [\"The\", \" answer\", \":\"] then both [\"\\nThe\", \" answer\", \":\"] and [\"The\", \" answer\", \":\"] would be recognized a the answer prefix.\n\nIf you don't know the tokenized version of your answer prefix, you can determine it with the following code:\n\nfrom langchain.callbacks.base import BaseCallbackHandler\n\n\nclass MyCallbackHandler(BaseCallbackHandler):\n    def on_llm_new_token(self, token, **kwargs) -> None:\n        # print every token on a new line\n        print(f\"#{token}#\")\n\n\nllm = OpenAI(streaming=True, callbacks=[MyCallbackHandler()])\ntools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm)\nagent = initialize_agent(\n    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=False\n)\nagent.run(\n    \"It's 2023 now. How many years ago did Konrad Adenauer become Chancellor of Germany.\"\n)\n\nAlso streaming the answer prefixesâ€‹\n\nWhen the parameter stream_prefix = True is set, the answer prefix itself will also be streamed. This can be useful when the answer prefix itself is part of the answer. For example, when your answer is a JSON like\n\n{ \"action\": \"Final answer\", \"action_input\": \"Konrad Adenauer became Chancellor 74 years ago.\" }\n\nand you don't only want the action_input to be streamed, but the entire JSON.\n\nPrevious\nShared memory across agents and tools\nNext\nUse ToolKits with OpenAI Functions"
}