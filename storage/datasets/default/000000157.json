{
	"title": "LangSmith Walkthrough | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/langsmith/walkthrough",
	"html": "LangSmithLangSmith Walkthrough\nLangSmith Walkthrough\n\nLangChain makes it easy to prototype LLM applications and Agents. However, delivering LLM applications to production can be deceptively difficult. You will likely have to heavily customize and iterate on your prompts, chains, and other components to create a high-quality product.\n\nTo aid in this process, we've launched LangSmith, a unified platform for debugging, testing, and monitoring your LLM applications.\n\nWhen might this come in handy? You may find it useful when you want to:\n\nQuickly debug a new chain, agent, or set of tools\nVisualize how components (chains, llms, retrievers, etc.) relate and are used\nEvaluate different prompts and LLMs for a single component\nRun a given chain several times over a dataset to ensure it consistently meets a quality bar\nCapture usage traces and using LLMs or analytics pipelines to generate insights\nPrerequisites‚Äã\n\nCreate a LangSmith account and create an API key (see bottom left corner). Familiarize yourself with the platform by looking through the docs\n\nNote LangSmith is in closed beta; we're in the process of rolling it out to more users. However, you can fill out the form on the website for expedited access.\n\nNow, let's get started!\n\nLog runs to LangSmith‚Äã\n\nFirst, configure your environment variables to tell LangChain to log traces. This is done by setting the LANGCHAIN_TRACING_V2 environment variable to true. You can tell LangChain which project to log to by setting the LANGCHAIN_PROJECT environment variable (if this isn't set, runs will be logged to the default project). This will automatically create the project for you if it doesn't exist. You must also set the LANGCHAIN_ENDPOINT and LANGCHAIN_API_KEY environment variables.\n\nFor more information on other ways to set up tracing, please reference the LangSmith documentation.\n\nNOTE: You must also set your OPENAI_API_KEY environment variables in order to run the following tutorial.\n\nNOTE: You can only access an API key when you first create it. Keep it somewhere safe.\n\nNOTE: You can also use a context manager in python to log traces using\n\nfrom langchain.callbacks.manager import tracing_v2_enabled\n\nwith tracing_v2_enabled(project_name=\"My Project\"):\n    agent.run(\"How many people live in canada as of 2023?\")\n\n\nHowever, in this example, we will use environment variables.\n\n%pip install openai tiktoken pandas duckduckgo-search --quiet\n\nimport os\nfrom uuid import uuid4\n\nunique_id = uuid4().hex[0:8]\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\nos.environ[\"LANGCHAIN_PROJECT\"] = f\"Tracing Walkthrough - {unique_id}\"\nos.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\nos.environ[\"LANGCHAIN_API_KEY\"] = \"<YOUR-API-KEY>\"  # Update to your API key\n\n# Used by the agent in this tutorial\nos.environ[\"OPENAI_API_KEY\"] = \"<YOUR-OPENAI-API-KEY>\"\n\n\nCreate the langsmith client to interact with the API\n\nfrom langsmith import Client\n\nclient = Client()\n\n\nCreate a LangChain component and log runs to the platform. In this example, we will create a ReAct-style agent with access to a general search tool (DuckDuckGo). The agent's prompt can be viewed in the Hub here.\n\nfrom langchain import hub\nfrom langchain.agents import AgentExecutor\nfrom langchain.agents.format_scratchpad import format_to_openai_function_messages\nfrom langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.tools import DuckDuckGoSearchResults\nfrom langchain.tools.render import format_tool_to_openai_function\n\n# Fetches the latest version of this prompt\nprompt = hub.pull(\"wfh/langsmith-agent-prompt:latest\")\n\nllm = ChatOpenAI(\n    model=\"gpt-3.5-turbo-16k\",\n    temperature=0,\n)\n\ntools = [\n    DuckDuckGoSearchResults(\n        name=\"duck_duck_go\"\n    ),  # General internet search using DuckDuckGo\n]\n\nllm_with_tools = llm.bind(functions=[format_tool_to_openai_function(t) for t in tools])\n\nrunnable_agent = (\n    {\n        \"input\": lambda x: x[\"input\"],\n        \"agent_scratchpad\": lambda x: format_to_openai_function_messages(\n            x[\"intermediate_steps\"]\n        ),\n    }\n    | prompt\n    | llm_with_tools\n    | OpenAIFunctionsAgentOutputParser()\n)\n\nagent_executor = AgentExecutor(\n    agent=runnable_agent, tools=tools, handle_parsing_errors=True\n)\n\n\nWe are running the agent concurrently on multiple inputs to reduce latency. Runs get logged to LangSmith in the background so execution latency is unaffected.\n\ninputs = [\n    \"What is LangChain?\",\n    \"What's LangSmith?\",\n    \"When was Llama-v2 released?\",\n    \"What is the langsmith cookbook?\",\n    \"When did langchain first announce the hub?\",\n]\n\nresults = agent_executor.batch([{\"input\": x} for x in inputs], return_exceptions=True)\n\nresults[:2]\n\n    [{'input': 'What is LangChain?',\n      'output': 'I\\'m sorry, but I couldn\\'t find any information about \"LangChain\". Could you please provide more context or clarify your question?'},\n     {'input': \"What's LangSmith?\",\n      'output': 'I\\'m sorry, but I couldn\\'t find any information about \"LangSmith\". It could be a specific term or a company that is not widely known. Can you provide more context or clarify what you are referring to?'}]\n\n\nAssuming you've successfully set up your environment, your agent traces should show up in the Projects section in the app. Congrats!\n\nIt looks like the agent isn't effectively using the tools though. Let's evaluate this so we have a baseline.\n\nEvaluate Agent‚Äã\n\nIn addition to logging runs, LangSmith also allows you to test and evaluate your LLM applications.\n\nIn this section, you will leverage LangSmith to create a benchmark dataset and run AI-assisted evaluators on an agent. You will do so in a few steps:\n\nCreate a dataset\nInitialize a new agent to benchmark\nConfigure evaluators to grade an agent's output\nRun the agent over the dataset and evaluate the results\n1. Create a LangSmith dataset‚Äã\n\nBelow, we use the LangSmith client to create a dataset from the input questions from above and a list labels. You will use these later to measure performance for a new agent. A dataset is a collection of examples, which are nothing more than input-output pairs you can use as test cases to your application.\n\nFor more information on datasets, including how to create them from CSVs or other files or how to create them in the platform, please refer to the LangSmith documentation.\n\noutputs = [\n    \"LangChain is an open-source framework for building applications using large language models. It is also the name of the company building LangSmith.\",\n    \"LangSmith is a unified platform for debugging, testing, and monitoring language model applications and agents powered by LangChain\",\n    \"July 18, 2023\",\n    \"The langsmith cookbook is a github repository containing detailed examples of how to use LangSmith to debug, evaluate, and monitor large language model-powered applications.\",\n    \"September 5, 2023\",\n]\n\ndataset_name = f\"agent-qa-{unique_id}\"\n\ndataset = client.create_dataset(\n    dataset_name,\n    description=\"An example dataset of questions over the LangSmith documentation.\",\n)\n\nfor query, answer in zip(inputs, outputs):\n    client.create_example(\n        inputs={\"input\": query}, outputs={\"output\": answer}, dataset_id=dataset.id\n    )\n\n2. Initialize a new agent to benchmark‚Äã\n\nLangSmith lets you evaluate any LLM, chain, agent, or even a custom function. Conversational agents are stateful (they have memory); to ensure that this state isn't shared between dataset runs, we will pass in a chain_factory (aka a constructor) function to initialize for each call.\n\nIn this case, we will test an agent that uses OpenAI's function calling endpoints.\n\nfrom langchain import hub\nfrom langchain.agents import AgentExecutor, AgentType, initialize_agent, load_tools\nfrom langchain.agents.format_scratchpad import format_to_openai_function_messages\nfrom langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.tools.render import format_tool_to_openai_function\n\n\n# Since chains can be stateful (e.g. they can have memory), we provide\n# a way to initialize a new chain for each row in the dataset. This is done\n# by passing in a factory function that returns a new chain for each row.\ndef agent_factory(prompt):\n    llm_with_tools = llm.bind(\n        functions=[format_tool_to_openai_function(t) for t in tools]\n    )\n    runnable_agent = (\n        {\n            \"input\": lambda x: x[\"input\"],\n            \"agent_scratchpad\": lambda x: format_to_openai_function_messages(\n                x[\"intermediate_steps\"]\n            ),\n        }\n        | prompt\n        | llm_with_tools\n        | OpenAIFunctionsAgentOutputParser()\n    )\n    return AgentExecutor(agent=runnable_agent, tools=tools, handle_parsing_errors=True)\n\n3. Configure evaluation‚Äã\n\nManually comparing the results of chains in the UI is effective, but it can be time consuming. It can be helpful to use automated metrics and AI-assisted feedback to evaluate your component's performance.\n\nBelow, we will create some pre-implemented run evaluators that do the following:\n\nCompare results against ground truth labels.\nMeasure semantic (dis)similarity using embedding distance\nEvaluate 'aspects' of the agent's response in a reference-free manner using custom criteria\n\nFor a longer discussion of how to select an appropriate evaluator for your use case and how to create your own custom evaluators, please refer to the LangSmith documentation.\n\nfrom langchain.evaluation import EvaluatorType\nfrom langchain.smith import RunEvalConfig\n\nevaluation_config = RunEvalConfig(\n    # Evaluators can either be an evaluator type (e.g., \"qa\", \"criteria\", \"embedding_distance\", etc.) or a configuration for that evaluator\n    evaluators=[\n        # Measures whether a QA response is \"Correct\", based on a reference answer\n        # You can also select via the raw string \"qa\"\n        EvaluatorType.QA,\n        # Measure the embedding distance between the output and the reference answer\n        # Equivalent to: EvalConfig.EmbeddingDistance(embeddings=OpenAIEmbeddings())\n        EvaluatorType.EMBEDDING_DISTANCE,\n        # Grade whether the output satisfies the stated criteria.\n        # You can select a default one such as \"helpfulness\" or provide your own.\n        RunEvalConfig.LabeledCriteria(\"helpfulness\"),\n        # The LabeledScoreString evaluator outputs a score on a scale from 1-10.\n        # You can use default criteria or write our own rubric\n        RunEvalConfig.LabeledScoreString(\n            {\n                \"accuracy\": \"\"\"\nScore 1: The answer is completely unrelated to the reference.\nScore 3: The answer has minor relevance but does not align with the reference.\nScore 5: The answer has moderate relevance but contains inaccuracies.\nScore 7: The answer aligns with the reference but has minor errors or omissions.\nScore 10: The answer is completely accurate and aligns perfectly with the reference.\"\"\"\n            },\n            normalize_by=10,\n        ),\n    ],\n    # You can add custom StringEvaluator or RunEvaluator objects here as well, which will automatically be\n    # applied to each prediction. Check out the docs for examples.\n    custom_evaluators=[],\n)\n\n4. Run the agent and evaluators‚Äã\n\nUse the run_on_dataset (or asynchronous arun_on_dataset) function to evaluate your model. This will:\n\nFetch example rows from the specified dataset.\nRun your agent (or any custom function) on each example.\nApply evaluators to the resulting run traces and corresponding reference examples to generate automated feedback.\n\nThe results will be visible in the LangSmith app.\n\nfrom langchain import hub\n\n# We will test this version of the prompt\nprompt = hub.pull(\"wfh/langsmith-agent-prompt:798e7324\")\n\nimport functools\n\nfrom langchain.smith import (\n    arun_on_dataset,\n    run_on_dataset,\n)\n\nchain_results = run_on_dataset(\n    dataset_name=dataset_name,\n    llm_or_chain_factory=functools.partial(agent_factory, prompt=prompt),\n    evaluation=evaluation_config,\n    verbose=True,\n    client=client,\n    project_name=f\"runnable-agent-test-5d466cbc-{unique_id}\",\n    tags=[\n        \"testing-notebook\",\n        \"prompt:5d466cbc\",\n    ],  # Optional, adds a tag to the resulting chain runs\n)\n\n# Sometimes, the agent will error due to parsing issues, incompatible tool inputs, etc.\n# These are logged as warnings here and captured as errors in the tracing UI.\n\n    View the evaluation results for project 'runnable-agent-test-5d466cbc-bf2162aa' at:\n    https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/projects/p/0c3d22fa-f8b0-4608-b086-2187c18361a5\n    [>                                                 ] 0/5\n\n    Chain failed for example 54b4fce8-4492-409d-94af-708f51698b39 with inputs {'input': 'Who trained Llama-v2?'}\n    Error Type: TypeError, Message: DuckDuckGoSearchResults._run() got an unexpected keyword argument 'arg1'\n\n\n    [------------------------------------------------->] 5/5\n     Eval quantiles:\n                                   0.25       0.5      0.75      mean      mode\n    embedding_cosine_distance  0.086614  0.118841  0.183672  0.151444  0.050158\n    correctness                0.000000  0.500000  1.000000  0.500000  0.000000\n    score_string:accuracy      0.775000  1.000000  1.000000  0.775000  1.000000\n    helpfulness                0.750000  1.000000  1.000000  0.750000  1.000000\n\nReview the test results‚Äã\n\nYou can review the test results tracing UI below by clicking the URL in the output above or navigating to the \"Testing & Datasets\" page in LangSmith \"agent-qa-{unique_id}\" dataset.\n\nThis will show the new runs and the feedback logged from the selected evaluators. You can also explore a summary of the results in tabular format below.\n\nchain_results.to_dataframe()\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>embedding_cosine_distance</th>\n      <th>correctness</th>\n      <th>score_string:accuracy</th>\n      <th>helpfulness</th>\n      <th>input</th>\n      <th>output</th>\n      <th>reference</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>42b639a2-17c4-4031-88a9-0ce2c45781ce</th>\n      <td>0.317938</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>{'input': 'What is the langsmith cookbook?'}</td>\n      <td>{'input': 'What is the langsmith cookbook?', '...</td>\n      <td>{'output': 'September 5, 2023'}</td>\n    </tr>\n    <tr>\n      <th>54b4fce8-4492-409d-94af-708f51698b39</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>{'input': 'Who trained Llama-v2?'}</td>\n      <td>{'Error': 'TypeError(\"DuckDuckGoSearchResults....</td>\n      <td>{'output': 'The langsmith cookbook is a github...</td>\n    </tr>\n    <tr>\n      <th>8ae5104e-bbb4-42cc-a84e-f9b8cfc92b8e</th>\n      <td>0.138916</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>{'input': 'When was Llama-v2 released?'}</td>\n      <td>{'input': 'When was Llama-v2 released?', 'outp...</td>\n      <td>{'output': 'July 18, 2023'}</td>\n    </tr>\n    <tr>\n      <th>678c0363-3ed1-410a-811f-ebadef2e783a</th>\n      <td>0.050158</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>{'input': 'What's LangSmith?'}</td>\n      <td>{'input': 'What's LangSmith?', 'output': 'Lang...</td>\n      <td>{'output': 'LangSmith is a unified platform fo...</td>\n    </tr>\n    <tr>\n      <th>762a616c-7aab-419c-9001-b43ab6200d26</th>\n      <td>0.098766</td>\n      <td>0.0</td>\n      <td>0.1</td>\n      <td>0.0</td>\n      <td>{'input': 'What is LangChain?'}</td>\n      <td>{'input': 'What is LangChain?', 'output': 'Lan...</td>\n      <td>{'output': 'LangChain is an open-source framew...</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n(Optional) Compare to another prompt‚Äã\n\nNow that we have our test run results, we can make changes to our agent and benchmark them. Let's try this again with a different prompt and see the results.\n\ncandidate_prompt = hub.pull(\"wfh/langsmith-agent-prompt:39f3bbd0\")\n\nchain_results = run_on_dataset(\n    dataset_name=dataset_name,\n    llm_or_chain_factory=functools.partial(agent_factory, prompt=candidate_prompt),\n    evaluation=evaluation_config,\n    verbose=True,\n    client=client,\n    project_name=f\"runnable-agent-test-39f3bbd0-{unique_id}\",\n    tags=[\n        \"testing-notebook\",\n        \"prompt:39f3bbd0\",\n    ],  # Optional, adds a tag to the resulting chain runs\n)\n\n    View the evaluation results for project 'runnable-agent-test-39f3bbd0-bf2162aa' at:\n    https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/projects/p/fa721ccc-dd0f-41c9-bf80-22215c44efd4\n    [------------------------------------------------->] 5/5\n     Eval quantiles:\n                                   0.25       0.5      0.75      mean      mode\n    embedding_cosine_distance  0.059506  0.155538  0.212864  0.157915  0.043119\n    correctness                0.000000  0.000000  1.000000  0.400000  0.000000\n    score_string:accuracy      0.700000  1.000000  1.000000  0.880000  1.000000\n    helpfulness                1.000000  1.000000  1.000000  0.800000  1.000000\n\nExporting datasets and runs‚Äã\n\nLangSmith lets you export data to common formats such as CSV or JSONL directly in the web app. You can also use the client to fetch runs for further analysis, to store in your own database, or to share with others. Let's fetch the run traces from the evaluation run.\n\nNote: It may be a few moments before all the runs are accessible.\n\nruns = client.list_runs(project_name=chain_results[\"project_name\"], execution_order=1)\n\n# After some time, these will be populated.\nclient.read_project(project_name=chain_results[\"project_name\"]).feedback_stats\n\nConclusion‚Äã\n\nCongratulations! You have successfully traced and evaluated an agent using LangSmith!\n\nThis was a quick guide to get started, but there are many more ways to use LangSmith to speed up your developer flow and produce better results.\n\nFor more information on how you can get the most out of LangSmith, check out LangSmith documentation, and please reach out with questions, feature requests, or feedback at support@langchain.dev.\n\nPrevious\nLangSmith"
}