{
	"title": "Embedding Distance | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/guides/evaluation/string/embedding_distance",
	"html": "EvaluationString EvaluatorsEmbedding Distance\nEmbedding Distance\n\nTo measure semantic similarity (or dissimilarity) between a prediction and a reference label string, you could use a vector vector distance metric the two embedded representations using the embedding_distance evaluator.[1]\n\nNote: This returns a distance score, meaning that the lower the number, the more similar the prediction is to the reference, according to their embedded representation.\n\nCheck out the reference docs for the EmbeddingDistanceEvalChain for more info.\n\nfrom langchain.evaluation import load_evaluator\n\nevaluator = load_evaluator(\"embedding_distance\")\n\nevaluator.evaluate_strings(prediction=\"I shall go\", reference=\"I shan't go\")\n\n    {'score': 0.0966466944859925}\n\nevaluator.evaluate_strings(prediction=\"I shall go\", reference=\"I will go\")\n\n    {'score': 0.03761174337464557}\n\nSelect the Distance Metric‚Äã\n\nBy default, the evaluator uses cosine distance. You can choose a different distance metric if you'd like.\n\nfrom langchain.evaluation import EmbeddingDistance\n\nlist(EmbeddingDistance)\n\n    [<EmbeddingDistance.COSINE: 'cosine'>,\n     <EmbeddingDistance.EUCLIDEAN: 'euclidean'>,\n     <EmbeddingDistance.MANHATTAN: 'manhattan'>,\n     <EmbeddingDistance.CHEBYSHEV: 'chebyshev'>,\n     <EmbeddingDistance.HAMMING: 'hamming'>]\n\n# You can load by enum or by raw python string\nevaluator = load_evaluator(\n    \"embedding_distance\", distance_metric=EmbeddingDistance.EUCLIDEAN\n)\n\nSelect Embeddings to Use‚Äã\n\nThe constructor uses OpenAI embeddings by default, but you can configure this however you want. Below, use huggingface local embeddings\n\nfrom langchain.embeddings import HuggingFaceEmbeddings\n\nembedding_model = HuggingFaceEmbeddings()\nhf_evaluator = load_evaluator(\"embedding_distance\", embeddings=embedding_model)\n\nhf_evaluator.evaluate_strings(prediction=\"I shall go\", reference=\"I shan't go\")\n\n    {'score': 0.5486443280477362}\n\nhf_evaluator.evaluate_strings(prediction=\"I shall go\", reference=\"I will go\")\n\n    {'score': 0.21018880025138598}\n\n1. Note: When it comes to semantic similarity, this often gives better results than older string distance metrics (such as those in the [StringDistanceEvalChain](https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.string_distance.base.StringDistanceEvalChain.html#langchain.evaluation.string_distance.base.StringDistanceEvalChain)), though it tends to be less reliable than evaluators that use the LLM directly (such as the [QAEvalChain](https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.qa.eval_chain.QAEvalChain.html#langchain.evaluation.qa.eval_chain.QAEvalChain) or [LabeledCriteriaEvalChain](https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.criteria.eval_chain.LabeledCriteriaEvalChain.html#langchain.evaluation.criteria.eval_chain.LabeledCriteriaEvalChain))\nPrevious\nCustom String Evaluator\nNext\nExact Match"
}