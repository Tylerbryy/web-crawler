{
	"title": "DingoDB | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/integrations/vectorstores/dingo",
	"html": "ComponentsVector storesDingoDB\nDingoDB\n\nDingoDB is a distributed multi-mode vector database, which combines the characteristics of data lakes and vector databases, and can store data of any type and size (Key-Value, PDF, audio, video, etc.). It has real-time low-latency processing capabilities to achieve rapid insight and response, and can efficiently conduct instant analysis and process multi-modal data.\n\nThis notebook shows how to use functionality related to the DingoDB vector database.\n\nTo run, you should have a DingoDB instance up and running.\n\npip install dingodb\n# or install latest:\npip install git+https://git@github.com/dingodb/pydingo.git\n\n\nWe want to use OpenAIEmbeddings so we have to get the OpenAI API Key.\n\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n\n    OpenAI API Key:¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n\nfrom langchain.document_loaders import TextLoader\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.vectorstores import Dingo\n\nfrom langchain.document_loaders import TextLoader\n\nloader = TextLoader(\"../../modules/state_of_the_union.txt\")\ndocuments = loader.load()\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\ndocs = text_splitter.split_documents(documents)\n\nembeddings = OpenAIEmbeddings()\n\nfrom dingodb import DingoDB\n\nindex_name = \"langchain_demo\"\n\ndingo_client = DingoDB(user=\"\", password=\"\", host=[\"127.0.0.1:13000\"])\n# First, check if our index already exists. If it doesn't, we create it\nif (\n    index_name not in dingo_client.get_index()\n    and index_name.upper() not in dingo_client.get_index()\n):\n    # we create a new index, modify to your own\n    dingo_client.create_index(\n        index_name=index_name, dimension=1536, metric_type=\"cosine\", auto_id=False\n    )\n\n# The OpenAI embedding model `text-embedding-ada-002 uses 1536 dimensions`\ndocsearch = Dingo.from_documents(\n    docs, embeddings, client=dingo_client, index_name=index_name\n)\n\nfrom langchain.document_loaders import TextLoader\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.vectorstores import Dingo\n\nquery = \"What did the president say about Ketanji Brown Jackson\"\ndocs = docsearch.similarity_search(query)\n\nprint(docs[0].page_content)\n\nAdding More Text to an Existing Index‚Äã\n\nMore text can embedded and upserted to an existing Dingo index using the add_texts function\n\nvectorstore = Dingo(embeddings, \"text\", client=dingo_client, index_name=index_name)\n\nvectorstore.add_texts([\"More text!\"])\n\nMaximal Marginal Relevance Searches‚Äã\n\nIn addition to using similarity search in the retriever object, you can also use mmr as retriever.\n\nretriever = docsearch.as_retriever(search_type=\"mmr\")\nmatched_docs = retriever.get_relevant_documents(query)\nfor i, d in enumerate(matched_docs):\n    print(f\"\\n## Document {i}\\n\")\n    print(d.page_content)\n\n\nOr use max_marginal_relevance_search directly:\n\nfound_docs = docsearch.max_marginal_relevance_search(query, k=2, fetch_k=10)\nfor i, doc in enumerate(found_docs):\n    print(f\"{i + 1}.\", doc.page_content, \"\\n\")\n\nPrevious\nDashVector\nNext\nDocArray HnswSearch"
}