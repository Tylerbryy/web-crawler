{
	"title": "OpenLLM | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/integrations/providers/openllm",
	"html": "ProvidersMoreOpenLLM\nOpenLLM\n\nThis page demonstrates how to use OpenLLM with LangChain.\n\nOpenLLM is an open platform for operating large language models (LLMs) in production. It enables developers to easily run inference with any open-source LLMs, deploy to the cloud or on-premises, and build powerful AI apps.\n\nInstallation and Setup‚Äã\n\nInstall the OpenLLM package via PyPI:\n\npip install openllm\n\nLLM‚Äã\n\nOpenLLM supports a wide range of open-source LLMs as well as serving users' own fine-tuned LLMs. Use openllm model command to see all available models that are pre-optimized for OpenLLM.\n\nWrappers‚Äã\n\nThere is a OpenLLM Wrapper which supports loading LLM in-process or accessing a remote OpenLLM server:\n\nfrom langchain.llms import OpenLLM\n\nWrapper for OpenLLM server‚Äã\n\nThis wrapper supports connecting to an OpenLLM server via HTTP or gRPC. The OpenLLM server can run either locally or on the cloud.\n\nTo try it out locally, start an OpenLLM server:\n\nopenllm start flan-t5\n\n\nWrapper usage:\n\nfrom langchain.llms import OpenLLM\n\nllm = OpenLLM(server_url='http://localhost:3000')\n\nllm(\"What is the difference between a duck and a goose? And why there are so many Goose in Canada?\")\n\nWrapper for Local Inference‚Äã\n\nYou can also use the OpenLLM wrapper to load LLM in current Python process for running inference.\n\nfrom langchain.llms import OpenLLM\n\nllm = OpenLLM(model_name=\"dolly-v2\", model_id='databricks/dolly-v2-7b')\n\nllm(\"What is the difference between a duck and a goose? And why there are so many Goose in Canada?\")\n\nUsage‚Äã\n\nFor a more detailed walkthrough of the OpenLLM Wrapper, see the example notebook\n\nPrevious\nObsidian\nNext\nOpenSearch"
}