{
	"title": "Prediction Guard | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/integrations/providers/predictionguard",
	"html": "ProvidersMorePrediction Guard\nPrediction Guard\n\nThis page covers how to use the Prediction Guard ecosystem within LangChain. It is broken into two parts: installation and setup, and then references to specific Prediction Guard wrappers.\n\nInstallation and Setup‚Äã\nInstall the Python SDK with pip install predictionguard\nGet a Prediction Guard access token (as described here) and set it as an environment variable (PREDICTIONGUARD_TOKEN)\nLLM Wrapper‚Äã\n\nThere exists a Prediction Guard LLM wrapper, which you can access with\n\nfrom langchain.llms import PredictionGuard\n\n\nYou can provide the name of the Prediction Guard model as an argument when initializing the LLM:\n\npgllm = PredictionGuard(model=\"MPT-7B-Instruct\")\n\n\nYou can also provide your access token directly as an argument:\n\npgllm = PredictionGuard(model=\"MPT-7B-Instruct\", token=\"<your access token>\")\n\n\nFinally, you can provide an \"output\" argument that is used to structure/ control the output of the LLM:\n\npgllm = PredictionGuard(model=\"MPT-7B-Instruct\", output={\"type\": \"boolean\"})\n\nExample usage‚Äã\n\nBasic usage of the controlled or guarded LLM wrapper:\n\nimport os\n\nimport predictionguard as pg\nfrom langchain.llms import PredictionGuard\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains import LLMChain\n\n# Your Prediction Guard API key. Get one at predictionguard.com\nos.environ[\"PREDICTIONGUARD_TOKEN\"] = \"<your Prediction Guard access token>\"\n\n# Define a prompt template\ntemplate = \"\"\"Respond to the following query based on the context.\n\nContext: EVERY comment, DM + email suggestion has led us to this EXCITING announcement! üéâ We have officially added TWO new candle subscription box options! üì¶\nExclusive Candle Box - $80 \nMonthly Candle Box - $45 (NEW!)\nScent of The Month Box - $28 (NEW!)\nHead to stories to get ALL the deets on each box! üëÜ BONUS: Save 50% on your first box with code 50OFF! üéâ\n\nQuery: {query}\n\nResult: \"\"\"\nprompt = PromptTemplate(template=template, input_variables=[\"query\"])\n\n# With \"guarding\" or controlling the output of the LLM. See the \n# Prediction Guard docs (https://docs.predictionguard.com) to learn how to \n# control the output with integer, float, boolean, JSON, and other types and\n# structures.\npgllm = PredictionGuard(model=\"MPT-7B-Instruct\", \n                        output={\n                                \"type\": \"categorical\",\n                                \"categories\": [\n                                    \"product announcement\", \n                                    \"apology\", \n                                    \"relational\"\n                                    ]\n                                })\npgllm(prompt.format(query=\"What kind of post is this?\"))\n\n\nBasic LLM Chaining with the Prediction Guard wrapper:\n\nimport os\n\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains import LLMChain\nfrom langchain.llms import PredictionGuard\n\n# Optional, add your OpenAI API Key. This is optional, as Prediction Guard allows\n# you to access all the latest open access models (see https://docs.predictionguard.com)\nos.environ[\"OPENAI_API_KEY\"] = \"<your OpenAI api key>\"\n\n# Your Prediction Guard API key. Get one at predictionguard.com\nos.environ[\"PREDICTIONGUARD_TOKEN\"] = \"<your Prediction Guard access token>\"\n\npgllm = PredictionGuard(model=\"OpenAI-text-davinci-003\")\n\ntemplate = \"\"\"Question: {question}\n\nAnswer: Let's think step by step.\"\"\"\nprompt = PromptTemplate(template=template, input_variables=[\"question\"])\nllm_chain = LLMChain(prompt=prompt, llm=pgllm, verbose=True)\n\nquestion = \"What NFL team won the Super Bowl in the year Justin Beiber was born?\"\n\nllm_chain.predict(question=question)\n\nPrevious\nPredibase\nNext\nPromptLayer"
}