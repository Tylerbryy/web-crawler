{
	"title": "Titan Takeoff | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/integrations/llms/titan_takeoff",
	"html": "ComponentsLLMsTitan Takeoff\nTitan Takeoff\n\nTitanML helps businesses build and deploy better, smaller, cheaper, and faster NLP models through our training, compression, and inference optimization platform.\n\nOur inference server, Titan Takeoff enables deployment of LLMs locally on your hardware in a single command. Most generative model architectures are supported, such as Falcon, Llama 2, GPT2, T5 and many more.\n\nInstallation‚Äã\n\nTo get started with Iris Takeoff, all you need is to have docker and python installed on your local system. If you wish to use the server with gpu support, then you will need to install docker with cuda support.\n\nFor Mac and Windows users, make sure you have the docker daemon running! You can check this by running docker ps in your terminal. To start the daemon, open the docker desktop app.\n\nRun the following command to install the Iris CLI that will enable you to run the takeoff server:\n\npip install titan-iris\n\nChoose a Model‚Äã\n\nTakeoff supports many of the most powerful generative text models, such as Falcon, MPT, and Llama. See the supported models for more information. For information about using your own models, see the custom models.\n\nGoing forward in this demo we will be using the falcon 7B instruct model. This is a good open-source model that is trained to follow instructions, and is small enough to easily inference even on CPUs.\n\nTaking off‚Äã\n\nModels are referred to by their model id on HuggingFace. Takeoff uses port 8000 by default, but can be configured to use another port. There is also support to use a Nvidia GPU by specifying cuda for the device flag.\n\nTo start the takeoff server, run:\n\niris takeoff --model tiiuae/falcon-7b-instruct --device cpu\niris takeoff --model tiiuae/falcon-7b-instruct --device cuda # Nvidia GPU required\niris takeoff --model tiiuae/falcon-7b-instruct --device cpu --port 5000 # run on port 5000 (default: 8000)\n\n\nYou will then be directed to a login page, where you will need to create an account to proceed. After logging in, run the command onscreen to check whether the server is ready. When it is ready, you can start using the Takeoff integration.\n\nTo shutdown the server, run the following command. You will be presented with options on which Takeoff server to shut down, in case you have multiple running servers.\n\niris takeoff --shutdown # shutdown the server\n\nInferencing your model‚Äã\n\nTo access your LLM, use the TitanTakeoff LLM wrapper:\n\nfrom langchain.llms import TitanTakeoff\n\nllm = TitanTakeoff(\n    base_url=\"http://localhost:8000\", generate_max_length=128, temperature=1.0\n)\n\nprompt = \"What is the largest planet in the solar system?\"\n\nllm(prompt)\n\n\nNo parameters are needed by default, but a baseURL that points to your desired URL where Takeoff is running can be specified and generation parameters can be supplied.\n\nStreaming‚Äã\n\nStreaming is also supported via the streaming flag:\n\nfrom langchain.callbacks.manager import CallbackManager\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n\nllm = TitanTakeoff(\n    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]), streaming=True\n)\n\nprompt = \"What is the capital of France?\"\n\nllm(prompt)\n\nIntegration with LLMChain‚Äã\nfrom langchain.chains import LLMChain\nfrom langchain.prompts import PromptTemplate\n\nllm = TitanTakeoff()\n\ntemplate = \"What is the capital of {country}\"\n\nprompt = PromptTemplate(template=template, input_variables=[\"country\"])\n\nllm_chain = LLMChain(llm=llm, prompt=prompt)\n\ngenerated = llm_chain.run(country=\"Belgium\")\nprint(generated)\n\nPrevious\nTextGen\nNext\nTitan Takeoff Pro"
}