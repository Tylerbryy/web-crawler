{
	"title": "JSONFormer | ðŸ¦œï¸ðŸ”— Langchain",
	"url": "https://python.langchain.com/docs/integrations/llms/jsonformer_experimental",
	"html": "ComponentsLLMsJSONFormer\nJSONFormer\n\nJSONFormer is a library that wraps local Hugging Face pipeline models for structured decoding of a subset of the JSON Schema.\n\nIt works by filling in the structure tokens and then sampling the content tokens from the model.\n\nWarning - this module is still experimental\n\npip install --upgrade jsonformer > /dev/null\n\nHugging Face Baselineâ€‹\n\nFirst, let's establish a qualitative baseline by checking the output of the model without structured decoding.\n\nimport logging\n\nlogging.basicConfig(level=logging.ERROR)\n\nimport json\nimport os\n\nimport requests\nfrom langchain.tools import tool\n\nHF_TOKEN = os.environ.get(\"HUGGINGFACE_API_KEY\")\n\n\n@tool\ndef ask_star_coder(query: str, temperature: float = 1.0, max_new_tokens: float = 250):\n    \"\"\"Query the BigCode StarCoder model about coding questions.\"\"\"\n    url = \"https://api-inference.huggingface.co/models/bigcode/starcoder\"\n    headers = {\n        \"Authorization\": f\"Bearer {HF_TOKEN}\",\n        \"content-type\": \"application/json\",\n    }\n    payload = {\n        \"inputs\": f\"{query}\\n\\nAnswer:\",\n        \"temperature\": temperature,\n        \"max_new_tokens\": int(max_new_tokens),\n    }\n    response = requests.post(url, headers=headers, data=json.dumps(payload))\n    response.raise_for_status()\n    return json.loads(response.content.decode(\"utf-8\"))\n\nprompt = \"\"\"You must respond using JSON format, with a single action and single action input.\nYou may 'ask_star_coder' for help on coding problems.\n\n{arg_schema}\n\nEXAMPLES\n----\nHuman: \"So what's all this about a GIL?\"\nAI Assistant:{{\n  \"action\": \"ask_star_coder\",\n  \"action_input\": {{\"query\": \"What is a GIL?\", \"temperature\": 0.0, \"max_new_tokens\": 100}}\"\n}}\nObservation: \"The GIL is python's Global Interpreter Lock\"\nHuman: \"Could you please write a calculator program in LISP?\"\nAI Assistant:{{\n  \"action\": \"ask_star_coder\",\n  \"action_input\": {{\"query\": \"Write a calculator program in LISP\", \"temperature\": 0.0, \"max_new_tokens\": 250}}\n}}\nObservation: \"(defun add (x y) (+ x y))\\n(defun sub (x y) (- x y ))\"\nHuman: \"What's the difference between an SVM and an LLM?\"\nAI Assistant:{{\n  \"action\": \"ask_star_coder\",\n  \"action_input\": {{\"query\": \"What's the difference between SGD and an SVM?\", \"temperature\": 1.0, \"max_new_tokens\": 250}}\n}}\nObservation: \"SGD stands for stochastic gradient descent, while an SVM is a Support Vector Machine.\"\n\nBEGIN! Answer the Human's question as best as you are able.\n------\nHuman: 'What's the difference between an iterator and an iterable?'\nAI Assistant:\"\"\".format(arg_schema=ask_star_coder.args)\n\nfrom langchain.llms import HuggingFacePipeline\nfrom transformers import pipeline\n\nhf_model = pipeline(\n    \"text-generation\", model=\"cerebras/Cerebras-GPT-590M\", max_new_tokens=200\n)\n\noriginal_model = HuggingFacePipeline(pipeline=hf_model)\n\ngenerated = original_model.predict(prompt, stop=[\"Observation:\", \"Human:\"])\nprint(generated)\n\n    Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n\n\n     'What's the difference between an iterator and an iterable?'\n    \n\n\nThat's not so impressive, is it? It didn't follow the JSON format at all! Let's try with the structured decoder.\n\nJSONFormer LLM Wrapperâ€‹\n\nLet's try that again, now providing a the Action input's JSON Schema to the model.\n\ndecoder_schema = {\n    \"title\": \"Decoding Schema\",\n    \"type\": \"object\",\n    \"properties\": {\n        \"action\": {\"type\": \"string\", \"default\": ask_star_coder.name},\n        \"action_input\": {\n            \"type\": \"object\",\n            \"properties\": ask_star_coder.args,\n        },\n    },\n}\n\nfrom langchain_experimental.llms import JsonFormer\n\njson_former = JsonFormer(json_schema=decoder_schema, pipeline=hf_model)\n\nresults = json_former.predict(prompt, stop=[\"Observation:\", \"Human:\"])\nprint(results)\n\n    {\"action\": \"ask_star_coder\", \"action_input\": {\"query\": \"What's the difference between an iterator and an iter\", \"temperature\": 0.0, \"max_new_tokens\": 50.0}}\n\n\nVoila! Free of parsing errors.\n\nPrevious\nJavelin AI Gateway Tutorial\nNext\nKoboldAI API"
}