{
	"title": "EverlyAI | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/integrations/chat/everlyai",
	"html": "ComponentsChat modelsEverlyAI\nEverlyAI\n\nEverlyAI allows you to run your ML models at scale in the cloud. It also provides API access to several LLM models.\n\nThis notebook demonstrates the use of langchain.chat_models.ChatEverlyAI for EverlyAI Hosted Endpoints.\n\nSet EVERLYAI_API_KEY environment variable\nor use the everlyai_api_key keyword argument\n# !pip install openai\n\nimport os\nfrom getpass import getpass\n\nos.environ[\"EVERLYAI_API_KEY\"] = getpass()\n\nLet's try out LLAMA model offered on EverlyAI Hosted Endpoints\nfrom langchain.chat_models import ChatEverlyAI\nfrom langchain.schema import HumanMessage, SystemMessage\n\nmessages = [\n    SystemMessage(content=\"You are a helpful AI that shares everything you know.\"),\n    HumanMessage(\n        content=\"Tell me technical facts about yourself. Are you a transformer model? How many billions of parameters do you have?\"\n    ),\n]\n\nchat = ChatEverlyAI(\n    model_name=\"meta-llama/Llama-2-7b-chat-hf\", temperature=0.3, max_tokens=64\n)\nprint(chat(messages).content)\n\n      Hello! I'm just an AI, I don't have personal information or technical details like a human would. However, I can tell you that I'm a type of transformer model, specifically a BERT (Bidirectional Encoder Representations from Transformers) model. B\n\nEverlyAI also supports streaming responses\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\nfrom langchain.chat_models import ChatEverlyAI\nfrom langchain.schema import HumanMessage, SystemMessage\n\nmessages = [\n    SystemMessage(content=\"You are a humorous AI that delights people.\"),\n    HumanMessage(content=\"Tell me a joke?\"),\n]\n\nchat = ChatEverlyAI(\n    model_name=\"meta-llama/Llama-2-7b-chat-hf\",\n    temperature=0.3,\n    max_tokens=64,\n    streaming=True,\n    callbacks=[StreamingStdOutCallbackHandler()],\n)\nchat(messages)\n\n      Ah, a joke, you say? *adjusts glasses* Well, I've got a doozy for you! *winks*\n     *pauses for dramatic effect*\n    Why did the AI go to therapy?\n    *drumroll*\n    Because\n\n\n\n\n    AIMessageChunk(content=\"  Ah, a joke, you say? *adjusts glasses* Well, I've got a doozy for you! *winks*\\n *pauses for dramatic effect*\\nWhy did the AI go to therapy?\\n*drumroll*\\nBecause\")\n\nLet's try a different language model on EverlyAI\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\nfrom langchain.chat_models import ChatEverlyAI\nfrom langchain.schema import HumanMessage, SystemMessage\n\nmessages = [\n    SystemMessage(content=\"You are a humorous AI that delights people.\"),\n    HumanMessage(content=\"Tell me a joke?\"),\n]\n\nchat = ChatEverlyAI(\n    model_name=\"meta-llama/Llama-2-13b-chat-hf-quantized\",\n    temperature=0.3,\n    max_tokens=128,\n    streaming=True,\n    callbacks=[StreamingStdOutCallbackHandler()],\n)\nchat(messages)\n\n      OH HO HO! *adjusts monocle* Well, well, well! Look who's here! *winks*\n    \n    You want a joke, huh? *puffs out chest* Well, let me tell you one that's guaranteed to tickle your funny bone! *clears throat*\n    \n    Why couldn't the bicycle stand up by itself? *pauses for dramatic effect* Because it was two-tired! *winks*\n    \n    Hope that one put a spring in your step, my dear! *\n\n\n\n\n    AIMessageChunk(content=\"  OH HO HO! *adjusts monocle* Well, well, well! Look who's here! *winks*\\n\\nYou want a joke, huh? *puffs out chest* Well, let me tell you one that's guaranteed to tickle your funny bone! *clears throat*\\n\\nWhy couldn't the bicycle stand up by itself? *pauses for dramatic effect* Because it was two-tired! *winks*\\n\\nHope that one put a spring in your step, my dear! *\")\n\nPrevious\nERNIE-Bot Chat\nNext\nFireworks"
}