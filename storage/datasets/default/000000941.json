{
	"title": "Few-shot examples for chat models | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/few_shot_examples_chat",
	"html": "ModulesModel I/OPromptsPrompt templatesFew-shot examples for chat models\nFew-shot examples for chat models\n\nThis notebook covers how to use few-shot examples in chat models. There does not appear to be solid consensus on how best to do few-shot prompting, and the optimal prompt compilation will likely vary by model. Because of this, we provide few-shot prompt templates like the FewShotChatMessagePromptTemplate as a flexible starting point, and you can modify or replace them as you see fit.\n\nThe goal of few-shot prompt templates are to dynamically select examples based on an input, and then format the examples in a final prompt to provide for the model.\n\nNote: The following code examples are for chat models. For similar few-shot prompt examples for completion models (LLMs), see the few-shot prompt templates guide.\n\nFixed Examples‚Äã\n\nThe most basic (and common) few-shot prompting technique is to use a fixed prompt example. This way you can select a chain, evaluate it, and avoid worrying about additional moving parts in production.\n\nThe basic components of the template are:\n\nexamples: A list of dictionary examples to include in the final prompt.\nexample_prompt: converts each example into 1 or more messages through its format_messages method. A common example would be to convert each example into one human message and one AI message response, or a human message followed by a function call message.\n\nBelow is a simple demonstration. First, import the modules for this example:\n\nfrom langchain.prompts import (\n    ChatPromptTemplate,\n    FewShotChatMessagePromptTemplate,\n)\n\n\nThen, define the examples you'd like to include.\n\nexamples = [\n    {\"input\": \"2+2\", \"output\": \"4\"},\n    {\"input\": \"2+3\", \"output\": \"5\"},\n]\n\n\nNext, assemble them into the few-shot prompt template.\n\n# This is a prompt template used to format each individual example.\nexample_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"human\", \"{input}\"),\n        (\"ai\", \"{output}\"),\n    ]\n)\nfew_shot_prompt = FewShotChatMessagePromptTemplate(\n    example_prompt=example_prompt,\n    examples=examples,\n)\n\nprint(few_shot_prompt.format())\n\n    Human: 2+2\n    AI: 4\n    Human: 2+3\n    AI: 5\n\n\nFinally, assemble your final prompt and use it with a model.\n\nfinal_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are a wondrous wizard of math.\"),\n        few_shot_prompt,\n        (\"human\", \"{input}\"),\n    ]\n)\n\nfrom langchain.chat_models import ChatAnthropic\n\nchain = final_prompt | ChatAnthropic(temperature=0.0)\n\nchain.invoke({\"input\": \"What's the square of a triangle?\"})\n\n    AIMessage(content=' Triangles do not have a \"square\". A square refers to a shape with 4 equal sides and 4 right angles. Triangles have 3 sides and 3 angles.\\n\\nThe area of a triangle can be calculated using the formula:\\n\\nA = 1/2 * b * h\\n\\nWhere:\\n\\nA is the area \\nb is the base (the length of one of the sides)\\nh is the height (the length from the base to the opposite vertex)\\n\\nSo the area depends on the specific dimensions of the triangle. There is no single \"square of a triangle\". The area can vary greatly depending on the base and height measurements.', additional_kwargs={}, example=False)\n\nDynamic few-shot prompting‚Äã\n\nSometimes you may want to condition which examples are shown based on the input. For this, you can replace the examples with an example_selector. The other components remain the same as above! To review, the dynamic few-shot prompt template would look like:\n\nexample_selector: responsible for selecting few-shot examples (and the order in which they are returned) for a given input. These implement the BaseExampleSelector interface. A common example is the vectorstore-backed SemanticSimilarityExampleSelector\nexample_prompt: convert each example into 1 or more messages through its format_messages method. A common example would be to convert each example into one human message and one AI message response, or a human message followed by a function call message.\n\nThese once again can be composed with other messages and chat templates to assemble your final prompt.\n\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.prompts import SemanticSimilarityExampleSelector\nfrom langchain.vectorstores import Chroma\n\n\nSince we are using a vectorstore to select examples based on semantic similarity, we will want to first populate the store.\n\nexamples = [\n    {\"input\": \"2+2\", \"output\": \"4\"},\n    {\"input\": \"2+3\", \"output\": \"5\"},\n    {\"input\": \"2+4\", \"output\": \"6\"},\n    {\"input\": \"What did the cow say to the moon?\", \"output\": \"nothing at all\"},\n    {\n        \"input\": \"Write me a poem about the moon\",\n        \"output\": \"One for the moon, and one for me, who are we to talk about the moon?\",\n    },\n]\n\nto_vectorize = [\" \".join(example.values()) for example in examples]\nembeddings = OpenAIEmbeddings()\nvectorstore = Chroma.from_texts(to_vectorize, embeddings, metadatas=examples)\n\nCreate the example_selector‚Äã\n\nWith a vectorstore created, you can create the example_selector. Here we will isntruct it to only fetch the top 2 examples.\n\nexample_selector = SemanticSimilarityExampleSelector(\n    vectorstore=vectorstore,\n    k=2,\n)\n\n# The prompt template will load examples by passing the input do the `select_examples` method\nexample_selector.select_examples({\"input\": \"horse\"})\n\n    [{'input': 'What did the cow say to the moon?', 'output': 'nothing at all'},\n     {'input': '2+4', 'output': '6'}]\n\nCreate prompt template‚Äã\n\nAssemble the prompt template, using the example_selector created above.\n\nfrom langchain.prompts import (\n    ChatPromptTemplate,\n    FewShotChatMessagePromptTemplate,\n)\n\n# Define the few-shot prompt.\nfew_shot_prompt = FewShotChatMessagePromptTemplate(\n    # The input variables select the values to pass to the example_selector\n    input_variables=[\"input\"],\n    example_selector=example_selector,\n    # Define how each example will be formatted.\n    # In this case, each example will become 2 messages:\n    # 1 human, and 1 AI\n    example_prompt=ChatPromptTemplate.from_messages(\n        [(\"human\", \"{input}\"), (\"ai\", \"{output}\")]\n    ),\n)\n\n\nBelow is an example of how this would be assembled.\n\nprint(few_shot_prompt.format(input=\"What's 3+3?\"))\n\n    Human: 2+3\n    AI: 5\n    Human: 2+2\n    AI: 4\n\n\nAssemble the final prompt template:\n\nfinal_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are a wondrous wizard of math.\"),\n        few_shot_prompt,\n        (\"human\", \"{input}\"),\n    ]\n)\n\nprint(few_shot_prompt.format(input=\"What's 3+3?\"))\n\n    Human: 2+3\n    AI: 5\n    Human: 2+2\n    AI: 4\n\nUse with an LLM‚Äã\n\nNow, you can connect your model to the few-shot prompt.\n\nfrom langchain.chat_models import ChatAnthropic\n\nchain = final_prompt | ChatAnthropic(temperature=0.0)\n\nchain.invoke({\"input\": \"What's 3+3?\"})\n\n    AIMessage(content=' 3 + 3 = 6', additional_kwargs={}, example=False)\n\nPrevious\nFew-shot prompt templates\nNext\nFormat template output"
}