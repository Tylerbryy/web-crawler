{
	"title": "Custom pairwise evaluator | ðŸ¦œï¸ðŸ”— Langchain",
	"url": "https://python.langchain.com/docs/guides/evaluation/comparison/custom",
	"html": "EvaluationComparison EvaluatorsCustom pairwise evaluator\nCustom pairwise evaluator\n\nYou can make your own pairwise string evaluators by inheriting from PairwiseStringEvaluator class and overwriting the _evaluate_string_pairs method (and the _aevaluate_string_pairs method if you want to use the evaluator asynchronously).\n\nIn this example, you will make a simple custom evaluator that just returns whether the first prediction has more whitespace tokenized 'words' than the second.\n\nYou can check out the reference docs for the PairwiseStringEvaluator interface for more info.\n\nfrom typing import Any, Optional\n\nfrom langchain.evaluation import PairwiseStringEvaluator\n\n\nclass LengthComparisonPairwiseEvaluator(PairwiseStringEvaluator):\n    \"\"\"\n    Custom evaluator to compare two strings.\n    \"\"\"\n\n    def _evaluate_string_pairs(\n        self,\n        *,\n        prediction: str,\n        prediction_b: str,\n        reference: Optional[str] = None,\n        input: Optional[str] = None,\n        **kwargs: Any,\n    ) -> dict:\n        score = int(len(prediction.split()) > len(prediction_b.split()))\n        return {\"score\": score}\n\nevaluator = LengthComparisonPairwiseEvaluator()\n\nevaluator.evaluate_string_pairs(\n    prediction=\"The quick brown fox jumped over the lazy dog.\",\n    prediction_b=\"The quick brown fox jumped over the dog.\",\n)\n\n    {'score': 1}\n\nLLM-Based Exampleâ€‹\n\nThat example was simple to illustrate the API, but it wasn't very useful in practice. Below, use an LLM with some custom instructions to form a simple preference scorer similar to the built-in PairwiseStringEvalChain. We will use ChatAnthropic for the evaluator chain.\n\n# %pip install anthropic\n# %env ANTHROPIC_API_KEY=YOUR_API_KEY\n\nfrom typing import Any, Optional\n\nfrom langchain.chains import LLMChain\nfrom langchain.chat_models import ChatAnthropic\nfrom langchain.evaluation import PairwiseStringEvaluator\n\n\nclass CustomPreferenceEvaluator(PairwiseStringEvaluator):\n    \"\"\"\n    Custom evaluator to compare two strings using a custom LLMChain.\n    \"\"\"\n\n    def __init__(self) -> None:\n        llm = ChatAnthropic(model=\"claude-2\", temperature=0)\n        self.eval_chain = LLMChain.from_string(\n            llm,\n            \"\"\"Which option is preferred? Do not take order into account. Evaluate based on accuracy and helpfulness. If neither is preferred, respond with C. Provide your reasoning, then finish with Preference: A/B/C\n\nInput: How do I get the path of the parent directory in python 3.8?\nOption A: You can use the following code:\n```python\nimport os\n\nos.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n\n\nOption B: You can use the following code:\n\nfrom pathlib import Path\nPath(__file__).absolute().parent\n\n\nReasoning: Both options return the same result. However, since option B is more concise and easily understand, it is preferred. Preference: B\n\nWhich option is preferred? Do not take order into account. Evaluate based on accuracy and helpfulness. If neither is preferred, respond with C. Provide your reasoning, then finish with Preference: A/B/C Input: {input} Option A: {prediction} Option B: {prediction_b} Reasoning:\"\"\", )\n\n@property\ndef requires_input(self) -> bool:\n    return True\n\n@property\ndef requires_reference(self) -> bool:\n    return False\n\ndef _evaluate_string_pairs(\n    self,\n    *,\n    prediction: str,\n    prediction_b: str,\n    reference: Optional[str] = None,\n    input: Optional[str] = None,\n    **kwargs: Any,\n) -> dict:\n    result = self.eval_chain(\n        {\n            \"input\": input,\n            \"prediction\": prediction,\n            \"prediction_b\": prediction_b,\n            \"stop\": [\"Which option is preferred?\"],\n        },\n        **kwargs,\n    )\n\n    response_text = result[\"text\"]\n    reasoning, preference = response_text.split(\"Preference:\", maxsplit=1)\n    preference = preference.strip()\n    score = 1.0 if preference == \"A\" else (0.0 if preference == \"B\" else None)\n    return {\"reasoning\": reasoning.strip(), \"value\": preference, \"score\": score}\n\n\n\n```python\nevaluator = CustomPreferenceEvaluator()\n\nevaluator.evaluate_string_pairs(\n    input=\"How do I import from a relative directory?\",\n    prediction=\"use importlib! importlib.import_module('.my_package', '.')\",\n    prediction_b=\"from .sibling import foo\",\n)\n\n    {'reasoning': 'Option B is preferred over option A for importing from a relative directory, because it is more straightforward and concise.\\n\\nOption A uses the importlib module, which allows importing a module by specifying the full name as a string. While this works, it is less clear compared to option B.\\n\\nOption B directly imports from the relative path using dot notation, which clearly shows that it is a relative import. This is the recommended way to do relative imports in Python.\\n\\nIn summary, option B is more accurate and helpful as it uses the standard Python relative import syntax.',\n     'value': 'B',\n     'score': 0.0}\n\n# Setting requires_input to return True adds additional validation to avoid returning a grade when insufficient data is provided to the chain.\n\ntry:\n    evaluator.evaluate_string_pairs(\n        prediction=\"use importlib! importlib.import_module('.my_package', '.')\",\n        prediction_b=\"from .sibling import foo\",\n    )\nexcept ValueError as e:\n    print(e)\n\n    CustomPreferenceEvaluator requires an input string.\n\nPrevious\nPairwise embedding distance\nNext\nTrajectory Evaluators"
}