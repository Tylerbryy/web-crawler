{
	"title": "Modal | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/integrations/providers/modal",
	"html": "ProvidersMoreModal\nModal\n\nThis page covers how to use the Modal ecosystem to run LangChain custom LLMs. It is broken into two parts:\n\nModal installation and web endpoint deployment\nUsing deployed web endpoint with LLM wrapper class.\nInstallation and Setup‚Äã\nInstall with pip install modal\nRun modal token new\nDefine your Modal Functions and Webhooks‚Äã\n\nYou must include a prompt. There is a rigid response structure:\n\nclass Item(BaseModel):\n    prompt: str\n\n@stub.function()\n@modal.web_endpoint(method=\"POST\")\ndef get_text(item: Item):\n    return {\"prompt\": run_gpt2.call(item.prompt)}\n\n\nThe following is an example with the GPT2 model:\n\nfrom pydantic import BaseModel\n\nimport modal\n\nCACHE_PATH = \"/root/model_cache\"\n\nclass Item(BaseModel):\n    prompt: str\n\nstub = modal.Stub(name=\"example-get-started-with-langchain\")\n\ndef download_model():\n    from transformers import GPT2Tokenizer, GPT2LMHeadModel\n    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n    model = GPT2LMHeadModel.from_pretrained('gpt2')\n    tokenizer.save_pretrained(CACHE_PATH)\n    model.save_pretrained(CACHE_PATH)\n\n# Define a container image for the LLM function below, which\n# downloads and stores the GPT-2 model.\nimage = modal.Image.debian_slim().pip_install(\n    \"tokenizers\", \"transformers\", \"torch\", \"accelerate\"\n).run_function(download_model)\n\n@stub.function(\n    gpu=\"any\",\n    image=image,\n    retries=3,\n)\ndef run_gpt2(text: str):\n    from transformers import GPT2Tokenizer, GPT2LMHeadModel\n    tokenizer = GPT2Tokenizer.from_pretrained(CACHE_PATH)\n    model = GPT2LMHeadModel.from_pretrained(CACHE_PATH)\n    encoded_input = tokenizer(text, return_tensors='pt').input_ids\n    output = model.generate(encoded_input, max_length=50, do_sample=True)\n    return tokenizer.decode(output[0], skip_special_tokens=True)\n\n@stub.function()\n@modal.web_endpoint(method=\"POST\")\ndef get_text(item: Item):\n    return {\"prompt\": run_gpt2.call(item.prompt)}\n\nDeploy the web endpoint‚Äã\n\nDeploy the web endpoint to Modal cloud with the modal deploy CLI command. Your web endpoint will acquire a persistent URL under the modal.run domain.\n\nLLM wrapper around Modal web endpoint‚Äã\n\nThe Modal LLM wrapper class which will accept your deployed web endpoint's URL.\n\nfrom langchain.llms import Modal\n\nendpoint_url = \"https://ecorp--custom-llm-endpoint.modal.run\"  # REPLACE ME with your deployed Modal web endpoint's URL\n\nllm = Modal(endpoint_url=endpoint_url)\nllm_chain = LLMChain(prompt=prompt, llm=llm)\n\nquestion = \"What NFL team won the Super Bowl in the year Justin Beiber was born?\"\n\nllm_chain.run(question)\n\nPrevious\nMLflow\nNext\nModelScope"
}