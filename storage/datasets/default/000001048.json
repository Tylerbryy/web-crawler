{
	"title": "Log, Trace, and Monitor | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/integrations/providers/portkey/logging_tracing_portkey",
	"html": "ProvidersMorePortkeyLog, Trace, and Monitor\nLog, Trace, and Monitor\n\nWhen building apps or agents using Langchain, you end up making multiple API calls to fulfill a single user request. However, these requests are not chained when you want to analyse them. With Portkey, all the embeddings, completion, and other requests from a single user request will get logged and traced to a common ID, enabling you to gain full visibility of user interactions.\n\nThis notebook serves as a step-by-step guide on how to log, trace, and monitor Langchain LLM calls using Portkey in your Langchain app.\n\nFirst, let's import Portkey, OpenAI, and Agent tools\n\nimport os\n\nfrom langchain.agents import AgentType, initialize_agent, load_tools\nfrom langchain.llms import OpenAI\nfrom langchain.utilities import Portkey\n\n\nPaste your OpenAI API key below. (You can find it here)\n\nos.environ[\"OPENAI_API_KEY\"] = \"<OPENAI_API_KEY>\"\n\nGet Portkey API Key‚Äã\nSign up for Portkey here\nOn your dashboard, click on the profile icon on the top left, then click on \"Copy API Key\"\nPaste it below\nPORTKEY_API_KEY = \"<PORTKEY_API_KEY>\"  # Paste your Portkey API Key here\n\nSet Trace ID‚Äã\nSet the trace id for your request below\nThe Trace ID can be common for all API calls originating from a single request\nTRACE_ID = \"portkey_langchain_demo\"  # Set trace id here\n\nGenerate Portkey Headers‚Äã\nheaders = Portkey.Config(\n    api_key=PORTKEY_API_KEY,\n    trace_id=TRACE_ID,\n)\n\n\nRun your agent as usual. The only change is that we will include the above headers in the request now.\n\nllm = OpenAI(temperature=0, headers=headers)\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\nagent = initialize_agent(\n    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n)\n\n# Let's test it out!\nagent.run(\n    \"What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?\"\n)\n\nHow Logging & Tracing Works on Portkey‚Äã\n\nLogging\n\nSending your request through Portkey ensures that all of the requests are logged by default\nEach request log contains timestamp, model name, total cost, request time, request json, response json, and additional Portkey features\n\nTracing\n\nTrace id is passed along with each request and is visibe on the logs on Portkey dashboard\nYou can also set a distinct trace id for each request if you want\nYou can append user feedback to a trace id as well. More info on this here\nAdvanced LLMOps Features - Caching, Tagging, Retries‚Äã\n\nIn addition to logging and tracing, Portkey provides more features that add production capabilities to your existing workflows:\n\nCaching\n\nRespond to previously served customers queries from cache instead of sending them again to OpenAI. Match exact strings OR semantically similar strings. Cache can save costs and reduce latencies by 20x.\n\nRetries\n\nAutomatically reprocess any unsuccessful API requests upto 5 times. Uses an exponential backoff strategy, which spaces out retry attempts to prevent network overload.\n\nFeature\tConfig Key\tValue (Type)\nüîÅ Automatic Retries\tretry_count\tinteger [1,2,3,4,5]\nüß† Enabling Cache\tcache\tsimple OR semantic\n\nTagging\n\nTrack and audit ach user interaction in high detail with predefined tags.\n\nTag\tConfig Key\tValue (Type)\nUser Tag\tuser\tstring\nOrganisation Tag\torganisation\tstring\nEnvironment Tag\tenvironment\tstring\nPrompt Tag (version/id/string)\tprompt\tstring\nCode Example With All Features‚Äã\nheaders = Portkey.Config(\n    # Mandatory\n    api_key=\"<PORTKEY_API_KEY>\",\n    # Cache Options\n    cache=\"semantic\",\n    cache_force_refresh=\"True\",\n    cache_age=1729,\n    # Advanced\n    retry_count=5,\n    trace_id=\"langchain_agent\",\n    # Metadata\n    environment=\"production\",\n    user=\"john\",\n    organisation=\"acme\",\n    prompt=\"Frost\",\n)\n\nllm = OpenAI(temperature=0.9, headers=headers)\n\nprint(llm(\"Two roads diverged in the yellow woods\"))\n\nPrevious\nPortkey\nNext\nPredibase"
}