{
	"title": "Runhouse | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/integrations/llms/runhouse",
	"html": "ComponentsLLMsRunhouse\nRunhouse\n\nThe Runhouse allows remote compute and data across environments and users. See the Runhouse docs.\n\nThis example goes over how to use LangChain and Runhouse to interact with models hosted on your own GPU, or on-demand GPUs on AWS, GCP, AWS, or Lambda.\n\nNote: Code uses SelfHosted name instead of the Runhouse.\n\npip install runhouse\n\nimport runhouse as rh\nfrom langchain.chains import LLMChain\nfrom langchain.llms import SelfHostedHuggingFaceLLM, SelfHostedPipeline\nfrom langchain.prompts import PromptTemplate\n\n    INFO | 2023-04-17 16:47:36,173 | No auth token provided, so not using RNS API to save and load configs\n\n# For an on-demand A100 with GCP, Azure, or Lambda\ngpu = rh.cluster(name=\"rh-a10x\", instance_type=\"A100:1\", use_spot=False)\n\n# For an on-demand A10G with AWS (no single A100s on AWS)\n# gpu = rh.cluster(name='rh-a10x', instance_type='g5.2xlarge', provider='aws')\n\n# For an existing cluster\n# gpu = rh.cluster(ips=['<ip of the cluster>'],\n#                  ssh_creds={'ssh_user': '...', 'ssh_private_key':'<path_to_key>'},\n#                  name='rh-a10x')\n\ntemplate = \"\"\"Question: {question}\n\nAnswer: Let's think step by step.\"\"\"\n\nprompt = PromptTemplate(template=template, input_variables=[\"question\"])\n\nllm = SelfHostedHuggingFaceLLM(\n    model_id=\"gpt2\", hardware=gpu, model_reqs=[\"pip:./\", \"transformers\", \"torch\"]\n)\n\nllm_chain = LLMChain(prompt=prompt, llm=llm)\n\nquestion = \"What NFL team won the Super Bowl in the year Justin Beiber was born?\"\n\nllm_chain.run(question)\n\n    INFO | 2023-02-17 05:42:23,537 | Running _generate_text via gRPC\n    INFO | 2023-02-17 05:42:24,016 | Time to send message: 0.48 seconds\n\n\n\n\n\n    \"\\n\\nLet's say we're talking sports teams who won the Super Bowl in the year Justin Beiber\"\n\n\nYou can also load more custom models through the SelfHostedHuggingFaceLLM interface:\n\nllm = SelfHostedHuggingFaceLLM(\n    model_id=\"google/flan-t5-small\",\n    task=\"text2text-generation\",\n    hardware=gpu,\n)\n\nllm(\"What is the capital of Germany?\")\n\n    INFO | 2023-02-17 05:54:21,681 | Running _generate_text via gRPC\n    INFO | 2023-02-17 05:54:21,937 | Time to send message: 0.25 seconds\n\n\n\n\n\n    'berlin'\n\n\nUsing a custom load function, we can load a custom pipeline directly on the remote hardware:\n\ndef load_pipeline():\n    from transformers import (\n        AutoModelForCausalLM,\n        AutoTokenizer,\n        pipeline,\n    )\n\n    model_id = \"gpt2\"\n    tokenizer = AutoTokenizer.from_pretrained(model_id)\n    model = AutoModelForCausalLM.from_pretrained(model_id)\n    pipe = pipeline(\n        \"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=10\n    )\n    return pipe\n\n\ndef inference_fn(pipeline, prompt, stop=None):\n    return pipeline(prompt)[0][\"generated_text\"][len(prompt) :]\n\nllm = SelfHostedHuggingFaceLLM(\n    model_load_fn=load_pipeline, hardware=gpu, inference_fn=inference_fn\n)\n\nllm(\"Who is the current US president?\")\n\n    INFO | 2023-02-17 05:42:59,219 | Running _generate_text via gRPC\n    INFO | 2023-02-17 05:42:59,522 | Time to send message: 0.3 seconds\n\n\n\n\n\n    'john w. bush'\n\n\nYou can send your pipeline directly over the wire to your model, but this will only work for small models (<2 Gb), and will be pretty slow:\n\npipeline = load_pipeline()\nllm = SelfHostedPipeline.from_pipeline(\n    pipeline=pipeline, hardware=gpu, model_reqs=model_reqs\n)\n\n\nInstead, we can also send it to the hardware's filesystem, which will be much faster.\n\nrh.blob(pickle.dumps(pipeline), path=\"models/pipeline.pkl\").save().to(\n    gpu, path=\"models\"\n)\n\nllm = SelfHostedPipeline.from_pipeline(pipeline=\"models/pipeline.pkl\", hardware=gpu)\n\nPrevious\nReplicate\nNext\nSageMakerEndpoint"
}