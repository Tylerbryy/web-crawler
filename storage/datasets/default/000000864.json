{
	"title": "Self Hosted | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/integrations/text_embedding/self-hosted",
	"html": "ComponentsText embedding modelsSelf Hosted\nSelf Hosted\n\nLet's load the SelfHostedEmbeddings, SelfHostedHuggingFaceEmbeddings, and SelfHostedHuggingFaceInstructEmbeddings classes.\n\nimport runhouse as rh\nfrom langchain.embeddings import (\n    SelfHostedEmbeddings,\n    SelfHostedHuggingFaceEmbeddings,\n    SelfHostedHuggingFaceInstructEmbeddings,\n)\n\n# For an on-demand A100 with GCP, Azure, or Lambda\ngpu = rh.cluster(name=\"rh-a10x\", instance_type=\"A100:1\", use_spot=False)\n\n# For an on-demand A10G with AWS (no single A100s on AWS)\n# gpu = rh.cluster(name='rh-a10x', instance_type='g5.2xlarge', provider='aws')\n\n# For an existing cluster\n# gpu = rh.cluster(ips=['<ip of the cluster>'],\n#                  ssh_creds={'ssh_user': '...', 'ssh_private_key':'<path_to_key>'},\n#                  name='my-cluster')\n\nembeddings = SelfHostedHuggingFaceEmbeddings(hardware=gpu)\n\ntext = \"This is a test document.\"\n\nquery_result = embeddings.embed_query(text)\n\n\nAnd similarly for SelfHostedHuggingFaceInstructEmbeddings:\n\nembeddings = SelfHostedHuggingFaceInstructEmbeddings(hardware=gpu)\n\n\nNow let's load an embedding model with a custom load function:\n\ndef get_pipeline():\n    from transformers import (\n        AutoModelForCausalLM,\n        AutoTokenizer,\n        pipeline,\n    )\n\n    model_id = \"facebook/bart-base\"\n    tokenizer = AutoTokenizer.from_pretrained(model_id)\n    model = AutoModelForCausalLM.from_pretrained(model_id)\n    return pipeline(\"feature-extraction\", model=model, tokenizer=tokenizer)\n\n\ndef inference_fn(pipeline, prompt):\n    # Return last hidden state of the model\n    if isinstance(prompt, list):\n        return [emb[0][-1] for emb in pipeline(prompt)]\n    return pipeline(prompt)[0][-1]\n\nembeddings = SelfHostedEmbeddings(\n    model_load_fn=get_pipeline,\n    hardware=gpu,\n    model_reqs=[\"./\", \"torch\", \"transformers\"],\n    inference_fn=inference_fn,\n)\n\nquery_result = embeddings.embed_query(text)\n\nPrevious\nSageMaker\nNext\nSentence Transformers on Hugging Face"
}