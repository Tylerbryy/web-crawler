{
	"title": "Johnsnowlabs | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/integrations/providers/johnsnowlabs",
	"html": "ProvidersMoreJohnsnowlabs\nJohnsnowlabs\n\nGain access to the johnsnowlabs ecosystem of enterprise NLP libraries with over 21.000 enterprise NLP models in over 200 languages with the open source johnsnowlabs library. For all 24.000+ models, see the John Snow Labs Model Models Hub\n\nInstallation and Setup‚Äã\npip install johnsnowlabs\n\n\nTo [install enterprise features](https://nlp.johnsnowlabs.com/docs/en/jsl/install_licensed_quick, run:\n\n# for more details see https://nlp.johnsnowlabs.com/docs/en/jsl/install_licensed_quick\nnlp.install()\n\n\nYou can embed your queries and documents with either gpu,cpu,apple_silicon,aarch based optimized binaries. By default cpu binaries are used. Once a session is started, you must restart your notebook to switch between GPU or CPU, or changes will not take effect.\n\nEmbed Query with CPU:‚Äã\ndocument = \"foo bar\"\nembedding = JohnSnowLabsEmbeddings('embed_sentence.bert')\noutput = embedding.embed_query(document)\n\nEmbed Query with GPU:‚Äã\ndocument = \"foo bar\"\nembedding = JohnSnowLabsEmbeddings('embed_sentence.bert','gpu')\noutput = embedding.embed_query(document)\n\nEmbed Query with Apple Silicon (M1,M2,etc..):‚Äã\ndocuments = [\"foo bar\", 'bar foo']\nembedding = JohnSnowLabsEmbeddings('embed_sentence.bert','apple_silicon')\noutput = embedding.embed_query(document)\n\nEmbed Query with AARCH:‚Äã\ndocuments = [\"foo bar\", 'bar foo']\nembedding = JohnSnowLabsEmbeddings('embed_sentence.bert','aarch')\noutput = embedding.embed_query(document)\n\nEmbed Document with CPU:‚Äã\ndocuments = [\"foo bar\", 'bar foo']\nembedding = JohnSnowLabsEmbeddings('embed_sentence.bert','gpu')\noutput = embedding.embed_documents(documents)\n\nEmbed Document with GPU:‚Äã\ndocuments = [\"foo bar\", 'bar foo']\nembedding = JohnSnowLabsEmbeddings('embed_sentence.bert','gpu')\noutput = embedding.embed_documents(documents)\n\nEmbed Document with Apple Silicon (M1,M2,etc..):‚Äã\n\n```python\ndocuments = [\"foo bar\", 'bar foo']\nembedding = JohnSnowLabsEmbeddings('embed_sentence.bert','apple_silicon')\noutput = embedding.embed_documents(documents)\n\nEmbed Document with AARCH:‚Äã\n\n```python\ndocuments = [\"foo bar\", 'bar foo']\nembedding = JohnSnowLabsEmbeddings('embed_sentence.bert','aarch')\noutput = embedding.embed_documents(documents)\n\n\nModels are loaded with nlp.load and spark session is started with nlp.start() under the hood.\n\nPrevious\nJina\nNext\nKonko"
}