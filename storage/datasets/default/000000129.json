{
	"title": "RAG | ðŸ¦œï¸ðŸ”— Langchain",
	"url": "https://python.langchain.com/docs/expression_language/cookbook/retrieval",
	"html": "LangChain Expression LanguageCookbookRAG\nRAG\n\nLet's look at adding in a retrieval step to a prompt and LLM, which adds up to a \"retrieval-augmented generation\" chain\n\npip install langchain openai faiss-cpu tiktoken\n\nfrom operator import itemgetter\n\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.schema.output_parser import StrOutputParser\nfrom langchain.schema.runnable import RunnableLambda, RunnablePassthrough\nfrom langchain.vectorstores import FAISS\n\nvectorstore = FAISS.from_texts(\n    [\"harrison worked at kensho\"], embedding=OpenAIEmbeddings()\n)\nretriever = vectorstore.as_retriever()\n\ntemplate = \"\"\"Answer the question based only on the following context:\n{context}\n\nQuestion: {question}\n\"\"\"\nprompt = ChatPromptTemplate.from_template(template)\n\nmodel = ChatOpenAI()\n\nchain = (\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\n    | prompt\n    | model\n    | StrOutputParser()\n)\n\nchain.invoke(\"where did harrison work?\")\n\n    'Harrison worked at Kensho.'\n\ntemplate = \"\"\"Answer the question based only on the following context:\n{context}\n\nQuestion: {question}\n\nAnswer in the following language: {language}\n\"\"\"\nprompt = ChatPromptTemplate.from_template(template)\n\nchain = (\n    {\n        \"context\": itemgetter(\"question\") | retriever,\n        \"question\": itemgetter(\"question\"),\n        \"language\": itemgetter(\"language\"),\n    }\n    | prompt\n    | model\n    | StrOutputParser()\n)\n\nchain.invoke({\"question\": \"where did harrison work\", \"language\": \"italian\"})\n\n    'Harrison ha lavorato a Kensho.'\n\nConversational Retrieval Chainâ€‹\n\nWe can easily add in conversation history. This primarily means adding in chat_message_history\n\nfrom langchain.schema import format_document\nfrom langchain.schema.runnable import RunnableMap\n\nfrom langchain.prompts.prompt import PromptTemplate\n\n_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n\nChat History:\n{chat_history}\nFollow Up Input: {question}\nStandalone question:\"\"\"\nCONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n\ntemplate = \"\"\"Answer the question based only on the following context:\n{context}\n\nQuestion: {question}\n\"\"\"\nANSWER_PROMPT = ChatPromptTemplate.from_template(template)\n\nDEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{page_content}\")\n\n\ndef _combine_documents(\n    docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\n\\n\"\n):\n    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n    return document_separator.join(doc_strings)\n\nfrom typing import List, Tuple\n\n\ndef _format_chat_history(chat_history: List[Tuple]) -> str:\n    buffer = \"\"\n    for dialogue_turn in chat_history:\n        human = \"Human: \" + dialogue_turn[0]\n        ai = \"Assistant: \" + dialogue_turn[1]\n        buffer += \"\\n\" + \"\\n\".join([human, ai])\n    return buffer\n\n_inputs = RunnableMap(\n    standalone_question=RunnablePassthrough.assign(\n        chat_history=lambda x: _format_chat_history(x[\"chat_history\"])\n    )\n    | CONDENSE_QUESTION_PROMPT\n    | ChatOpenAI(temperature=0)\n    | StrOutputParser(),\n)\n_context = {\n    \"context\": itemgetter(\"standalone_question\") | retriever | _combine_documents,\n    \"question\": lambda x: x[\"standalone_question\"],\n}\nconversational_qa_chain = _inputs | _context | ANSWER_PROMPT | ChatOpenAI()\n\nconversational_qa_chain.invoke(\n    {\n        \"question\": \"where did harrison work?\",\n        \"chat_history\": [],\n    }\n)\n\n    AIMessage(content='Harrison was employed at Kensho.', additional_kwargs={}, example=False)\n\nconversational_qa_chain.invoke(\n    {\n        \"question\": \"where did he work?\",\n        \"chat_history\": [(\"Who wrote this notebook?\", \"Harrison\")],\n    }\n)\n\n    AIMessage(content='Harrison worked at Kensho.', additional_kwargs={}, example=False)\n\nWith Memory and returning source documentsâ€‹\n\nThis shows how to use memory with the above. For memory, we need to manage that outside at the memory. For returning the retrieved documents, we just need to pass them through all the way.\n\nfrom operator import itemgetter\n\nfrom langchain.memory import ConversationBufferMemory\n\nmemory = ConversationBufferMemory(\n    return_messages=True, output_key=\"answer\", input_key=\"question\"\n)\n\n# First we add a step to load memory\n# This adds a \"memory\" key to the input object\nloaded_memory = RunnablePassthrough.assign(\n    chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"),\n)\n# Now we calculate the standalone question\nstandalone_question = {\n    \"standalone_question\": {\n        \"question\": lambda x: x[\"question\"],\n        \"chat_history\": lambda x: _format_chat_history(x[\"chat_history\"]),\n    }\n    | CONDENSE_QUESTION_PROMPT\n    | ChatOpenAI(temperature=0)\n    | StrOutputParser(),\n}\n# Now we retrieve the documents\nretrieved_documents = {\n    \"docs\": itemgetter(\"standalone_question\") | retriever,\n    \"question\": lambda x: x[\"standalone_question\"],\n}\n# Now we construct the inputs for the final prompt\nfinal_inputs = {\n    \"context\": lambda x: _combine_documents(x[\"docs\"]),\n    \"question\": itemgetter(\"question\"),\n}\n# And finally, we do the part that returns the answers\nanswer = {\n    \"answer\": final_inputs | ANSWER_PROMPT | ChatOpenAI(),\n    \"docs\": itemgetter(\"docs\"),\n}\n# And now we put it all together!\nfinal_chain = loaded_memory | standalone_question | retrieved_documents | answer\n\ninputs = {\"question\": \"where did harrison work?\"}\nresult = final_chain.invoke(inputs)\nresult\n\n    {'answer': AIMessage(content='Harrison was employed at Kensho.', additional_kwargs={}, example=False),\n     'docs': [Document(page_content='harrison worked at kensho', metadata={})]}\n\n# Note that the memory does not save automatically\n# This will be improved in the future\n# For now you need to save it yourself\nmemory.save_context(inputs, {\"answer\": result[\"answer\"].content})\n\nmemory.load_memory_variables({})\n\n    {'history': [HumanMessage(content='where did harrison work?', additional_kwargs={}, example=False),\n      AIMessage(content='Harrison was employed at Kensho.', additional_kwargs={}, example=False)]}\n\nPrevious\nPrompt + LLM\nNext\nMultiple chains"
}