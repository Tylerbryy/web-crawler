{
	"title": "C Transformers | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/integrations/providers/ctransformers",
	"html": "ProvidersMoreC Transformers\nC Transformers\n\nThis page covers how to use the C Transformers library within LangChain. It is broken into two parts: installation and setup, and then references to specific C Transformers wrappers.\n\nInstallation and Setup‚Äã\nInstall the Python package with pip install ctransformers\nDownload a supported GGML model (see Supported Models)\nWrappers‚Äã\nLLM‚Äã\n\nThere exists a CTransformers LLM wrapper, which you can access with:\n\nfrom langchain.llms import CTransformers\n\n\nIt provides a unified interface for all models:\n\nllm = CTransformers(model='/path/to/ggml-gpt-2.bin', model_type='gpt2')\n\nprint(llm('AI is going to'))\n\n\nIf you are getting illegal instruction error, try using lib='avx' or lib='basic':\n\nllm = CTransformers(model='/path/to/ggml-gpt-2.bin', model_type='gpt2', lib='avx')\n\n\nIt can be used with models hosted on the Hugging Face Hub:\n\nllm = CTransformers(model='marella/gpt-2-ggml')\n\n\nIf a model repo has multiple model files (.bin files), specify a model file using:\n\nllm = CTransformers(model='marella/gpt-2-ggml', model_file='ggml-model.bin')\n\n\nAdditional parameters can be passed using the config parameter:\n\nconfig = {'max_new_tokens': 256, 'repetition_penalty': 1.1}\n\nllm = CTransformers(model='marella/gpt-2-ggml', config=config)\n\n\nSee Documentation for a list of available parameters.\n\nFor a more detailed walkthrough of this, see this notebook.\n\nPrevious\nConfluence\nNext\nDashVector"
}