{
	"title": "Logical Fallacy chain | 🦜️🔗 Langchain",
	"url": "https://python.langchain.com/docs/guides/safety/logical_fallacy_chain",
	"html": "SafetyLogical Fallacy chain\nLogical Fallacy chain\n\nThis example shows how to remove logical fallacies from model output.\n\nLogical Fallacies​\n\nLogical fallacies are flawed reasoning or false arguments that can undermine the validity of a model's outputs.\n\nExamples include circular reasoning, false dichotomies, ad hominem attacks, etc. Machine learning models are optimized to perform well on specific metrics like accuracy, perplexity, or loss. However, optimizing for metrics alone does not guarantee logically sound reasoning.\n\nLanguage models can learn to exploit flaws in reasoning to generate plausible-sounding but logically invalid arguments. When models rely on fallacies, their outputs become unreliable and untrustworthy, even if they achieve high scores on metrics. Users cannot depend on such outputs. Propagating logical fallacies can spread misinformation, confuse users, and lead to harmful real-world consequences when models are deployed in products or services.\n\nMonitoring and testing specifically for logical flaws is challenging unlike other quality issues. It requires reasoning about arguments rather than pattern matching.\n\nTherefore, it is crucial that model developers proactively address logical fallacies after optimizing metrics. Specialized techniques like causal modeling, robustness testing, and bias mitigation can help avoid flawed reasoning. Overall, allowing logical flaws to persist makes models less safe and ethical. Eliminating fallacies ensures model outputs remain logically valid and aligned with human reasoning. This maintains user trust and mitigates risks.\n\nExample​\n# Imports\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains.llm import LLMChain\nfrom langchain_experimental.fallacy_removal.base import FallacyChain\n\n# Example of a model output being returned with a logical fallacy\nmisleading_prompt = PromptTemplate(\n    template=\"\"\"You have to respond by using only logical fallacies inherent in your answer explanations.\n\nQuestion: {question}\n\nBad answer:\"\"\",\n    input_variables=[\"question\"],\n)\n\nllm = OpenAI(temperature=0)\nmisleading_chain = LLMChain(llm=llm, prompt=misleading_prompt)\nmisleading_chain.run(question=\"How do I know the earth is round?\")\n\n    'The earth is round because my professor said it is, and everyone believes my professor'\n\nfallacies = FallacyChain.get_fallacies([\"correction\"])\nfallacy_chain = FallacyChain.from_llm(\n    chain=misleading_chain,\n    logical_fallacies=fallacies,\n    llm=llm,\n    verbose=True,\n)\n\nfallacy_chain.run(question=\"How do I know the earth is round?\")\n\n\n\n    > Entering new FallacyChain chain...\n    Initial response:  The earth is round because my professor said it is, and everyone believes my professor.\n\n    Applying correction...\n\n    Fallacy Critique: The model's response uses an appeal to authority and ad populum (everyone believes the professor). Fallacy Critique Needed.\n\n    Updated response: You can find evidence of a round earth due to empirical evidence like photos from space, observations of ships disappearing over the horizon, seeing the curved shadow on the moon, or the ability to circumnavigate the globe.\n\n\n    > Finished chain.\n\n\n\n\n\n    'You can find evidence of a round earth due to empirical evidence like photos from space, observations of ships disappearing over the horizon, seeing the curved shadow on the moon, or the ability to circumnavigate the globe.'\n\nPrevious\nHugging Face prompt injection identification\nNext\nModeration chain"
}