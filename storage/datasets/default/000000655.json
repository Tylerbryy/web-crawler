{
	"title": "Databricks | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/integrations/providers/databricks",
	"html": "ProvidersMoreDatabricks\nDatabricks\n\nThe Databricks Lakehouse Platform unifies data, analytics, and AI on one platform.\n\nDatabricks embraces the LangChain ecosystem in various ways:\n\nDatabricks connector for the SQLDatabase Chain: SQLDatabase.from_databricks() provides an easy way to query your data on Databricks through LangChain\nDatabricks MLflow integrates with LangChain: Tracking and serving LangChain applications with fewer steps\nDatabricks MLflow AI Gateway\nDatabricks as an LLM provider: Deploy your fine-tuned LLMs on Databricks via serving endpoints or cluster driver proxy apps, and query it as langchain.llms.Databricks\nDatabricks Dolly: Databricks open-sourced Dolly which allows for commercial use, and can be accessed through the Hugging Face Hub\nDatabricks connector for the SQLDatabase Chain‚Äã\n\nYou can connect to Databricks runtimes and Databricks SQL using the SQLDatabase wrapper of LangChain. See the notebook Connect to Databricks for details.\n\nDatabricks MLflow integrates with LangChain‚Äã\n\nMLflow is an open-source platform to manage the ML lifecycle, including experimentation, reproducibility, deployment, and a central model registry. See the notebook MLflow Callback Handler for details about MLflow's integration with LangChain.\n\nDatabricks provides a fully managed and hosted version of MLflow integrated with enterprise security features, high availability, and other Databricks workspace features such as experiment and run management and notebook revision capture. MLflow on Databricks offers an integrated experience for tracking and securing machine learning model training runs and running machine learning projects. See MLflow guide for more details.\n\nDatabricks MLflow makes it more convenient to develop LangChain applications on Databricks. For MLflow tracking, you don't need to set the tracking uri. For MLflow Model Serving, you can save LangChain Chains in the MLflow langchain flavor, and then register and serve the Chain with a few clicks on Databricks, with credentials securely managed by MLflow Model Serving.\n\nDatabricks MLflow AI Gateway‚Äã\n\nSee MLflow AI Gateway.\n\nDatabricks as an LLM provider‚Äã\n\nThe notebook Wrap Databricks endpoints as LLMs illustrates the method to wrap Databricks endpoints as LLMs in LangChain. It supports two types of endpoints: the serving endpoint, which is recommended for both production and development, and the cluster driver proxy app, which is recommended for interactive development.\n\nDatabricks endpoints support Dolly, but are also great for hosting models like MPT-7B or any other models from the Hugging Face ecosystem. Databricks endpoints can also be used with proprietary models like OpenAI to provide a governance layer for enterprises.\n\nDatabricks Dolly‚Äã\n\nDatabricks‚Äô Dolly is an instruction-following large language model trained on the Databricks machine learning platform that is licensed for commercial use. The model is available on Hugging Face Hub as databricks/dolly-v2-12b. See the notebook Hugging Face Hub for instructions to access it through the Hugging Face Hub integration with LangChain.\n\nPrevious\nDashVector\nNext\nDatadog Tracing"
}