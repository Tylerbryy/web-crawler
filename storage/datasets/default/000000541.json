{
	"title": "DeepSparse | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/integrations/llms/deepsparse",
	"html": "ComponentsLLMsDeepSparse\nDeepSparse\n\nThis page covers how to use the DeepSparse inference runtime within LangChain. It is broken into two parts: installation and setup, and then examples of DeepSparse usage.\n\nInstallation and Setup‚Äã\nInstall the Python package with pip install deepsparse\nChoose a SparseZoo model or export a support model to ONNX using Optimum\n\nThere exists a DeepSparse LLM wrapper, that provides a unified interface for all models:\n\nfrom langchain.llms import DeepSparse\n\nllm = DeepSparse(\n    model=\"zoo:nlg/text_generation/codegen_mono-350m/pytorch/huggingface/bigpython_bigquery_thepile/base-none\"\n)\n\nprint(llm(\"def fib():\"))\n\n\nAdditional parameters can be passed using the config parameter:\n\nconfig = {\"max_generated_tokens\": 256}\n\nllm = DeepSparse(\n    model=\"zoo:nlg/text_generation/codegen_mono-350m/pytorch/huggingface/bigpython_bigquery_thepile/base-none\",\n    config=config,\n)\n\nPrevious\nDeepInfra\nNext\nEden AI"
}