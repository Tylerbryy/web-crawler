{
	"title": "String Distance | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/guides/evaluation/string/string_distance",
	"html": "EvaluationString EvaluatorsString Distance\nString Distance\n\nOne of the simplest ways to compare an LLM or chain's string output against a reference label is by using string distance measurements such as Levenshtein or postfix distance. This can be used alongside approximate/fuzzy matching criteria for very basic unit testing.\n\nThis can be accessed using the string_distance evaluator, which uses distance metric's from the rapidfuzz library.\n\nNote: The returned scores are distances, meaning lower is typically \"better\".\n\nFor more information, check out the reference docs for the StringDistanceEvalChain for more info.\n\n# %pip install rapidfuzz\n\nfrom langchain.evaluation import load_evaluator\n\nevaluator = load_evaluator(\"string_distance\")\n\nevaluator.evaluate_strings(\n    prediction=\"The job is completely done.\",\n    reference=\"The job is done\",\n)\n\n    {'score': 0.11555555555555552}\n\n# The results purely character-based, so it's less useful when negation is concerned\nevaluator.evaluate_strings(\n    prediction=\"The job is done.\",\n    reference=\"The job isn't done\",\n)\n\n    {'score': 0.0724999999999999}\n\nConfigure the String Distance Metric‚Äã\n\nBy default, the StringDistanceEvalChain uses levenshtein distance, but it also supports other string distance algorithms. Configure using the distance argument.\n\nfrom langchain.evaluation import StringDistance\n\nlist(StringDistance)\n\n    [<StringDistance.DAMERAU_LEVENSHTEIN: 'damerau_levenshtein'>,\n     <StringDistance.LEVENSHTEIN: 'levenshtein'>,\n     <StringDistance.JARO: 'jaro'>,\n     <StringDistance.JARO_WINKLER: 'jaro_winkler'>]\n\njaro_evaluator = load_evaluator(\"string_distance\", distance=StringDistance.JARO)\n\njaro_evaluator.evaluate_strings(\n    prediction=\"The job is completely done.\",\n    reference=\"The job is done\",\n)\n\n    {'score': 0.19259259259259254}\n\njaro_evaluator.evaluate_strings(\n    prediction=\"The job is done.\",\n    reference=\"The job isn't done\",\n)\n\n    {'score': 0.12083333333333324}\n\nPrevious\nScoring Evaluator\nNext\nComparison Evaluators"
}