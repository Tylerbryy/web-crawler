{
	"title": "OpenLLM | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/integrations/llms/openllm",
	"html": "ComponentsLLMsOpenLLM\nOpenLLM\n\nü¶æ OpenLLM is an open platform for operating large language models (LLMs) in production. It enables developers to easily run inference with any open-source LLMs, deploy to the cloud or on-premises, and build powerful AI apps.\n\nInstallation‚Äã\n\nInstall openllm through PyPI\n\npip install openllm\n\nLaunch OpenLLM server locally‚Äã\n\nTo start an LLM server, use openllm start command. For example, to start a dolly-v2 server, run the following command from a terminal:\n\nopenllm start dolly-v2\n\nWrapper‚Äã\nfrom langchain.llms import OpenLLM\n\nserver_url = \"http://localhost:3000\"  # Replace with remote host if you are running on a remote server\nllm = OpenLLM(server_url=server_url)\n\nOptional: Local LLM Inference‚Äã\n\nYou may also choose to initialize an LLM managed by OpenLLM locally from current process. This is useful for development purpose and allows developers to quickly try out different types of LLMs.\n\nWhen moving LLM applications to production, we recommend deploying the OpenLLM server separately and access via the server_url option demonstrated above.\n\nTo load an LLM locally via the LangChain wrapper:\n\nfrom langchain.llms import OpenLLM\n\nllm = OpenLLM(\n    model_name=\"dolly-v2\",\n    model_id=\"databricks/dolly-v2-3b\",\n    temperature=0.94,\n    repetition_penalty=1.2,\n)\n\nIntegrate with a LLMChain‚Äã\nfrom langchain.chains import LLMChain\nfrom langchain.prompts import PromptTemplate\n\ntemplate = \"What is a good name for a company that makes {product}?\"\n\nprompt = PromptTemplate(template=template, input_variables=[\"product\"])\n\nllm_chain = LLMChain(prompt=prompt, llm=llm)\n\ngenerated = llm_chain.run(product=\"mechanical keyboard\")\nprint(generated)\n\n    iLkb\n\nPrevious\nOpenAI\nNext\nOpenLM"
}