{
	"title": "Chat Over Documents with Vectara | ðŸ¦œï¸ðŸ”— Langchain",
	"url": "https://python.langchain.com/docs/integrations/providers/vectara/vectara_chat",
	"html": "ProvidersMoreVectaraChat Over Documents with Vectara\nChat Over Documents with Vectara\n\nThis notebook is based on the chat_vector_db notebook, but using Vectara as the vector database.\n\nimport os\n\nfrom langchain.chains import ConversationalRetrievalChain\nfrom langchain.llms import OpenAI\nfrom langchain.vectorstores import Vectara\n\n\nLoad in documents. You can replace this with a loader for whatever type of data you want\n\nfrom langchain.document_loaders import TextLoader\n\nloader = TextLoader(\"../../../modules/state_of_the_union.txt\")\ndocuments = loader.load()\n\n\nWe now split the documents, create embeddings for them, and put them in a vectorstore. This allows us to do semantic search over them.\n\nvectorstore = Vectara.from_documents(documents, embedding=None)\n\n\nWe can now create a memory object, which is neccessary to track the inputs/outputs and hold a conversation.\n\nfrom langchain.memory import ConversationBufferMemory\n\nmemory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n\n\nWe now initialize the ConversationalRetrievalChain\n\nopenai_api_key = os.environ[\"OPENAI_API_KEY\"]\nllm = OpenAI(openai_api_key=openai_api_key, temperature=0)\nretriever = vectorstore.as_retriever(lambda_val=0.025, k=5, filter=None)\nd = retriever.get_relevant_documents(\n    \"What did the president say about Ketanji Brown Jackson\"\n)\n\nqa = ConversationalRetrievalChain.from_llm(llm, retriever, memory=memory)\n\nquery = \"What did the president say about Ketanji Brown Jackson\"\nresult = qa({\"question\": query})\n\nresult[\"answer\"]\n\n    \" The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice, and that she will continue Justice Breyer's legacy of excellence.\"\n\nquery = \"Did he mention who she succeeded\"\nresult = qa({\"question\": query})\n\nresult[\"answer\"]\n\n    ' Ketanji Brown Jackson succeeded Justice Breyer.'\n\nPass in chat historyâ€‹\n\nIn the above example, we used a Memory object to track chat history. We can also just pass it in explicitly. In order to do this, we need to initialize a chain without any memory object.\n\nqa = ConversationalRetrievalChain.from_llm(\n    OpenAI(temperature=0), vectorstore.as_retriever()\n)\n\n\nHere's an example of asking a question with no chat history\n\nchat_history = []\nquery = \"What did the president say about Ketanji Brown Jackson\"\nresult = qa({\"question\": query, \"chat_history\": chat_history})\n\nresult[\"answer\"]\n\n    \" The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice, and that she will continue Justice Breyer's legacy of excellence.\"\n\n\nHere's an example of asking a question with some chat history\n\nchat_history = [(query, result[\"answer\"])]\nquery = \"Did he mention who she succeeded\"\nresult = qa({\"question\": query, \"chat_history\": chat_history})\n\nresult[\"answer\"]\n\n    ' Ketanji Brown Jackson succeeded Justice Breyer.'\n\nReturn Source Documentsâ€‹\n\nYou can also easily return source documents from the ConversationalRetrievalChain. This is useful for when you want to inspect what documents were returned.\n\nqa = ConversationalRetrievalChain.from_llm(\n    llm, vectorstore.as_retriever(), return_source_documents=True\n)\n\nchat_history = []\nquery = \"What did the president say about Ketanji Brown Jackson\"\nresult = qa({\"question\": query, \"chat_history\": chat_history})\n\nresult[\"source_documents\"][0]\n\n    Document(page_content='Justice Breyer, thank you for your service. One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence. A former top litigator in private practice.', metadata={'source': '../../../modules/state_of_the_union.txt'})\n\nConversationalRetrievalChain with search_distanceâ€‹\n\nIf you are using a vector store that supports filtering by search distance, you can add a threshold value parameter.\n\nvectordbkwargs = {\"search_distance\": 0.9}\n\nqa = ConversationalRetrievalChain.from_llm(\n    OpenAI(temperature=0), vectorstore.as_retriever(), return_source_documents=True\n)\nchat_history = []\nquery = \"What did the president say about Ketanji Brown Jackson\"\nresult = qa(\n    {\"question\": query, \"chat_history\": chat_history, \"vectordbkwargs\": vectordbkwargs}\n)\n\nprint(result[\"answer\"])\n\n     The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice, and that she will continue Justice Breyer's legacy of excellence.\n\nConversationalRetrievalChain with map_reduceâ€‹\n\nWe can also use different types of combine document chains with the ConversationalRetrievalChain chain.\n\nfrom langchain.chains import LLMChain\nfrom langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT\nfrom langchain.chains.question_answering import load_qa_chain\n\nquestion_generator = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT)\ndoc_chain = load_qa_chain(llm, chain_type=\"map_reduce\")\n\nchain = ConversationalRetrievalChain(\n    retriever=vectorstore.as_retriever(),\n    question_generator=question_generator,\n    combine_docs_chain=doc_chain,\n)\n\nchat_history = []\nquery = \"What did the president say about Ketanji Brown Jackson\"\nresult = chain({\"question\": query, \"chat_history\": chat_history})\n\nresult[\"answer\"]\n\n    \" The president said that he nominated Circuit Court of Appeals Judge Ketanji Brown Jackson, who is one of the nation's top legal minds and a former top litigator in private practice.\"\n\nConversationalRetrievalChain with Question Answering with sourcesâ€‹\n\nYou can also use this chain with the question answering with sources chain.\n\nfrom langchain.chains.qa_with_sources import load_qa_with_sources_chain\n\nquestion_generator = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT)\ndoc_chain = load_qa_with_sources_chain(llm, chain_type=\"map_reduce\")\n\nchain = ConversationalRetrievalChain(\n    retriever=vectorstore.as_retriever(),\n    question_generator=question_generator,\n    combine_docs_chain=doc_chain,\n)\n\nchat_history = []\nquery = \"What did the president say about Ketanji Brown Jackson\"\nresult = chain({\"question\": query, \"chat_history\": chat_history})\n\nresult[\"answer\"]\n\n    \" The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice.\\nSOURCES: ../../../modules/state_of_the_union.txt\"\n\nConversationalRetrievalChain with streaming to stdoutâ€‹\n\nOutput from the chain will be streamed to stdout token by token in this example.\n\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\nfrom langchain.chains.conversational_retrieval.prompts import (\n    CONDENSE_QUESTION_PROMPT,\n    QA_PROMPT,\n)\nfrom langchain.chains.llm import LLMChain\nfrom langchain.chains.question_answering import load_qa_chain\n\n# Construct a ConversationalRetrievalChain with a streaming llm for combine docs\n# and a separate, non-streaming llm for question generation\nllm = OpenAI(temperature=0, openai_api_key=openai_api_key)\nstreaming_llm = OpenAI(\n    streaming=True,\n    callbacks=[StreamingStdOutCallbackHandler()],\n    temperature=0,\n    openai_api_key=openai_api_key,\n)\n\nquestion_generator = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT)\ndoc_chain = load_qa_chain(streaming_llm, chain_type=\"stuff\", prompt=QA_PROMPT)\n\nqa = ConversationalRetrievalChain(\n    retriever=vectorstore.as_retriever(),\n    combine_docs_chain=doc_chain,\n    question_generator=question_generator,\n)\n\nchat_history = []\nquery = \"What did the president say about Ketanji Brown Jackson\"\nresult = qa({\"question\": query, \"chat_history\": chat_history})\n\n     The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice, and that she will continue Justice Breyer's legacy of excellence.\n\nchat_history = [(query, result[\"answer\"])]\nquery = \"Did he mention who she succeeded\"\nresult = qa({\"question\": query, \"chat_history\": chat_history})\n\n     Justice Breyer\n\nget_chat_history Functionâ€‹\n\nYou can also specify a get_chat_history function, which can be used to format the chat_history string.\n\ndef get_chat_history(inputs) -> str:\n    res = []\n    for human, ai in inputs:\n        res.append(f\"Human:{human}\\nAI:{ai}\")\n    return \"\\n\".join(res)\n\n\nqa = ConversationalRetrievalChain.from_llm(\n    llm, vectorstore.as_retriever(), get_chat_history=get_chat_history\n)\n\nchat_history = []\nquery = \"What did the president say about Ketanji Brown Jackson\"\nresult = qa({\"question\": query, \"chat_history\": chat_history})\n\nresult[\"answer\"]\n\n    \" The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice, and that she will continue Justice Breyer's legacy of excellence.\"\n\nPrevious\nVectara\nNext\nVectara Text Generation"
}