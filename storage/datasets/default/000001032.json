{
	"title": "LangSmith LLM Runs | 🦜️🔗 Langchain",
	"url": "https://python.langchain.com/docs/integrations/chat_loaders/langsmith_llm_runs",
	"html": "ComponentsChat loadersLangSmith LLM Runs\nLangSmith LLM Runs\n\nThis notebook demonstrates how to directly load data from LangSmith's LLM runs and fine-tune a model on that data. The process is simple and comprises 3 steps.\n\nSelect the LLM runs to train on.\nUse the LangSmithRunChatLoader to load runs as chat sessions.\nFine-tune your model.\n\nThen you can use the fine-tuned model in your LangChain app.\n\nBefore diving in, let's install our prerequisites.\n\nPrerequisites​\n\nEnsure you've installed langchain >= 0.0.311 and have configured your environment with your LangSmith API key.\n\n%pip install -U langchain openai\n\nimport os\nimport uuid\n\nuid = uuid.uuid4().hex[:6]\nproject_name = f\"Run Fine-tuning Walkthrough {uid}\"\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\nos.environ[\"LANGCHAIN_API_KEY\"] = \"YOUR API KEY\"\nos.environ[\"LANGCHAIN_PROJECT\"] = project_name\n\n1. Select Runs​\n\nThe first step is selecting which runs to fine-tune on. A common case would be to select LLM runs within traces that have received positive user feedback. You can find examples of this in theLangSmith Cookbook and in the docs.\n\nFor the sake of this tutorial, we will generate some runs for you to use here. Let's try fine-tuning a simple function-calling chain.\n\nfrom enum import Enum\n\nfrom langchain.pydantic_v1 import BaseModel, Field\n\n\nclass Operation(Enum):\n    add = \"+\"\n    subtract = \"-\"\n    multiply = \"*\"\n    divide = \"/\"\n\n\nclass Calculator(BaseModel):\n    \"\"\"A calculator function\"\"\"\n\n    num1: float\n    num2: float\n    operation: Operation = Field(..., description=\"+,-,*,/\")\n\n    def calculate(self):\n        if self.operation == Operation.add:\n            return self.num1 + self.num2\n        elif self.operation == Operation.subtract:\n            return self.num1 - self.num2\n        elif self.operation == Operation.multiply:\n            return self.num1 * self.num2\n        elif self.operation == Operation.divide:\n            if self.num2 != 0:\n                return self.num1 / self.num2\n            else:\n                return \"Cannot divide by zero\"\n\nfrom pprint import pprint\n\nfrom langchain.pydantic_v1 import BaseModel\nfrom langchain.utils.openai_functions import convert_pydantic_to_openai_function\n\nopenai_function_def = convert_pydantic_to_openai_function(Calculator)\npprint(openai_function_def)\n\n    {'description': 'A calculator function',\n     'name': 'Calculator',\n     'parameters': {'description': 'A calculator function',\n                    'properties': {'num1': {'title': 'Num1', 'type': 'number'},\n                                   'num2': {'title': 'Num2', 'type': 'number'},\n                                   'operation': {'allOf': [{'description': 'An '\n                                                                           'enumeration.',\n                                                            'enum': ['+',\n                                                                     '-',\n                                                                     '*',\n                                                                     '/'],\n                                                            'title': 'Operation'}],\n                                                 'description': '+,-,*,/'}},\n                    'required': ['num1', 'num2', 'operation'],\n                    'title': 'Calculator',\n                    'type': 'object'}}\n\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.output_parsers.openai_functions import PydanticOutputFunctionsParser\nfrom langchain.prompts import ChatPromptTemplate\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are an accounting assistant.\"),\n        (\"user\", \"{input}\"),\n    ]\n)\nchain = (\n    prompt\n    | ChatOpenAI().bind(functions=[openai_function_def])\n    | PydanticOutputFunctionsParser(pydantic_schema=Calculator)\n    | (lambda x: x.calculate())\n)\n\nmath_questions = [\n    \"What's 45/9?\",\n    \"What's 81/9?\",\n    \"What's 72/8?\",\n    \"What's 56/7?\",\n    \"What's 36/6?\",\n    \"What's 64/8?\",\n    \"What's 12*6?\",\n    \"What's 8*8?\",\n    \"What's 10*10?\",\n    \"What's 11*11?\",\n    \"What's 13*13?\",\n    \"What's 45+30?\",\n    \"What's 72+28?\",\n    \"What's 56+44?\",\n    \"What's 63+37?\",\n    \"What's 70-35?\",\n    \"What's 60-30?\",\n    \"What's 50-25?\",\n    \"What's 40-20?\",\n    \"What's 30-15?\",\n]\nresults = chain.batch([{\"input\": q} for q in math_questions], return_exceptions=True)\n\n    Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n\nLoad runs that did not error​\n\nNow we can select the successful runs to fine-tune on.\n\nfrom langsmith.client import Client\n\nclient = Client()\n\nsuccessful_traces = {\n    run.trace_id\n    for run in client.list_runs(\n        project_name=project_name,\n        execution_order=1,\n        error=False,\n    )\n}\n\nllm_runs = [\n    run\n    for run in client.list_runs(\n        project_name=project_name,\n        run_type=\"llm\",\n    )\n    if run.trace_id in successful_traces\n]\n\n2. Prepare data​\n\nNow we can create an instance of LangSmithRunChatLoader and load the chat sessions using its lazy_load() method.\n\nfrom langchain.chat_loaders.langsmith import LangSmithRunChatLoader\n\nloader = LangSmithRunChatLoader(runs=llm_runs)\n\nchat_sessions = loader.lazy_load()\n\nWith the chat sessions loaded, convert them into a format suitable for fine-tuning.​\nfrom langchain.adapters.openai import convert_messages_for_finetuning\n\ntraining_data = convert_messages_for_finetuning(chat_sessions)\n\n3. Fine-tune the model​\n\nNow, initiate the fine-tuning process using the OpenAI library.\n\nimport json\nimport time\nfrom io import BytesIO\n\nimport openai\n\nmy_file = BytesIO()\nfor dialog in training_data:\n    my_file.write((json.dumps({\"messages\": dialog}) + \"\\n\").encode(\"utf-8\"))\n\nmy_file.seek(0)\ntraining_file = openai.File.create(file=my_file, purpose=\"fine-tune\")\n\njob = openai.FineTuningJob.create(\n    training_file=training_file.id,\n    model=\"gpt-3.5-turbo\",\n)\n\n# Wait for the fine-tuning to complete (this may take some time)\nstatus = openai.FineTuningJob.retrieve(job.id).status\nstart_time = time.time()\nwhile status != \"succeeded\":\n    print(f\"Status=[{status}]... {time.time() - start_time:.2f}s\", end=\"\\r\", flush=True)\n    time.sleep(5)\n    status = openai.FineTuningJob.retrieve(job.id).status\n\n# Now your model is fine-tuned!\n\n    Status=[running]... 346.26s. 31.70s\n\n4. Use in LangChain​\n\nAfter fine-tuning, use the resulting model ID with the ChatOpenAI model class in your LangChain app.\n\n# Get the fine-tuned model ID\njob = openai.FineTuningJob.retrieve(job.id)\nmodel_id = job.fine_tuned_model\n\n# Use the fine-tuned model in LangChain\nmodel = ChatOpenAI(\n    model=model_id,\n    temperature=1,\n)\n\n(prompt | model).invoke({\"input\": \"What's 56/7?\"})\n\n    AIMessage(content='{\\n  \"num1\": 56,\\n  \"num2\": 7,\\n  \"operation\": \"/\"\\n}')\n\n\nNow you have successfully fine-tuned a model using data from LangSmith LLM runs!\n\nPrevious\nLangSmith Chat Datasets\nNext\nSlack"
}