{
	"title": "Async callbacks | ðŸ¦œï¸ðŸ”— Langchain",
	"url": "https://python.langchain.com/docs/modules/callbacks/async_callbacks",
	"html": "ModulesMoreCallbacksAsync callbacks\nAsync callbacks\n\nIf you are planning to use the async API, it is recommended to use AsyncCallbackHandler to avoid blocking the runloop.\n\nAdvanced if you use a sync CallbackHandler while using an async method to run your LLM / Chain / Tool / Agent, it will still work. However, under the hood, it will be called with run_in_executor which can cause issues if your CallbackHandler is not thread-safe.\n\nimport asyncio\nfrom typing import Any, Dict, List\n\nfrom langchain.callbacks.base import AsyncCallbackHandler, BaseCallbackHandler\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.schema import HumanMessage, LLMResult\n\n\nclass MyCustomSyncHandler(BaseCallbackHandler):\n    def on_llm_new_token(self, token: str, **kwargs) -> None:\n        print(f\"Sync handler being called in a `thread_pool_executor`: token: {token}\")\n\n\nclass MyCustomAsyncHandler(AsyncCallbackHandler):\n    \"\"\"Async callback handler that can be used to handle callbacks from langchain.\"\"\"\n\n    async def on_llm_start(\n        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any\n    ) -> None:\n        \"\"\"Run when chain starts running.\"\"\"\n        print(\"zzzz....\")\n        await asyncio.sleep(0.3)\n        class_name = serialized[\"name\"]\n        print(\"Hi! I just woke up. Your llm is starting\")\n\n    async def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:\n        \"\"\"Run when chain ends running.\"\"\"\n        print(\"zzzz....\")\n        await asyncio.sleep(0.3)\n        print(\"Hi! I just woke up. Your llm is ending\")\n\n\n# To enable streaming, we pass in `streaming=True` to the ChatModel constructor\n# Additionally, we pass in a list with our custom handler\nchat = ChatOpenAI(\n    max_tokens=25,\n    streaming=True,\n    callbacks=[MyCustomSyncHandler(), MyCustomAsyncHandler()],\n)\n\nawait chat.agenerate([[HumanMessage(content=\"Tell me a joke\")]])\n\n    zzzz....\n    Hi! I just woke up. Your llm is starting\n    Sync handler being called in a `thread_pool_executor`: token: \n    Sync handler being called in a `thread_pool_executor`: token: Why\n    Sync handler being called in a `thread_pool_executor`: token:  don\n    Sync handler being called in a `thread_pool_executor`: token: 't\n    Sync handler being called in a `thread_pool_executor`: token:  scientists\n    Sync handler being called in a `thread_pool_executor`: token:  trust\n    Sync handler being called in a `thread_pool_executor`: token:  atoms\n    Sync handler being called in a `thread_pool_executor`: token: ?\n    Sync handler being called in a `thread_pool_executor`: token:  \n    \n    \n    Sync handler being called in a `thread_pool_executor`: token: Because\n    Sync handler being called in a `thread_pool_executor`: token:  they\n    Sync handler being called in a `thread_pool_executor`: token:  make\n    Sync handler being called in a `thread_pool_executor`: token:  up\n    Sync handler being called in a `thread_pool_executor`: token:  everything\n    Sync handler being called in a `thread_pool_executor`: token: .\n    Sync handler being called in a `thread_pool_executor`: token: \n    zzzz....\n    Hi! I just woke up. Your llm is ending\n\n\n\n\n\n    LLMResult(generations=[[ChatGeneration(text=\"Why don't scientists trust atoms? \\n\\nBecause they make up everything.\", generation_info=None, message=AIMessage(content=\"Why don't scientists trust atoms? \\n\\nBecause they make up everything.\", additional_kwargs={}, example=False))]], llm_output={'token_usage': {}, 'model_name': 'gpt-3.5-turbo'})\n\nPrevious\nCallbacks\nNext\nCustom callback handlers"
}