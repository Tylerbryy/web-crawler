{
	"title": "Hugging Face Hub | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/integrations/llms/huggingface_hub.html",
	"html": "ComponentsLLMsHugging Face Hub\nHugging Face Hub\n\nThe Hugging Face Hub is a platform with over 120k models, 20k datasets, and 50k demo apps (Spaces), all open source and publicly available, in an online platform where people can easily collaborate and build ML together.\n\nThis example showcases how to connect to the Hugging Face Hub and use different models.\n\nInstallation and Setup‚Äã\n\nTo use, you should have the huggingface_hub python package installed.\n\npip install huggingface_hub\n\n# get a token: https://huggingface.co/docs/api-inference/quicktour#get-your-api-token\n\nfrom getpass import getpass\n\nHUGGINGFACEHUB_API_TOKEN = getpass()\n\n     ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n\nimport os\n\nos.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACEHUB_API_TOKEN\n\nPrepare Examples‚Äã\nfrom langchain.llms import HuggingFaceHub\n\nfrom langchain.chains import LLMChain\nfrom langchain.prompts import PromptTemplate\n\nquestion = \"Who won the FIFA World Cup in the year 1994? \"\n\ntemplate = \"\"\"Question: {question}\n\nAnswer: Let's think step by step.\"\"\"\n\nprompt = PromptTemplate(template=template, input_variables=[\"question\"])\n\nExamples‚Äã\n\nBelow are some examples of models you can access through the Hugging Face Hub integration.\n\nFlan, by Google‚Äã\nrepo_id = \"google/flan-t5-xxl\"  # See https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads for some other options\n\nllm = HuggingFaceHub(\n    repo_id=repo_id, model_kwargs={\"temperature\": 0.5, \"max_length\": 64}\n)\nllm_chain = LLMChain(prompt=prompt, llm=llm)\n\nprint(llm_chain.run(question))\n\n    The FIFA World Cup was held in the year 1994. West Germany won the FIFA World Cup in 1994\n\nDolly, by Databricks‚Äã\n\nSee Databricks organization page for a list of available models.\n\nrepo_id = \"databricks/dolly-v2-3b\"\n\nllm = HuggingFaceHub(\n    repo_id=repo_id, model_kwargs={\"temperature\": 0.5, \"max_length\": 64}\n)\nllm_chain = LLMChain(prompt=prompt, llm=llm)\nprint(llm_chain.run(question))\n\n     First of all, the world cup was won by the Germany. Then the Argentina won the world cup in 2022. So, the Argentina won the world cup in 1994.\n    \n    \n    Question: Who\n\nCamel, by Writer‚Äã\n\nSee Writer's organization page for a list of available models.\n\nrepo_id = \"Writer/camel-5b-hf\"  # See https://huggingface.co/Writer for other options\n\nllm = HuggingFaceHub(\n    repo_id=repo_id, model_kwargs={\"temperature\": 0.5, \"max_length\": 64}\n)\nllm_chain = LLMChain(prompt=prompt, llm=llm)\nprint(llm_chain.run(question))\n\nXGen, by Salesforce‚Äã\n\nSee more information.\n\nrepo_id = \"Salesforce/xgen-7b-8k-base\"\n\nllm = HuggingFaceHub(\n    repo_id=repo_id, model_kwargs={\"temperature\": 0.5, \"max_length\": 64}\n)\nllm_chain = LLMChain(prompt=prompt, llm=llm)\nprint(llm_chain.run(question))\n\nFalcon, by Technology Innovation Institute (TII)‚Äã\n\nSee more information.\n\nrepo_id = \"tiiuae/falcon-40b\"\n\nllm = HuggingFaceHub(\n    repo_id=repo_id, model_kwargs={\"temperature\": 0.5, \"max_length\": 64}\n)\nllm_chain = LLMChain(prompt=prompt, llm=llm)\nprint(llm_chain.run(question))\n\nInternLM-Chat, by Shanghai AI Laboratory‚Äã\n\nSee more information.\n\nrepo_id = \"internlm/internlm-chat-7b\"\n\nllm = HuggingFaceHub(\n    repo_id=repo_id, model_kwargs={\"max_length\": 128, \"temperature\": 0.8}\n)\nllm_chain = LLMChain(prompt=prompt, llm=llm)\nprint(llm_chain.run(question))\n\nQwen, by Alibaba Cloud‚Äã\n\nTongyi Qianwen-7B (Qwen-7B) is a model with a scale of 7 billion parameters in the Tongyi Qianwen large model series developed by Alibaba Cloud. Qwen-7B is a large language model based on Transformer, which is trained on ultra-large-scale pre-training data.\n\nSee more information on HuggingFace of on GitHub.\n\nSee here a big example for LangChain integration and Qwen.\n\nrepo_id = \"Qwen/Qwen-7B\"\n\nllm = HuggingFaceHub(\n    repo_id=repo_id, model_kwargs={\"max_length\": 128, \"temperature\": 0.5}\n)\nllm_chain = LLMChain(prompt=prompt, llm=llm)\nprint(llm_chain.run(question))\n\nYi series models, by 01.ai‚Äã\n\nThe Yi series models are large language models trained from scratch by developers at 01.ai. The first public release contains two bilingual(English/Chinese) base models with the parameter sizes of 6B(Yi-6B) and 34B(Yi-34B). Both of them are trained with 4K sequence length and can be extended to 32K during inference time. The Yi-6B-200K and Yi-34B-200K are base model with 200K context length.\n\nHere we test the Yi-34B model.\n\nrepo_id = \"01-ai/Yi-34B\"\n\nllm = HuggingFaceHub(\n    repo_id=repo_id, model_kwargs={\"max_length\": 128, \"temperature\": 0.5}\n)\nllm_chain = LLMChain(prompt=prompt, llm=llm)\nprint(llm_chain.run(question))\n\nPrevious\nGradient\nNext\nHugging Face Local Pipelines"
}