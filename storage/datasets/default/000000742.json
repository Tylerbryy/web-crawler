{
	"title": "Portkey | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/integrations/providers/portkey/",
	"html": "ProvidersMorePortkey\nPortkey\n\nPortkey is a platform designed to streamline the deployment and management of Generative AI applications. It provides comprehensive features for monitoring, managing models, and improving the performance of your AI applications.\n\nLLMOps for Langchain‚Äã\n\nPortkey brings production readiness to Langchain. With Portkey, you can\n\n view detailed metrics & logs for all requests,\n enable semantic cache to reduce latency & costs,\n implement automatic retries & fallbacks for failed requests,\n add custom tags to requests for better tracking and analysis and more.\nUsing Portkey with Langchain‚Äã\n\nUsing Portkey is as simple as just choosing which Portkey features you want, enabling them via headers=Portkey.Config and passing it in your LLM calls.\n\nTo start, get your Portkey API key by signing up here. (Click the profile icon on the top left, then click on \"Copy API Key\")\n\nFor OpenAI, a simple integration with logging feature would look like this:\n\nfrom langchain.llms import OpenAI\nfrom langchain.utilities import Portkey\n\n# Add the Portkey API Key from your account\nheaders = Portkey.Config(\n    api_key = \"<PORTKEY_API_KEY>\"\n)\n\nllm = OpenAI(temperature=0.9, headers=headers)\nllm.predict(\"What would be a good company name for a company that makes colorful socks?\")\n\n\nYour logs will be captured on your Portkey dashboard.\n\nA common Portkey X Langchain use case is to trace a chain or an agent and view all the LLM calls originating from that request.\n\nTracing Chains & Agents‚Äã\nfrom langchain.agents import AgentType, initialize_agent, load_tools  \nfrom langchain.llms import OpenAI\nfrom langchain.utilities import Portkey\n\n# Add the Portkey API Key from your account\nheaders = Portkey.Config(\n    api_key = \"<PORTKEY_API_KEY>\",\n    trace_id = \"fef659\"\n)\n\nllm = OpenAI(temperature=0, headers=headers)  \ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)  \nagent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)  \n  \n# Let's test it out!  \nagent.run(\"What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?\")\n\n\nYou can see the requests' logs along with the trace id on Portkey dashboard:\n\nAdvanced Features‚Äã\nLogging: Log all your LLM requests automatically by sending them through Portkey. Each request log contains timestamp, model name, total cost, request time, request json, response json, and additional Portkey features.\nTracing: Trace id can be passed along with each request and is visibe on the logs on Portkey dashboard. You can also set a distinct trace id for each request. You can append user feedback to a trace id as well.\nCaching: Respond to previously served customers queries from cache instead of sending them again to OpenAI. Match exact strings OR semantically similar strings. Cache can save costs and reduce latencies by 20x.\nRetries: Automatically reprocess any unsuccessful API requests upto 5 times. Uses an exponential backoff strategy, which spaces out retry attempts to prevent network overload.\nTagging: Track and audit each user interaction in high detail with predefined tags.\nFeature\tConfig Key\tValue (Type)\tRequired/Optional\nAPI Key\tapi_key\tAPI Key (string)\t‚úÖ Required\nTracing Requests\ttrace_id\tCustom string\t‚ùî Optional\nAutomatic Retries\tretry_count\tinteger [1,2,3,4,5]\t‚ùî Optional\nEnabling Cache\tcache\tsimple OR semantic\t‚ùî Optional\nCache Force Refresh\tcache_force_refresh\tTrue\t‚ùî Optional\nSet Cache Expiry\tcache_age\tinteger (in seconds)\t‚ùî Optional\nAdd User\tuser\tstring\t‚ùî Optional\nAdd Organisation\torganisation\tstring\t‚ùî Optional\nAdd Environment\tenvironment\tstring\t‚ùî Optional\nAdd Prompt (version/id/string)\tprompt\tstring\t‚ùî Optional\nEnabling all Portkey Features:‚Äã\nheaders = Portkey.Config(\n    \n    # Mandatory\n    api_key=\"<PORTKEY_API_KEY>\",  \n    \n    # Cache Options\n    cache=\"semantic\",                 \n    cache_force_refresh=\"True\",             \n    cache_age=1729,  \n\n    # Advanced\n    retry_count=5,                                           \n    trace_id=\"langchain_agent\",                          \n\n    # Metadata\n    environment=\"production\",        \n    user=\"john\",                      \n    organisation=\"acme\",             \n    prompt=\"Frost\"\n    \n)\n\n\nFor detailed information on each feature and how to use it, please refer to the Portkey docs. If you have any questions or need further assistance, reach out to us on Twitter..\n\nPrevious\nPipelineAI\nNext\nLog, Trace, and Monitor"
}