{
	"title": "Momento Vector Index (MVI) | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/integrations/vectorstores/momento_vector_index",
	"html": "ComponentsVector storesMomento Vector Index (MVI)\nMomento Vector Index (MVI)\n\nMVI: the most productive, easiest to use, serverless vector index for your data. To get started with MVI, simply sign up for an account. There's no need to handle infrastructure, manage servers, or be concerned about scaling. MVI is a service that scales automatically to meet your needs.\n\nTo sign up and access MVI, visit the Momento Console.\n\nSetup\nInstall prerequisites‚Äã\n\nYou will need:\n\nthe momento package for interacting with MVI, and\nthe openai package for interacting with the OpenAI API.\nthe tiktoken package for tokenizing text.\npip install momento openai tiktoken\n\nEnter API keys‚Äã\nimport getpass\nimport os\n\nMomento: for indexing data‚Äã\n\nVisit the Momento Console to get your API key.\n\nos.environ[\"MOMENTO_API_KEY\"] = getpass.getpass(\"Momento API Key:\")\n\nOpenAI: for text embeddings‚Äã\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n\nLoad your data\n\nHere we use the example dataset from Langchain, the state of the union address.\n\nFirst we load relevant modules:\n\nfrom langchain.document_loaders import TextLoader\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.vectorstores import MomentoVectorIndex\n\n\nThen we load the data:\n\nloader = TextLoader(\"../../modules/state_of_the_union.txt\")\ndocuments = loader.load()\nlen(documents)\n\n    1\n\n\nNote the data is one large file, hence there is only one document:\n\nlen(documents[0].page_content)\n\n    38539\n\n\nBecause this is one large text file, we split it into chunks for question answering. That way, user questions will be answered from the most relevant chunk.\n\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\ndocs = text_splitter.split_documents(documents)\nlen(docs)\n\n    42\n\nIndex your data\n\nIndexing your data is as simple as instantiating the MomentoVectorIndex object. Here we use the from_documents helper to both instantiate and index the data:\n\nvector_db = MomentoVectorIndex.from_documents(\n    docs, OpenAIEmbeddings(), index_name=\"sotu\"\n)\n\n\nThis connects to the Momento Vector Index service using your API key and indexes the data. If the index did not exist before, this process creates it for you. The data is now searchable.\n\nQuery your data\nAsk a question directly against the index‚Äã\n\nThe most direct way to query the data is to search against the index. We can do that as follows using the VectorStore API:\n\nquery = \"What did the president say about Ketanji Brown Jackson\"\ndocs = vector_db.similarity_search(query)\n\ndocs[0].page_content\n\n    'Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you‚Äôre at it, pass the Disclose Act so Americans can know who is funding our elections. \\n\\nTonight, I‚Äôd like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer‚Äîan Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \\n\\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \\n\\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation‚Äôs top legal minds, who will continue Justice Breyer‚Äôs legacy of excellence.'\n\n\nWhile this does contain relevant information about Ketanji Brown Jackson, we don't have a concise, human-readable answer. We'll tackle that in the next section.\n\nUse an LLM to generate fluent answers‚Äã\n\nWith the data indexed in MVI, we can integrate with any chain that leverages vector similarity search. Here we use the RetrievalQA chain to demonstrate how to answer questions from the indexed data.\n\nFirst we load the relevant modules:\n\nfrom langchain.chains import RetrievalQA\nfrom langchain.chat_models import ChatOpenAI\n\n\nThen we instantiate the retrieval QA chain:\n\nllm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\nqa_chain = RetrievalQA.from_chain_type(llm, retriever=vector_db.as_retriever())\n\nqa_chain({\"query\": \"What did the president say about Ketanji Brown Jackson?\"})\n\n    {'query': 'What did the president say about Ketanji Brown Jackson?',\n     'result': \"The President said that he nominated Circuit Court of Appeals Judge Ketanji Brown Jackson to serve on the United States Supreme Court. He described her as one of the nation's top legal minds and mentioned that she has received broad support from various groups, including the Fraternal Order of Police and former judges appointed by Democrats and Republicans.\"}\n\nNext Steps\n\nThat's it! You've now indexed your data and can query it using the Momento Vector Index. You can use the same index to query your data from any chain that supports vector similarity search.\n\nWith Momento you can not only index your vector data, but also cache your API calls and store your chat message history. Check out the other Momento langchain integrations to learn more.\n\nTo learn more about the Momento Vector Index, visit the Momento Documentation.\n\nPrevious\nMilvus\nNext\nMongoDB Atlas"
}