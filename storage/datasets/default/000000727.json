{
	"title": "Evaluating Structured Output: JSON Evaluators | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/guides/evaluation/string/json",
	"html": "EvaluationString EvaluatorsEvaluating Structured Output: JSON Evaluators\nEvaluating Structured Output: JSON Evaluators\n\nEvaluating extraction and function calling applications often comes down to validation that the LLM's string output can be parsed correctly and how it compares to a reference object. The following JSON validators provide provide functionality to check your model's output in a consistent way.\n\nJsonValidityEvaluator‚Äã\n\nThe JsonValidityEvaluator is designed to check the validity of a JSON string prediction.\n\nOverview:‚Äã\nRequires Input?: No\nRequires Reference?: No\nfrom langchain.evaluation import JsonValidityEvaluator\n\nevaluator = JsonValidityEvaluator()\n# Equivalently\n# evaluator = load_evaluator(\"json_validity\")\nprediction = '{\"name\": \"John\", \"age\": 30, \"city\": \"New York\"}'\n\nresult = evaluator.evaluate_strings(prediction=prediction)\nprint(result)\n\n    {'score': 1}\n\nprediction = '{\"name\": \"John\", \"age\": 30, \"city\": \"New York\",}'\nresult = evaluator.evaluate_strings(prediction=prediction)\nprint(result)\n\n    {'score': 0, 'reasoning': 'Expecting property name enclosed in double quotes: line 1 column 48 (char 47)'}\n\nJsonEqualityEvaluator‚Äã\n\nThe JsonEqualityEvaluator assesses whether a JSON prediction matches a given reference after both are parsed.\n\nOverview:‚Äã\nRequires Input?: No\nRequires Reference?: Yes\nfrom langchain.evaluation import JsonEqualityEvaluator\n\nevaluator = JsonEqualityEvaluator()\n# Equivalently\n# evaluator = load_evaluator(\"json_equality\")\nresult = evaluator.evaluate_strings(prediction='{\"a\": 1}', reference='{\"a\": 1}')\nprint(result)\n\n    {'score': True}\n\nresult = evaluator.evaluate_strings(prediction='{\"a\": 1}', reference='{\"a\": 2}')\nprint(result)\n\n    {'score': False}\n\n\nThe evaluator also by default lets you provide a dictionary directly\n\nresult = evaluator.evaluate_strings(prediction={\"a\": 1}, reference={\"a\": 2})\nprint(result)\n\n    {'score': False}\n\nJsonEditDistanceEvaluator‚Äã\n\nThe JsonEditDistanceEvaluator computes a normalized Damerau-Levenshtein distance between two \"canonicalized\" JSON strings.\n\nOverview:‚Äã\nRequires Input?: No\nRequires Reference?: Yes\nDistance Function: Damerau-Levenshtein (by default)\n\nNote: Ensure that rapidfuzz is installed or provide an alternative string_distance function to avoid an ImportError.\n\nfrom langchain.evaluation import JsonEditDistanceEvaluator\n\nevaluator = JsonEditDistanceEvaluator()\n# Equivalently\n# evaluator = load_evaluator(\"json_edit_distance\")\n\nresult = evaluator.evaluate_strings(\n    prediction='{\"a\": 1, \"b\": 2}', reference='{\"a\": 1, \"b\": 3}'\n)\nprint(result)\n\n    {'score': 0.07692307692307693}\n\n# The values are canonicalized prior to comparison\nresult = evaluator.evaluate_strings(\n    prediction=\"\"\"\n    {\n        \"b\": 3,\n        \"a\":   1\n    }\"\"\",\n    reference='{\"a\": 1, \"b\": 3}',\n)\nprint(result)\n\n    {'score': 0.0}\n\n# Lists maintain their order, however\nresult = evaluator.evaluate_strings(\n    prediction='{\"a\": [1, 2]}', reference='{\"a\": [2, 1]}'\n)\nprint(result)\n\n    {'score': 0.18181818181818182}\n\n# You can also pass in objects directly\nresult = evaluator.evaluate_strings(prediction={\"a\": 1}, reference={\"a\": 2})\nprint(result)\n\n    {'score': 0.14285714285714285}\n\nJsonSchemaEvaluator‚Äã\n\nThe JsonSchemaEvaluator validates a JSON prediction against a provided JSON schema. If the prediction conforms to the schema, it returns a score of True (indicating no errors). Otherwise, it returns a score of 0 (indicating an error).\n\nOverview:‚Äã\nRequires Input?: Yes\nRequires Reference?: Yes (A JSON schema)\nScore: True (No errors) or False (Error occurred)\nfrom langchain.evaluation import JsonSchemaEvaluator\n\nevaluator = JsonSchemaEvaluator()\n# Equivalently\n# evaluator = load_evaluator(\"json_schema_validation\")\n\nresult = evaluator.evaluate_strings(\n    prediction='{\"name\": \"John\", \"age\": 30}',\n    reference={\n        \"type\": \"object\",\n        \"properties\": {\"name\": {\"type\": \"string\"}, \"age\": {\"type\": \"integer\"}},\n    },\n)\nprint(result)\n\n    {'score': True}\n\nresult = evaluator.evaluate_strings(\n    prediction='{\"name\": \"John\", \"age\": 30}',\n    reference='{\"type\": \"object\", \"properties\": {\"name\": {\"type\": \"string\"}, \"age\": {\"type\": \"integer\"}}}',\n)\nprint(result)\n\n    {'score': True}\n\nresult = evaluator.evaluate_strings(\n    prediction='{\"name\": \"John\", \"age\": 30}',\n    reference='{\"type\": \"object\", \"properties\": {\"name\": {\"type\": \"string\"},'\n    '\"age\": {\"type\": \"integer\", \"minimum\": 66}}}',\n)\nprint(result)\n\n    {'score': False, 'reasoning': \"<ValidationError: '30 is less than the minimum of 66'>\"}\n\nPrevious\nExact Match\nNext\nRegex Match"
}