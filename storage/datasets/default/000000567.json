{
	"title": "Petals | 🦜️🔗 Langchain",
	"url": "https://python.langchain.com/docs/integrations/llms/petals",
	"html": "ComponentsLLMsPetals\nPetals\n\nPetals runs 100B+ language models at home, BitTorrent-style.\n\nThis notebook goes over how to use Langchain with Petals.\n\nInstall petals​\n\nThe petals package is required to use the Petals API. Install petals using pip3 install petals.\n\nFor Apple Silicon(M1/M2) users please follow this guide https://github.com/bigscience-workshop/petals/issues/147#issuecomment-1365379642 to install petals\n\npip3 install petals\n\nImports​\nimport os\n\nfrom langchain.chains import LLMChain\nfrom langchain.llms import Petals\nfrom langchain.prompts import PromptTemplate\n\nSet the Environment API Key​\n\nMake sure to get your API key from Huggingface.\n\nfrom getpass import getpass\n\nHUGGINGFACE_API_KEY = getpass()\n\n     ········\n\nos.environ[\"HUGGINGFACE_API_KEY\"] = HUGGINGFACE_API_KEY\n\nCreate the Petals instance​\n\nYou can specify different parameters such as the model name, max new tokens, temperature, etc.\n\n# this can take several minutes to download big files!\n\nllm = Petals(model_name=\"bigscience/bloom-petals\")\n\n    Downloading:   1%|▏                        | 40.8M/7.19G [00:24<15:44, 7.57MB/s]\n\nCreate a Prompt Template​\n\nWe will create a prompt template for Question and Answer.\n\ntemplate = \"\"\"Question: {question}\n\nAnswer: Let's think step by step.\"\"\"\n\nprompt = PromptTemplate(template=template, input_variables=[\"question\"])\n\nInitiate the LLMChain​\nllm_chain = LLMChain(prompt=prompt, llm=llm)\n\nRun the LLMChain​\n\nProvide a question and run the LLMChain.\n\nquestion = \"What NFL team won the Super Bowl in the year Justin Beiber was born?\"\n\nllm_chain.run(question)\n\nPrevious\nAliCloud PAI EAS\nNext\nPipelineAI"
}