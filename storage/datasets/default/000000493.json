{
	"title": "LLM | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/modules/chains/foundational/llm_chain",
	"html": "ModulesMoreChainsFoundationalLLM\nLLM\n\nThe most common type of chaining in any LLM application is combining a prompt template with an LLM and optionally an output parser.\n\nThe recommended way to do this is using LangChain Expression Language. We also continue to support the legacy LLMChain, which is a single class for composing these three components.\n\nUsing LCEL‚Äã\n\nBasePromptTemplate, BaseLanguageModel and BaseOutputParser all implement the Runnable interface and are designed to be piped into one another, making LCEL composition very easy:\n\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.prompts import PromptTemplate\nfrom langchain.schema import StrOutputParser\n\nprompt = PromptTemplate.from_template(\n    \"What is a good name for a company that makes {product}?\"\n)\nrunnable = prompt | ChatOpenAI() | StrOutputParser()\nrunnable.invoke({\"product\": \"colorful socks\"})\n\n    'VibrantSocks'\n\n\nHead to the LCEL section for more on the interface, built-in features, and cookbook examples.\n\n[Legacy] LLMChain‚Äã\nTHIS IS A LEGACY CLASS, USING LCEL AS SHOWN ABOVE IS PREFFERED.\n\nAn LLMChain is a simple chain that adds some functionality around language models. It is used widely throughout LangChain, including in other chains and agents.\n\nAn LLMChain consists of a PromptTemplate and a language model (either an LLM or chat model). It formats the prompt template using the input key values provided (and also memory key values, if available), passes the formatted string to LLM and returns the LLM output.\n\nGet started‚Äã\nfrom langchain.chains import LLMChain\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\n\nprompt_template = \"What is a good name for a company that makes {product}?\"\n\nllm = OpenAI(temperature=0)\nllm_chain = LLMChain(llm=llm, prompt=PromptTemplate.from_template(prompt_template))\nllm_chain(\"colorful socks\")\n\n    {'product': 'colorful socks', 'text': '\\n\\nSocktastic!'}\n\nAdditional ways of running LLMChain‚Äã\n\nAside from __call__ and run methods shared by all Chain object, LLMChain offers a few more ways of calling the chain logic:\n\napply allows you run the chain against a list of inputs:\ninput_list = [{\"product\": \"socks\"}, {\"product\": \"computer\"}, {\"product\": \"shoes\"}]\nllm_chain.apply(input_list)\n\n    [{'text': '\\n\\nSocktastic!'},\n     {'text': '\\n\\nTechCore Solutions.'},\n     {'text': '\\n\\nFootwear Factory.'}]\n\ngenerate is similar to apply, except it return an LLMResult instead of string. LLMResult often contains useful generation such as token usages and finish reason.\nllm_chain.generate(input_list)\n\n    LLMResult(generations=[[Generation(text='\\n\\nSocktastic!', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nTechCore Solutions.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nFootwear Factory.', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 36, 'total_tokens': 55}, 'model_name': 'text-davinci-003'}, run=[RunInfo(run_id=UUID('9a423a43-6d35-4e8f-9aca-cacfc8e0dc49')), RunInfo(run_id=UUID('a879c077-b521-461c-8f29-ba63adfc327c')), RunInfo(run_id=UUID('40b892fa-e8c2-47d0-a309-4f7a4ed5b64a'))])\n\npredict is similar to run method except that the input keys are specified as keyword arguments instead of a Python dict.\n# Single input example\nllm_chain.predict(product=\"colorful socks\")\n\n    '\\n\\nSocktastic!'\n\n# Multiple inputs example\ntemplate = \"\"\"Tell me a {adjective} joke about {subject}.\"\"\"\nprompt = PromptTemplate(template=template, input_variables=[\"adjective\", \"subject\"])\nllm_chain = LLMChain(prompt=prompt, llm=OpenAI(temperature=0))\n\nllm_chain.predict(adjective=\"sad\", subject=\"ducks\")\n\n    '\\n\\nQ: What did the duck say when his friend died?\\nA: Quack, quack, goodbye.'\n\nParsing the outputs‚Äã\n\nBy default, LLMChain does not parse the output even if the underlying prompt object has an output parser. If you would like to apply that output parser on the LLM output, use predict_and_parse instead of predict and apply_and_parse instead of apply.\n\nWith predict:\n\nfrom langchain.output_parsers import CommaSeparatedListOutputParser\n\noutput_parser = CommaSeparatedListOutputParser()\ntemplate = \"\"\"List all the colors in a rainbow\"\"\"\nprompt = PromptTemplate(\n    template=template, input_variables=[], output_parser=output_parser\n)\nllm_chain = LLMChain(prompt=prompt, llm=llm)\n\nllm_chain.predict()\n\n    '\\n\\nRed, orange, yellow, green, blue, indigo, violet'\n\n\nWith predict_and_parse:\n\nllm_chain.predict_and_parse()\n\n    /Users/bagatur/langchain/libs/langchain/langchain/chains/llm.py:280: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n      warnings.warn(\n\n\n\n\n\n    ['Red', 'orange', 'yellow', 'green', 'blue', 'indigo', 'violet']\n\nInitialize from string‚Äã\n\nYou can also construct an LLMChain from a string template directly.\n\ntemplate = \"\"\"Tell me a {adjective} joke about {subject}.\"\"\"\nllm_chain = LLMChain.from_string(llm=llm, template=template)\n\nllm_chain.predict(adjective=\"sad\", subject=\"ducks\")\n\n    '\\n\\nQ: What did the duck say when his friend died?\\nA: Quack, quack, goodbye.'\n\nPrevious\nFoundational\nNext\nRouter"
}