{
	"title": "YouTube audio | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/integrations/document_loaders/youtube_audio",
	"html": "ComponentsDocument loadersYouTube audio\nYouTube audio\n\nBuilding chat or QA applications on YouTube videos is a topic of high interest.\n\nBelow we show how to easily go from a YouTube url to audio of the video to text to chat!\n\nWe wil use the OpenAIWhisperParser, which will use the OpenAI Whisper API to transcribe audio to text, and the OpenAIWhisperParserLocal for local support and running on private clouds or on premise.\n\nNote: You will need to have an OPENAI_API_KEY supplied.\n\nfrom langchain.document_loaders.blob_loaders.youtube_audio import YoutubeAudioLoader\nfrom langchain.document_loaders.generic import GenericLoader\nfrom langchain.document_loaders.parsers import (\n    OpenAIWhisperParser,\n    OpenAIWhisperParserLocal,\n)\n\n\nWe will use yt_dlp to download audio for YouTube urls.\n\nWe will use pydub to split downloaded audio files (such that we adhere to Whisper API's 25MB file size limit).\n\npip install yt_dlp\n pip install pydub\n pip install librosa\n\nYouTube url to text‚Äã\n\nUse YoutubeAudioLoader to fetch / download the audio files.\n\nThen, ues OpenAIWhisperParser() to transcribe them to text.\n\nLet's take the first lecture of Andrej Karpathy's YouTube course as an example!\n\n# set a flag to switch between local and remote parsing\n# change this to True if you want to use local parsing\nlocal = False\n\n# Two Karpathy lecture videos\nurls = [\"https://youtu.be/kCc8FmEb1nY\", \"https://youtu.be/VMj-3S1tku0\"]\n\n# Directory to save audio files\nsave_dir = \"~/Downloads/YouTube\"\n\n# Transcribe the videos to text\nif local:\n    loader = GenericLoader(\n        YoutubeAudioLoader(urls, save_dir), OpenAIWhisperParserLocal()\n    )\nelse:\n    loader = GenericLoader(YoutubeAudioLoader(urls, save_dir), OpenAIWhisperParser())\ndocs = loader.load()\n\n    [youtube] Extracting URL: https://youtu.be/kCc8FmEb1nY\n    [youtube] kCc8FmEb1nY: Downloading webpage\n    [youtube] kCc8FmEb1nY: Downloading android player API JSON\n    [info] kCc8FmEb1nY: Downloading 1 format(s): 140\n    [dashsegments] Total fragments: 11\n    [download] Destination: /Users/31treehaus/Desktop/AI/langchain-fork/docs/modules/indexes/document_loaders/examples/Let's build GPTÔºö from scratch, in code, spelled out..m4a\n    [download] 100% of  107.73MiB in 00:00:18 at 5.92MiB/s                   \n    [FixupM4a] Correcting container of \"/Users/31treehaus/Desktop/AI/langchain-fork/docs/modules/indexes/document_loaders/examples/Let's build GPTÔºö from scratch, in code, spelled out..m4a\"\n    [ExtractAudio] Not converting audio /Users/31treehaus/Desktop/AI/langchain-fork/docs/modules/indexes/document_loaders/examples/Let's build GPTÔºö from scratch, in code, spelled out..m4a; file is already in target format m4a\n    [youtube] Extracting URL: https://youtu.be/VMj-3S1tku0\n    [youtube] VMj-3S1tku0: Downloading webpage\n    [youtube] VMj-3S1tku0: Downloading android player API JSON\n    [info] VMj-3S1tku0: Downloading 1 format(s): 140\n    [download] /Users/31treehaus/Desktop/AI/langchain-fork/docs/modules/indexes/document_loaders/examples/The spelled-out intro to neural networks and backpropagationÔºö building micrograd.m4a has already been downloaded\n    [download] 100% of  134.98MiB\n    [ExtractAudio] Not converting audio /Users/31treehaus/Desktop/AI/langchain-fork/docs/modules/indexes/document_loaders/examples/The spelled-out intro to neural networks and backpropagationÔºö building micrograd.m4a; file is already in target format m4a\n\n# Returns a list of Documents, which can be easily viewed or parsed\ndocs[0].page_content[0:500]\n\n    \"Hello, my name is Andrej and I've been training deep neural networks for a bit more than a decade. And in this lecture I'd like to show you what neural network training looks like under the hood. So in particular we are going to start with a blank Jupyter notebook and by the end of this lecture we will define and train a neural net and you'll get to see everything that goes on under the hood and exactly sort of how that works on an intuitive level. Now specifically what I would like to do is I w\"\n\nBuilding a chat app from YouTube video‚Äã\n\nGiven Documents, we can easily enable chat / question+answering.\n\nfrom langchain.chains import RetrievalQA\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.vectorstores import FAISS\n\n# Combine doc\ncombined_docs = [doc.page_content for doc in docs]\ntext = \" \".join(combined_docs)\n\n# Split them\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=150)\nsplits = text_splitter.split_text(text)\n\n# Build an index\nembeddings = OpenAIEmbeddings()\nvectordb = FAISS.from_texts(splits, embeddings)\n\n# Build a QA chain\nqa_chain = RetrievalQA.from_chain_type(\n    llm=ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0),\n    chain_type=\"stuff\",\n    retriever=vectordb.as_retriever(),\n)\n\n# Ask a question!\nquery = \"Why do we need to zero out the gradient before backprop at each step?\"\nqa_chain.run(query)\n\n    \"We need to zero out the gradient before backprop at each step because the backward pass accumulates gradients in the grad attribute of each parameter. If we don't reset the grad to zero before each backward pass, the gradients will accumulate and add up, leading to incorrect updates and slower convergence. By resetting the grad to zero before each backward pass, we ensure that the gradients are calculated correctly and that the optimization process works as intended.\"\n\nquery = \"What is the difference between an encoder and decoder?\"\nqa_chain.run(query)\n\n    'In the context of transformers, an encoder is a component that reads in a sequence of input tokens and generates a sequence of hidden representations. On the other hand, a decoder is a component that takes in a sequence of hidden representations and generates a sequence of output tokens. The main difference between the two is that the encoder is used to encode the input sequence into a fixed-length representation, while the decoder is used to decode the fixed-length representation into an output sequence. In machine translation, for example, the encoder reads in the source language sentence and generates a fixed-length representation, which is then used by the decoder to generate the target language sentence.'\n\nquery = \"For any token, what are x, k, v, and q?\"\nqa_chain.run(query)\n\n    'For any token, x is the input vector that contains the private information of that token, k and q are the key and query vectors respectively, which are produced by forwarding linear modules on x, and v is the vector that is calculated by propagating the same linear module on x again. The key vector represents what the token contains, and the query vector represents what the token is looking for. The vector v is the information that the token will communicate to other tokens if it finds them interesting, and it gets aggregated for the purposes of the self-attention mechanism.'\n\nPrevious\nXorbits Pandas DataFrame\nNext\nYouTube transcripts"
}