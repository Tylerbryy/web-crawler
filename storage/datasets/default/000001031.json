{
	"title": "iMessage | ðŸ¦œï¸ðŸ”— Langchain",
	"url": "https://python.langchain.com/docs/integrations/chat_loaders/imessage",
	"html": "ComponentsChat loadersiMessage\niMessage\n\nThis notebook shows how to use the iMessage chat loader. This class helps convert iMessage conversations to LangChain chat messages.\n\nOn MacOS, iMessage stores conversations in a sqlite database at ~/Library/Messages/chat.db (at least for macOS Ventura 13.4). The IMessageChatLoader loads from this database file.\n\nCreate the IMessageChatLoader with the file path pointed to chat.db database you'd like to process.\nCall loader.load() (or loader.lazy_load()) to perform the conversion. Optionally use merge_chat_runs to combine message from the same sender in sequence, and/or map_ai_messages to convert messages from the specified sender to the \"AIMessage\" class.\n1. Access Chat DBâ€‹\n\nIt's likely that your terminal is denied access to ~/Library/Messages. To use this class, you can copy the DB to an accessible directory (e.g., Documents) and load from there. Alternatively (and not recommended), you can grant full disk access for your terminal emulator in System Settings > Security and Privacy > Full Disk Access.\n\nWe have created an example database you can use at this linked drive file.\n\n# This uses some example data\nimport requests\n\n\ndef download_drive_file(url: str, output_path: str = \"chat.db\") -> None:\n    file_id = url.split(\"/\")[-2]\n    download_url = f\"https://drive.google.com/uc?export=download&id={file_id}\"\n\n    response = requests.get(download_url)\n    if response.status_code != 200:\n        print(\"Failed to download the file.\")\n        return\n\n    with open(output_path, \"wb\") as file:\n        file.write(response.content)\n        print(f\"File {output_path} downloaded.\")\n\n\nurl = (\n    \"https://drive.google.com/file/d/1NebNKqTA2NXApCmeH6mu0unJD2tANZzo/view?usp=sharing\"\n)\n\n# Download file to chat.db\ndownload_drive_file(url)\n\n    File chat.db downloaded.\n\n2. Create the Chat Loaderâ€‹\n\nProvide the loader with the file path to the zip directory. You can optionally specify the user id that maps to an ai message as well an configure whether to merge message runs.\n\nfrom langchain.chat_loaders.imessage import IMessageChatLoader\n\nloader = IMessageChatLoader(\n    path=\"./chat.db\",\n)\n\n3. Load messagesâ€‹\n\nThe load() (or lazy_load) methods return a list of \"ChatSessions\" that currently just contain a list of messages per loaded conversation. All messages are mapped to \"HumanMessage\" objects to start.\n\nYou can optionally choose to merge message \"runs\" (consecutive messages from the same sender) and select a sender to represent the \"AI\". The fine-tuned LLM will learn to generate these AI messages.\n\nfrom typing import List\n\nfrom langchain.chat_loaders.base import ChatSession\nfrom langchain.chat_loaders.utils import (\n    map_ai_messages,\n    merge_chat_runs,\n)\n\nraw_messages = loader.lazy_load()\n# Merge consecutive messages from the same sender into a single message\nmerged_messages = merge_chat_runs(raw_messages)\n# Convert messages from \"Tortoise\" to AI messages. Do you have a guess who these conversations are between?\nchat_sessions: List[ChatSession] = list(\n    map_ai_messages(merged_messages, sender=\"Tortoise\")\n)\n\n# Now all of the Tortoise's messages will take the AI message class\n# which maps to the 'assistant' role in OpenAI's training format\nalternating_sessions[0][\"messages\"][:3]\n\n    [AIMessage(content=\"Slow and steady, that's my motto.\", additional_kwargs={'message_time': 1693182723, 'sender': 'Tortoise'}, example=False),\n     HumanMessage(content='Speed is key!', additional_kwargs={'message_time': 1693182753, 'sender': 'Hare'}, example=False),\n     AIMessage(content='A balanced approach is more reliable.', additional_kwargs={'message_time': 1693182783, 'sender': 'Tortoise'}, example=False)]\n\n3. Prepare for fine-tuningâ€‹\n\nNow it's time to convert our chat messages to OpenAI dictionaries. We can use the convert_messages_for_finetuning utility to do so.\n\nfrom langchain.adapters.openai import convert_messages_for_finetuning\n\ntraining_data = convert_messages_for_finetuning(alternating_sessions)\nprint(f\"Prepared {len(training_data)} dialogues for training\")\n\n    Prepared 10 dialogues for training\n\n4. Fine-tune the modelâ€‹\n\nIt's time to fine-tune the model. Make sure you have openai installed and have set your OPENAI_API_KEY appropriately\n\n# %pip install -U openai --quiet\n\nimport json\nimport time\nfrom io import BytesIO\n\nimport openai\n\n# We will write the jsonl file in memory\nmy_file = BytesIO()\nfor m in training_data:\n    my_file.write((json.dumps({\"messages\": m}) + \"\\n\").encode(\"utf-8\"))\n\nmy_file.seek(0)\ntraining_file = openai.File.create(file=my_file, purpose=\"fine-tune\")\n\n# OpenAI audits each training file for compliance reasons.\n# This make take a few minutes\nstatus = openai.File.retrieve(training_file.id).status\nstart_time = time.time()\nwhile status != \"processed\":\n    print(f\"Status=[{status}]... {time.time() - start_time:.2f}s\", end=\"\\r\", flush=True)\n    time.sleep(5)\n    status = openai.File.retrieve(training_file.id).status\nprint(f\"File {training_file.id} ready after {time.time() - start_time:.2f} seconds.\")\n\n    File file-zHIgf4r8LltZG3RFpkGd4Sjf ready after 10.19 seconds.\n\n\nWith the file ready, it's time to kick off a training job.\n\njob = openai.FineTuningJob.create(\n    training_file=training_file.id,\n    model=\"gpt-3.5-turbo\",\n)\n\n\nGrab a cup of tea while your model is being prepared. This may take some time!\n\nstatus = openai.FineTuningJob.retrieve(job.id).status\nstart_time = time.time()\nwhile status != \"succeeded\":\n    print(f\"Status=[{status}]... {time.time() - start_time:.2f}s\", end=\"\\r\", flush=True)\n    time.sleep(5)\n    job = openai.FineTuningJob.retrieve(job.id)\n    status = job.status\n\n    Status=[running]... 524.95s\n\nprint(job.fine_tuned_model)\n\n    ft:gpt-3.5-turbo-0613:personal::7sKoRdlz\n\n5. Use in LangChainâ€‹\n\nYou can use the resulting model ID directly the ChatOpenAI model class.\n\nfrom langchain.chat_models import ChatOpenAI\n\nmodel = ChatOpenAI(\n    model=job.fine_tuned_model,\n    temperature=1,\n)\n\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.schema.output_parser import StrOutputParser\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are speaking to hare.\"),\n        (\"human\", \"{input}\"),\n    ]\n)\n\nchain = prompt | model | StrOutputParser()\n\nfor tok in chain.stream({\"input\": \"What's the golden thread?\"}):\n    print(tok, end=\"\", flush=True)\n\n    A symbol of interconnectedness.\n\nPrevious\nGMail\nNext\nLangSmith Chat Datasets"
}