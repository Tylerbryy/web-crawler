{
	"title": "Rockset | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/integrations/vectorstores/rockset#setting-up-environment",
	"html": "ComponentsVector storesRockset\nRockset\n\nRockset is a real-time search and analytics database built for the cloud. Rockset uses a Converged Index‚Ñ¢ with an efficient store for vector embeddings to serve low latency, high concurrency search queries at scale. Rockset has full support for metadata filtering and handles real-time ingestion for constantly updating, streaming data.\n\nThis notebook demonstrates how to use¬†Rockset as a vector store in LangChain. Before getting started, make sure you have access to a Rockset account and an API key available. Start your free trial today.\n\nSetting Up Your Environment‚Äã\n\nLeverage the Rockset console to create a collection with the Write API as your source. In this walkthrough, we create a collection named¬†langchain_demo.\n\nConfigure the following ingest transformation to mark your embeddings¬†field and take advantage of performance and storage optimizations:\n\n(We used OpenAI text-embedding-ada-002 for this examples, where #length_of_vector_embedding = 1536)\n\nSELECT _input.* EXCEPT(_meta), \nVECTOR_ENFORCE(_input.description_embedding, #length_of_vector_embedding, 'float') as description_embedding \nFROM _input\n\n\nAfter creating your collection, use the console to retrieve an API key. For the purpose of this notebook, we assume you are using the¬†Oregon(us-west-2) region.\n\nInstall the rockset-python-client to enable LangChain to communicate directly with Rockset.\n\npip install rockset\n\nLangChain Tutorial‚Äã\n\nFollow along in your own Python notebook to generate and store vector embeddings in Rockset. Start using Rockset to search for documents similar to your search queries.\n\n1. Define Key Variables‚Äã\nimport os\n\nimport rockset\n\nROCKSET_API_KEY = os.environ.get(\n    \"ROCKSET_API_KEY\"\n)  # Verify ROCKSET_API_KEY environment variable\nROCKSET_API_SERVER = rockset.Regions.usw2a1  # Verify Rockset region\nrockset_client = rockset.RocksetClient(ROCKSET_API_SERVER, ROCKSET_API_KEY)\n\nCOLLECTION_NAME = \"langchain_demo\"\nTEXT_KEY = \"description\"\nEMBEDDING_KEY = \"description_embedding\"\n\n2. Prepare Documents‚Äã\nfrom langchain.document_loaders import TextLoader\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.vectorstores import Rockset\n\nloader = TextLoader(\"../../modules/state_of_the_union.txt\")\ndocuments = loader.load()\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\ndocs = text_splitter.split_documents(documents)\n\n3. Insert Documents‚Äã\nembeddings = OpenAIEmbeddings()  # Verify OPENAI_API_KEY environment variable\n\ndocsearch = Rockset(\n    client=rockset_client,\n    embeddings=embeddings,\n    collection_name=COLLECTION_NAME,\n    text_key=TEXT_KEY,\n    embedding_key=EMBEDDING_KEY,\n)\n\nids = docsearch.add_texts(\n    texts=[d.page_content for d in docs],\n    metadatas=[d.metadata for d in docs],\n)\n\n4. Search for Similar Documents‚Äã\nquery = \"What did the president say about Ketanji Brown Jackson\"\noutput = docsearch.similarity_search_with_relevance_scores(\n    query, 4, Rockset.DistanceFunction.COSINE_SIM\n)\nprint(\"output length:\", len(output))\nfor d, dist in output:\n    print(dist, d.metadata, d.page_content[:20] + \"...\")\n\n##\n# output length: 4\n# 0.764990692109871 {'source': '../../../state_of_the_union.txt'} Madam Speaker, Madam...\n# 0.7485416901622112 {'source': '../../../state_of_the_union.txt'} And I‚Äôm taking robus...\n# 0.7468678973398306 {'source': '../../../state_of_the_union.txt'} And so many families...\n# 0.7436231261419488 {'source': '../../../state_of_the_union.txt'} Groups of citizens b...\n\n5. Search for Similar Documents with Filtering‚Äã\noutput = docsearch.similarity_search_with_relevance_scores(\n    query,\n    4,\n    Rockset.DistanceFunction.COSINE_SIM,\n    where_str=\"{} NOT LIKE '%citizens%'\".format(TEXT_KEY),\n)\nprint(\"output length:\", len(output))\nfor d, dist in output:\n    print(dist, d.metadata, d.page_content[:20] + \"...\")\n\n##\n# output length: 4\n# 0.7651359650263554 {'source': '../../../state_of_the_union.txt'} Madam Speaker, Madam...\n# 0.7486265516824893 {'source': '../../../state_of_the_union.txt'} And I‚Äôm taking robus...\n# 0.7469625542348115 {'source': '../../../state_of_the_union.txt'} And so many families...\n# 0.7344177777547739 {'source': '../../../state_of_the_union.txt'} We see the unity amo...\n\n6.¬†[Optional]¬†Delete Inserted Documents‚Äã\n\nYou must have the unique ID associated with each document to delete them from your collection. Define IDs when inserting documents with Rockset.add_texts(). Rockset will otherwise generate a unique ID for each document. Regardless, Rockset.add_texts() returns the IDs of inserted documents.\n\nTo delete these docs, simply use the Rockset.delete_texts() function.\n\ndocsearch.delete_texts(ids)\n\nSummary‚Äã\n\nIn this tutorial, we successfully created a Rockset collection, inserted documents with OpenAI embeddings, and searched for similar documents with and without metadata filters.\n\nKeep an eye on¬†https://rockset.com/ for future updates in this space.\n\nPrevious\nRedis\nNext\nScaNN"
}