{
	"title": "Prompt + LLM | ðŸ¦œï¸ðŸ”— Langchain",
	"url": "https://python.langchain.com/docs/expression_language/cookbook/prompt_llm_parser",
	"html": "LangChain Expression LanguageCookbookPrompt + LLM\nPrompt + LLM\n\nThe most common and valuable composition is taking:\n\nPromptTemplate / ChatPromptTemplate -> LLM / ChatModel -> OutputParser\n\nAlmost any other chains you build will use this building block.\n\nPromptTemplate + LLMâ€‹\n\nThe simplest composition is just combining a prompt and model to create a chain that takes user input, adds it to a prompt, passes it to a model, and returns the raw model output.\n\nNote, you can mix and match PromptTemplate/ChatPromptTemplates and LLMs/ChatModels as you like here.\n\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.prompts import ChatPromptTemplate\n\nprompt = ChatPromptTemplate.from_template(\"tell me a joke about {foo}\")\nmodel = ChatOpenAI()\nchain = prompt | model\n\nchain.invoke({\"foo\": \"bears\"})\n\n    AIMessage(content=\"Why don't bears wear shoes?\\n\\nBecause they have bear feet!\", additional_kwargs={}, example=False)\n\n\nOften times we want to attach kwargs that'll be passed to each model call. Here are a few examples of that:\n\nAttaching Stop Sequencesâ€‹\nchain = prompt | model.bind(stop=[\"\\n\"])\n\nchain.invoke({\"foo\": \"bears\"})\n\n    AIMessage(content='Why did the bear never wear shoes?', additional_kwargs={}, example=False)\n\nAttaching Function Call informationâ€‹\nfunctions = [\n    {\n        \"name\": \"joke\",\n        \"description\": \"A joke\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"setup\": {\"type\": \"string\", \"description\": \"The setup for the joke\"},\n                \"punchline\": {\n                    \"type\": \"string\",\n                    \"description\": \"The punchline for the joke\",\n                },\n            },\n            \"required\": [\"setup\", \"punchline\"],\n        },\n    }\n]\nchain = prompt | model.bind(function_call={\"name\": \"joke\"}, functions=functions)\n\nchain.invoke({\"foo\": \"bears\"}, config={})\n\n    AIMessage(content='', additional_kwargs={'function_call': {'name': 'joke', 'arguments': '{\\n  \"setup\": \"Why don\\'t bears wear shoes?\",\\n  \"punchline\": \"Because they have bear feet!\"\\n}'}}, example=False)\n\nPromptTemplate + LLM + OutputParserâ€‹\n\nWe can also add in an output parser to easily transform the raw LLM/ChatModel output into a more workable format\n\nfrom langchain.schema.output_parser import StrOutputParser\n\nchain = prompt | model | StrOutputParser()\n\n\nNotice that this now returns a string - a much more workable format for downstream tasks\n\nchain.invoke({\"foo\": \"bears\"})\n\n    \"Why don't bears wear shoes?\\n\\nBecause they have bear feet!\"\n\nFunctions Output Parserâ€‹\n\nWhen you specify the function to return, you may just want to parse that directly\n\nfrom langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n\nchain = (\n    prompt\n    | model.bind(function_call={\"name\": \"joke\"}, functions=functions)\n    | JsonOutputFunctionsParser()\n)\n\nchain.invoke({\"foo\": \"bears\"})\n\n    {'setup': \"Why don't bears like fast food?\",\n     'punchline': \"Because they can't catch it!\"}\n\nfrom langchain.output_parsers.openai_functions import JsonKeyOutputFunctionsParser\n\nchain = (\n    prompt\n    | model.bind(function_call={\"name\": \"joke\"}, functions=functions)\n    | JsonKeyOutputFunctionsParser(key_name=\"setup\")\n)\n\nchain.invoke({\"foo\": \"bears\"})\n\n    \"Why don't bears wear shoes?\"\n\nSimplifying inputâ€‹\n\nTo make invocation even simpler, we can add a RunnableMap to take care of creating the prompt input dict for us:\n\nfrom langchain.schema.runnable import RunnableMap, RunnablePassthrough\n\nmap_ = RunnableMap(foo=RunnablePassthrough())\nchain = (\n    map_\n    | prompt\n    | model.bind(function_call={\"name\": \"joke\"}, functions=functions)\n    | JsonKeyOutputFunctionsParser(key_name=\"setup\")\n)\n\nchain.invoke(\"bears\")\n\n    \"Why don't bears wear shoes?\"\n\n\nSince we're composing our map with another Runnable, we can even use some syntactic sugar and just use a dict:\n\nchain = (\n    {\"foo\": RunnablePassthrough()}\n    | prompt\n    | model.bind(function_call={\"name\": \"joke\"}, functions=functions)\n    | JsonKeyOutputFunctionsParser(key_name=\"setup\")\n)\n\nchain.invoke(\"bears\")\n\n    \"Why don't bears like fast food?\"\n\nPrevious\nCookbook\nNext\nRAG"
}