{
	"title": "ChatGLM | ğŸ¦œï¸ğŸ”— Langchain",
	"url": "https://python.langchain.com/docs/integrations/llms/chatglm",
	"html": "ComponentsLLMsChatGLM\nChatGLM\n\nChatGLM-6B is an open bilingual language model based on General Language Model (GLM) framework, with 6.2 billion parameters. With the quantization technique, users can deploy locally on consumer-grade graphics cards (only 6GB of GPU memory is required at the INT4 quantization level).\n\nChatGLM2-6B is the second-generation version of the open-source bilingual (Chinese-English) chat model ChatGLM-6B. It retains the smooth conversation flow and low deployment threshold of the first-generation model, while introducing the new features like better performance, longer context and more efficient inference.\n\nThis example goes over how to use LangChain to interact with ChatGLM2-6B Inference for text completion. ChatGLM-6B and ChatGLM2-6B has the same api specs, so this example should work with both.\n\nfrom langchain.chains import LLMChain\nfrom langchain.llms import ChatGLM\nfrom langchain.prompts import PromptTemplate\n\n# import os\n\ntemplate = \"\"\"{question}\"\"\"\nprompt = PromptTemplate(template=template, input_variables=[\"question\"])\n\n# default endpoint_url for a local deployed ChatGLM api server\nendpoint_url = \"http://127.0.0.1:8000\"\n\n# direct access endpoint in a proxied environment\n# os.environ['NO_PROXY'] = '127.0.0.1'\n\nllm = ChatGLM(\n    endpoint_url=endpoint_url,\n    max_token=80000,\n    history=[\n        [\"æˆ‘å°†ä»ç¾å›½åˆ°ä¸­å›½æ¥æ—…æ¸¸ï¼Œå‡ºè¡Œå‰å¸Œæœ›äº†è§£ä¸­å›½çš„åŸå¸‚\", \"æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚\"]\n    ],\n    top_p=0.9,\n    model_kwargs={\"sample_model_args\": False},\n)\n\n# turn on with_history only when you want the LLM object to keep track of the conversation history\n# and send the accumulated context to the backend model api, which make it stateful. By default it is stateless.\n# llm.with_history = True\n\nllm_chain = LLMChain(prompt=prompt, llm=llm)\n\nquestion = \"åŒ—äº¬å’Œä¸Šæµ·ä¸¤åº§åŸå¸‚æœ‰ä»€ä¹ˆä¸åŒï¼Ÿ\"\n\nllm_chain.run(question)\n\n    ChatGLM payload: {'prompt': 'åŒ—äº¬å’Œä¸Šæµ·ä¸¤åº§åŸå¸‚æœ‰ä»€ä¹ˆä¸åŒï¼Ÿ', 'temperature': 0.1, 'history': [['æˆ‘å°†ä»ç¾å›½åˆ°ä¸­å›½æ¥æ—…æ¸¸ï¼Œå‡ºè¡Œå‰å¸Œæœ›äº†è§£ä¸­å›½çš„åŸå¸‚', 'æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚']], 'max_length': 80000, 'top_p': 0.9, 'sample_model_args': False}\n\n\n\n\n\n    'åŒ—äº¬å’Œä¸Šæµ·æ˜¯ä¸­å›½çš„ä¸¤ä¸ªé¦–éƒ½ï¼Œå®ƒä»¬åœ¨è®¸å¤šæ–¹é¢éƒ½æœ‰æ‰€ä¸åŒã€‚\\n\\nåŒ—äº¬æ˜¯ä¸­å›½çš„æ”¿æ²»å’Œæ–‡åŒ–ä¸­å¿ƒï¼Œæ‹¥æœ‰æ‚ ä¹…çš„å†å²å’Œç¿çƒ‚çš„æ–‡åŒ–ã€‚å®ƒæ˜¯ä¸­å›½æœ€é‡è¦çš„å¤éƒ½ä¹‹ä¸€ï¼Œä¹Ÿæ˜¯ä¸­å›½å†å²ä¸Šæœ€åä¸€ä¸ªå°å»ºç‹æœçš„éƒ½åŸã€‚åŒ—äº¬æœ‰è®¸å¤šè‘—åçš„å¤è¿¹å’Œæ™¯ç‚¹ï¼Œä¾‹å¦‚ç´«ç¦åŸã€å¤©å®‰é—¨å¹¿åœºå’Œé•¿åŸç­‰ã€‚\\n\\nä¸Šæµ·æ˜¯ä¸­å›½æœ€ç°ä»£åŒ–çš„åŸå¸‚ä¹‹ä¸€ï¼Œä¹Ÿæ˜¯ä¸­å›½å•†ä¸šå’Œé‡‘èä¸­å¿ƒã€‚ä¸Šæµ·æ‹¥æœ‰è®¸å¤šå›½é™…çŸ¥åçš„ä¼ä¸šå’Œé‡‘èæœºæ„ï¼ŒåŒæ—¶ä¹Ÿæœ‰è®¸å¤šè‘—åçš„æ™¯ç‚¹å’Œç¾é£Ÿã€‚ä¸Šæµ·çš„å¤–æ»©æ˜¯ä¸€ä¸ªå†å²æ‚ ä¹…çš„å•†ä¸šåŒºï¼Œæ‹¥æœ‰è®¸å¤šæ¬§å¼å»ºç­‘å’Œé¤é¦†ã€‚\\n\\né™¤æ­¤ä¹‹å¤–ï¼ŒåŒ—äº¬å’Œä¸Šæµ·åœ¨äº¤é€šå’Œäººå£æ–¹é¢ä¹Ÿæœ‰å¾ˆå¤§å·®å¼‚ã€‚åŒ—äº¬æ˜¯ä¸­å›½çš„é¦–éƒ½ï¼Œäººå£ä¼—å¤šï¼Œäº¤é€šæ‹¥å µé—®é¢˜è¾ƒä¸ºä¸¥é‡ã€‚è€Œä¸Šæµ·æ˜¯ä¸­å›½çš„å•†ä¸šå’Œé‡‘èä¸­å¿ƒï¼Œäººå£å¯†åº¦è¾ƒä½ï¼Œäº¤é€šç›¸å¯¹è¾ƒä¸ºä¾¿åˆ©ã€‚\\n\\næ€»çš„æ¥è¯´ï¼ŒåŒ—äº¬å’Œä¸Šæµ·æ˜¯ä¸¤ä¸ªæ‹¥æœ‰ç‹¬ç‰¹é­…åŠ›å’Œç‰¹ç‚¹çš„åŸå¸‚ï¼Œå¯ä»¥æ ¹æ®è‡ªå·±çš„å…´è¶£å’Œæ—¶é—´æ¥é€‰æ‹©å‰å¾€å…¶ä¸­ä¸€åº§åŸå¸‚æ—…æ¸¸ã€‚'\n\nPrevious\nCerebriumAI\nNext\nClarifai"
}