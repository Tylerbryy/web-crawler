{
	"title": "PromptLayer OpenAI | ğŸ¦œï¸ğŸ”— Langchain",
	"url": "https://python.langchain.com/docs/integrations/llms/promptlayer_openai",
	"html": "ComponentsLLMsPromptLayer OpenAI\nPromptLayer OpenAI\n\nPromptLayer is the first platform that allows you to track, manage, and share your GPT prompt engineering. PromptLayer acts a middleware between your code and OpenAIâ€™s python library.\n\nPromptLayer records all your OpenAI API requests, allowing you to search and explore request history in the PromptLayer dashboard.\n\nThis example showcases how to connect to PromptLayer to start recording your OpenAI requests.\n\nAnother example is here.\n\nInstall PromptLayerâ€‹\n\nThe promptlayer package is required to use PromptLayer with OpenAI. Install promptlayer using pip.\n\npip install promptlayer\n\nImportsâ€‹\nimport os\n\nimport promptlayer\nfrom langchain.llms import PromptLayerOpenAI\n\nSet the Environment API Keyâ€‹\n\nYou can create a PromptLayer API Key at www.promptlayer.com by clicking the settings cog in the navbar.\n\nSet it as an environment variable called PROMPTLAYER_API_KEY.\n\nYou also need an OpenAI Key, called OPENAI_API_KEY.\n\nfrom getpass import getpass\n\nPROMPTLAYER_API_KEY = getpass()\n\n     Â·Â·Â·Â·Â·Â·Â·Â·\n\nos.environ[\"PROMPTLAYER_API_KEY\"] = PROMPTLAYER_API_KEY\n\nfrom getpass import getpass\n\nOPENAI_API_KEY = getpass()\n\n     Â·Â·Â·Â·Â·Â·Â·Â·\n\nos.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n\nUse the PromptLayerOpenAI LLM like normalâ€‹\n\nYou can optionally pass in pl_tags to track your requests with PromptLayer's tagging feature.\n\nllm = PromptLayerOpenAI(pl_tags=[\"langchain\"])\nllm(\"I am a cat and I want\")\n\n\nThe above request should now appear on your PromptLayer dashboard.\n\nUsing PromptLayer Trackâ€‹\n\nIf you would like to use any of the PromptLayer tracking features, you need to pass the argument return_pl_id when instantiating the PromptLayer LLM to get the request id.\n\nllm = PromptLayerOpenAI(return_pl_id=True)\nllm_results = llm.generate([\"Tell me a joke\"])\n\nfor res in llm_results.generations:\n    pl_request_id = res[0].generation_info[\"pl_request_id\"]\n    promptlayer.track.score(request_id=pl_request_id, score=100)\n\n\nUsing this allows you to track the performance of your model in the PromptLayer dashboard. If you are using a prompt template, you can attach a template to a request as well. Overall, this gives you the opportunity to track the performance of different templates and models in the PromptLayer dashboard.\n\nPrevious\nPrediction Guard\nNext\nRELLM"
}