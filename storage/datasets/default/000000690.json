{
	"title": "Helicone | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/integrations/providers/helicone",
	"html": "ProvidersMoreHelicone\nHelicone\n\nThis page covers how to use the Helicone ecosystem within LangChain.\n\nWhat is Helicone?‚Äã\n\nHelicone is an open-source observability platform that proxies your OpenAI traffic and provides you key insights into your spend, latency and usage.\n\nQuick start‚Äã\n\nWith your LangChain environment you can just add the following parameter.\n\nexport OPENAI_API_BASE=\"https://oai.hconeai.com/v1\"\n\n\nNow head over to helicone.ai to create your account, and add your OpenAI API key within our dashboard to view your logs.\n\nHow to enable Helicone caching‚Äã\nfrom langchain.llms import OpenAI\nimport openai\nopenai.api_base = \"https://oai.hconeai.com/v1\"\n\nllm = OpenAI(temperature=0.9, headers={\"Helicone-Cache-Enabled\": \"true\"})\ntext = \"What is a helicone?\"\nprint(llm(text))\n\n\nHelicone caching docs\n\nHow to use Helicone custom properties‚Äã\nfrom langchain.llms import OpenAI\nimport openai\nopenai.api_base = \"https://oai.hconeai.com/v1\"\n\nllm = OpenAI(temperature=0.9, headers={\n        \"Helicone-Property-Session\": \"24\",\n        \"Helicone-Property-Conversation\": \"support_issue_2\",\n        \"Helicone-Property-App\": \"mobile\",\n      })\ntext = \"What is a helicone?\"\nprint(llm(text))\n\n\nHelicone property docs\n\nPrevious\nHazy Research\nNext\nHologres"
}