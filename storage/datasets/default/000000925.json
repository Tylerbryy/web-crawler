{
	"title": "Vespa | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/integrations/vectorstores/vespa",
	"html": "ComponentsVector storesVespa\nVespa\n\nVespa is a fully featured search engine and vector database. It supports vector search (ANN), lexical search, and search in structured data, all in the same query.\n\nThis notebook shows how to use Vespa.ai as a LangChain vector store.\n\nIn order to create the vector store, we use pyvespa to create a connection a Vespa service.\n\n#!pip install pyvespa\n\n\nUsing the pyvespa package, you can either connect to a Vespa Cloud instance or a local Docker instance. Here, we will create a new Vespa application and deploy that using Docker.\n\nCreating a Vespa application‚Äã\n\nFirst, we need to create an application package:\n\nfrom vespa.package import ApplicationPackage, Field, RankProfile\n\napp_package = ApplicationPackage(name=\"testapp\")\napp_package.schema.add_fields(\n    Field(\n        name=\"text\", type=\"string\", indexing=[\"index\", \"summary\"], index=\"enable-bm25\"\n    ),\n    Field(\n        name=\"embedding\",\n        type=\"tensor<float>(x[384])\",\n        indexing=[\"attribute\", \"summary\"],\n        attribute=[\"distance-metric: angular\"],\n    ),\n)\napp_package.schema.add_rank_profile(\n    RankProfile(\n        name=\"default\",\n        first_phase=\"closeness(field, embedding)\",\n        inputs=[(\"query(query_embedding)\", \"tensor<float>(x[384])\")],\n    )\n)\n\n\nThis sets up a Vespa application with a schema for each document that contains two fields: text for holding the document text and embedding for holding the embedding vector. The text field is set up to use a BM25 index for efficient text retrieval, and we'll see how to use this and hybrid search a bit later.\n\nThe embedding field is set up with a vector of length 384 to hold the embedding representation of the text. See Vespa's Tensor Guide for more on tensors in Vespa.\n\nLastly, we add a rank profile to instruct Vespa how to order documents. Here we set this up with a nearest neighbor search.\n\nNow we can deploy this application locally:\n\nfrom vespa.deployment import VespaDocker\n\nvespa_docker = VespaDocker()\nvespa_app = vespa_docker.deploy(application_package=app_package)\n\n\nThis deploys and creates a connection to a Vespa service. In case you already have a Vespa application running, for instance in the cloud, please refer to the PyVespa application for how to connect.\n\nCreating a Vespa vector store‚Äã\n\nNow, let's load some documents:\n\nfrom langchain.document_loaders import TextLoader\nfrom langchain.text_splitter import CharacterTextSplitter\n\nloader = TextLoader(\"../../modules/state_of_the_union.txt\")\ndocuments = loader.load()\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\ndocs = text_splitter.split_documents(documents)\n\nfrom langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n\nembedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n\n\nHere, we also set up local sentence embedder to transform the text to embedding vectors. One could also use OpenAI embeddings, but the vector length needs to be updated to 1536 to reflect the larger size of that embedding.\n\nTo feed these to Vespa, we need to configure how the vector store should map to fields in the Vespa application. Then we create the vector store directly from this set of documents:\n\nvespa_config = dict(\n    page_content_field=\"text\",\n    embedding_field=\"embedding\",\n    input_field=\"query_embedding\",\n)\n\nfrom langchain.vectorstores import VespaStore\n\ndb = VespaStore.from_documents(docs, embedding_function, app=vespa_app, **vespa_config)\n\n\nThis creates a Vespa vector store and feeds that set of documents to Vespa. The vector store takes care of calling the embedding function for each document and inserts them into the database.\n\nWe can now query the vector store:\n\nquery = \"What did the president say about Ketanji Brown Jackson\"\nresults = db.similarity_search(query)\n\nprint(results[0].page_content)\n\n\nThis will use the embedding function given above to create a representation for the query and use that to search Vespa. Note that this will use the default ranking function, which we set up in the application package above. You can use the ranking argument to similarity_search to specify which ranking function to use.\n\nPlease refer to the pyvespa documentation for more information.\n\nThis covers the basic usage of the Vespa store in LangChain. Now you can return the results and continue using these in LangChain.\n\nUpdating documents‚Äã\n\nAn alternative to calling from_documents, you can create the vector store directly and call add_texts from that. This can also be used to update documents:\n\nquery = \"What did the president say about Ketanji Brown Jackson\"\nresults = db.similarity_search(query)\nresult = results[0]\n\nresult.page_content = \"UPDATED: \" + result.page_content\ndb.add_texts([result.page_content], [result.metadata], result.metadata[\"id\"])\n\nresults = db.similarity_search(query)\nprint(results[0].page_content)\n\n\nHowever, the pyvespa library contains methods to manipulate content on Vespa which you can use directly.\n\nDeleting documents‚Äã\n\nYou can delete documents using the delete function:\n\nresult = db.similarity_search(query)\n# docs[0].metadata[\"id\"] == \"id:testapp:testapp::32\"\n\ndb.delete([\"32\"])\nresult = db.similarity_search(query)\n# docs[0].metadata[\"id\"] != \"id:testapp:testapp::32\"\n\n\nAgain, the pyvespa connection contains methods to delete documents as well.\n\nReturning with scores‚Äã\n\nThe similarity_search method only returns the documents in order of relevancy. To retrieve the actual scores:\n\nresults = db.similarity_search_with_score(query)\nresult = results[0]\n# result[1] ~= 0.463\n\n\nThis is a result of using the \"all-MiniLM-L6-v2\" embedding model using the cosine distance function (as given by the argument angular in the application function).\n\nDifferent embedding functions need different distance functions, and Vespa needs to know which distance function to use when orderings documents. Please refer to the documentation on distance functions for more information.\n\nAs retriever‚Äã\n\nTo use this vector store as a LangChain retriever simply call the as_retriever function, which is a standard vector store method:\n\ndb = VespaStore.from_documents(docs, embedding_function, app=vespa_app, **vespa_config)\nretriever = db.as_retriever()\nquery = \"What did the president say about Ketanji Brown Jackson\"\nresults = retriever.get_relevant_documents(query)\n\n# results[0].metadata[\"id\"] == \"id:testapp:testapp::32\"\n\n\nThis allows for more general, unstructured, retrieval from the vector store.\n\nMetadata‚Äã\n\nIn the example so far, we've only used the text and the embedding for that text. Documents usually contain additional information, which in LangChain is referred to as metadata.\n\nVespa can contain many fields with different types by adding them to the application package:\n\napp_package.schema.add_fields(\n    # ...\n    Field(name=\"date\", type=\"string\", indexing=[\"attribute\", \"summary\"]),\n    Field(name=\"rating\", type=\"int\", indexing=[\"attribute\", \"summary\"]),\n    Field(name=\"author\", type=\"string\", indexing=[\"attribute\", \"summary\"]),\n    # ...\n)\nvespa_app = vespa_docker.deploy(application_package=app_package)\n\n\nWe can add some metadata fields in the documents:\n\n# Add metadata\nfor i, doc in enumerate(docs):\n    doc.metadata[\"date\"] = f\"2023-{(i % 12)+1}-{(i % 28)+1}\"\n    doc.metadata[\"rating\"] = range(1, 6)[i % 5]\n    doc.metadata[\"author\"] = [\"Joe Biden\", \"Unknown\"][min(i, 1)]\n\n\nAnd let the Vespa vector store know about these fields:\n\nvespa_config.update(dict(metadata_fields=[\"date\", \"rating\", \"author\"]))\n\n\nNow, when searching for these documents, these fields will be returned. Also, these fields can be filtered on:\n\ndb = VespaStore.from_documents(docs, embedding_function, app=vespa_app, **vespa_config)\nquery = \"What did the president say about Ketanji Brown Jackson\"\nresults = db.similarity_search(query, filter=\"rating > 3\")\n# results[0].metadata[\"id\"] == \"id:testapp:testapp::34\"\n# results[0].metadata[\"author\"] == \"Unknown\"\n\nCustom query‚Äã\n\nIf the default behavior of the similarity search does not fit your requirements, you can always provide your own query. Thus, you don't need to provide all of the configuration to the vector store, but rather just write this yourself.\n\nFirst, let's add a BM25 ranking function to our application:\n\nfrom vespa.package import FieldSet\n\napp_package.schema.add_field_set(FieldSet(name=\"default\", fields=[\"text\"]))\napp_package.schema.add_rank_profile(RankProfile(name=\"bm25\", first_phase=\"bm25(text)\"))\nvespa_app = vespa_docker.deploy(application_package=app_package)\ndb = VespaStore.from_documents(docs, embedding_function, app=vespa_app, **vespa_config)\n\n\nThen, to perform a regular text search based on BM25:\n\nquery = \"What did the president say about Ketanji Brown Jackson\"\ncustom_query = {\n    \"yql\": \"select * from sources * where userQuery()\",\n    \"query\": query,\n    \"type\": \"weakAnd\",\n    \"ranking\": \"bm25\",\n    \"hits\": 4,\n}\nresults = db.similarity_search_with_score(query, custom_query=custom_query)\n# results[0][0].metadata[\"id\"] == \"id:testapp:testapp::32\"\n# results[0][1] ~= 14.384\n\n\nAll of the powerful search and query capabilities of Vespa can be used by using a custom query. Please refer to the Vespa documentation on it's Query API for more details.\n\nHybrid search‚Äã\n\nHybrid search means using both a classic term-based search such as BM25 and a vector search and combining the results. We need to create a new rank profile for hybrid search on Vespa:\n\napp_package.schema.add_rank_profile(\n    RankProfile(\n        name=\"hybrid\",\n        first_phase=\"log(bm25(text)) + 0.5 * closeness(field, embedding)\",\n        inputs=[(\"query(query_embedding)\", \"tensor<float>(x[384])\")],\n    )\n)\nvespa_app = vespa_docker.deploy(application_package=app_package)\ndb = VespaStore.from_documents(docs, embedding_function, app=vespa_app, **vespa_config)\n\n\nHere, we score each document as a combination of it's BM25 score and its distance score. We can query using a custom query:\n\nquery = \"What did the president say about Ketanji Brown Jackson\"\nquery_embedding = embedding_function.embed_query(query)\nnearest_neighbor_expression = (\n    \"{targetHits: 4}nearestNeighbor(embedding, query_embedding)\"\n)\ncustom_query = {\n    \"yql\": f\"select * from sources * where {nearest_neighbor_expression} and userQuery()\",\n    \"query\": query,\n    \"type\": \"weakAnd\",\n    \"input.query(query_embedding)\": query_embedding,\n    \"ranking\": \"hybrid\",\n    \"hits\": 4,\n}\nresults = db.similarity_search_with_score(query, custom_query=custom_query)\n# results[0][0].metadata[\"id\"], \"id:testapp:testapp::32\")\n# results[0][1] ~= 2.897\n\nNative embedders in Vespa‚Äã\n\nUp until this point we've used an embedding function in Python to provide embeddings for the texts. Vespa supports embedding function natively, so you can defer this calculation in to Vespa. One benefit is the ability to use GPUs when embedding documents if you have a large collections.\n\nPlease refer to Vespa embeddings for more information.\n\nFirst, we need to modify our application package:\n\nfrom vespa.package import Component, Parameter\n\napp_package.components = [\n    Component(\n        id=\"hf-embedder\",\n        type=\"hugging-face-embedder\",\n        parameters=[\n            Parameter(\"transformer-model\", {\"path\": \"...\"}),\n            Parameter(\"tokenizer-model\", {\"url\": \"...\"}),\n        ],\n    )\n]\nField(\n    name=\"hfembedding\",\n    type=\"tensor<float>(x[384])\",\n    is_document_field=False,\n    indexing=[\"input text\", \"embed hf-embedder\", \"attribute\", \"summary\"],\n    attribute=[\"distance-metric: angular\"],\n)\napp_package.schema.add_rank_profile(\n    RankProfile(\n        name=\"hf_similarity\",\n        first_phase=\"closeness(field, hfembedding)\",\n        inputs=[(\"query(query_embedding)\", \"tensor<float>(x[384])\")],\n    )\n)\n\n\nPlease refer to the embeddings documentation on adding embedder models and tokenizers to the application. Note that the hfembedding field includes instructions for embedding using the hf-embedder.\n\nNow we can query with a custom query:\n\nquery = \"What did the president say about Ketanji Brown Jackson\"\nnearest_neighbor_expression = (\n    \"{targetHits: 4}nearestNeighbor(internalembedding, query_embedding)\"\n)\ncustom_query = {\n    \"yql\": f\"select * from sources * where {nearest_neighbor_expression}\",\n    \"input.query(query_embedding)\": f'embed(hf-embedder, \"{query}\")',\n    \"ranking\": \"internal_similarity\",\n    \"hits\": 4,\n}\nresults = db.similarity_search_with_score(query, custom_query=custom_query)\n# results[0][0].metadata[\"id\"], \"id:testapp:testapp::32\")\n# results[0][1] ~= 0.630\n\n\nNote that the query here includes an embed instruction to embed the query using the same model as for the documents.\n\nApproximate nearest neighbor‚Äã\n\nIn all of the above examples, we've used exact nearest neighbor to find results. However, for large collections of documents this is not feasible as one has to scan through all documents to find the best matches. To avoid this, we can use approximate nearest neighbors.\n\nFirst, we can change the embedding field to create a HNSW index:\n\nfrom vespa.package import HNSW\n\napp_package.schema.add_fields(\n    Field(\n        name=\"embedding\",\n        type=\"tensor<float>(x[384])\",\n        indexing=[\"attribute\", \"summary\", \"index\"],\n        ann=HNSW(\n            distance_metric=\"angular\",\n            max_links_per_node=16,\n            neighbors_to_explore_at_insert=200,\n        ),\n    )\n)\n\n\nThis creates a HNSW index on the embedding data which allows for efficient searching. With this set, we can easily search using ANN by setting the approximate argument to True:\n\nquery = \"What did the president say about Ketanji Brown Jackson\"\nresults = db.similarity_search(query, approximate=True)\n# results[0][0].metadata[\"id\"], \"id:testapp:testapp::32\")\n\n\nThis covers most of the functionality in the Vespa vector store in LangChain.\n\nPrevious\nVectara\nNext\nWeaviate"
}