{
	"title": "Titan Takeoff Pro | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/integrations/llms/titan_takeoff_pro",
	"html": "ComponentsLLMsTitan Takeoff Pro\nTitan Takeoff Pro\n\nTitanML helps businesses build and deploy better, smaller, cheaper, and faster NLP models through our training, compression, and inference optimization platform.\n\nNote: These docs are for the Pro version of Titan Takeoff. For the community version, see the page for Titan Takeoff.\n\nOur inference server, Titan Takeoff (Pro Version) enables deployment of LLMs locally on your hardware in a single command. Most generative model architectures are supported, such as Falcon, Llama 2, GPT2, T5 and many more.\n\nExample usage‚Äã\n\nHere are some helpful examples to get started using the Pro version of Titan Takeoff Server. No parameters are needed by default, but a baseURL that points to your desired URL where Takeoff is running can be specified and generation parameters can be supplied.\n\nfrom langchain.callbacks.manager import CallbackManager\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\nfrom langchain.llms import TitanTakeoffPro\nfrom langchain.prompts import PromptTemplate\n\n# Example 1: Basic use\nllm = TitanTakeoffPro()\noutput = llm(\"What is the weather in London in August?\")\nprint(output)\n\n\n# Example 2: Specifying a port and other generation parameters\nllm = TitanTakeoffPro(\n    base_url=\"http://localhost:3000\",\n    min_new_tokens=128,\n    max_new_tokens=512,\n    no_repeat_ngram_size=2,\n    sampling_topk=1,\n    sampling_topp=1.0,\n    sampling_temperature=1.0,\n    repetition_penalty=1.0,\n    regex_string=\"\",\n)\noutput = llm(\"What is the largest rainforest in the world?\")\nprint(output)\n\n\n# Example 3: Using generate for multiple inputs\nllm = TitanTakeoffPro()\nrich_output = llm.generate([\"What is Deep Learning?\", \"What is Machine Learning?\"])\nprint(rich_output.generations)\n\n\n# Example 4: Streaming output\nllm = TitanTakeoffPro(\n    streaming=True, callback_manager=CallbackManager([StreamingStdOutCallbackHandler()])\n)\nprompt = \"What is the capital of France?\"\nllm(prompt)\n\n# Example 5: Using LCEL\nllm = TitanTakeoffPro()\nprompt = PromptTemplate.from_template(\"Tell me about {topic}\")\nchain = prompt | llm\nchain.invoke({\"topic\": \"the universe\"})\n\nPrevious\nTitan Takeoff\nNext\nTogether AI"
}