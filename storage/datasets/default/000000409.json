{
	"title": "Prompts | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/modules/model_io/chat/prompts",
	"html": "ModulesModel I/OChat modelsPrompts\nPrompts\n\nPrompts for chat models are built around messages, instead of just plain text.\n\nYou can make use of templating by using a MessagePromptTemplate. You can build a ChatPromptTemplate from one or more MessagePromptTemplates. You can use ChatPromptTemplate's format_prompt -- this returns a PromptValue, which you can convert to a string or Message object, depending on whether you want to use the formatted value as input to an llm or chat model.\n\nFor convenience, there is a from_template method defined on the template. If you were to use this template, this is what it would look like:\n\nfrom langchain.prompts import PromptTemplate\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    SystemMessagePromptTemplate,\n    AIMessagePromptTemplate,\n    HumanMessagePromptTemplate,\n)\n\ntemplate=\"You are a helpful assistant that translates {input_language} to {output_language}.\"\nsystem_message_prompt = SystemMessagePromptTemplate.from_template(template)\nhuman_template=\"{text}\"\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n\nchat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n\n# get a chat completion from the formatted messages\nchat(chat_prompt.format_prompt(input_language=\"English\", output_language=\"French\", text=\"I love programming.\").to_messages())\n\n    AIMessage(content=\"J'adore la programmation.\", additional_kwargs={})\n\n\nIf you wanted to construct the MessagePromptTemplate more directly, you could create a PromptTemplate outside and then pass it in, e.g.:\n\nprompt=PromptTemplate(\n    template=\"You are a helpful assistant that translates {input_language} to {output_language}.\",\n    input_variables=[\"input_language\", \"output_language\"],\n)\nsystem_message_prompt = SystemMessagePromptTemplate(prompt=prompt)\n\nPrevious\nCaching\nNext\nStreaming"
}