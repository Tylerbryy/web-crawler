{
	"title": "PromptLayer | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/integrations/callbacks/promptlayer",
	"html": "ComponentsCallbacksPromptLayer\nPromptLayer\n\nPromptLayer is a an LLM observability platform that lets you visualize requests, version prompts, and track usage. In this guide we will go over how to setup the PromptLayerCallbackHandler.\n\nWhile PromptLayer does have LLMs that integrate directly with LangChain (e.g. PromptLayerOpenAI), this callback is the recommended way to integrate PromptLayer with LangChain.\n\nSee our docs for more information.\n\nInstallation and Setup‚Äã\npip install promptlayer --upgrade\n\nGetting API Credentials‚Äã\n\nIf you do not have a PromptLayer account, create one on promptlayer.com. Then get an API key by clicking on the settings cog in the navbar and set it as an environment variabled called PROMPTLAYER_API_KEY\n\nUsage‚Äã\n\nGetting started with PromptLayerCallbackHandler is fairly simple, it takes two optional arguments:\n\npl_tags - an optional list of strings that will be tracked as tags on PromptLayer.\npl_id_callback - an optional function that will take promptlayer_request_id as an argument. This ID can be used with all of PromptLayer's tracking features to track, metadata, scores, and prompt usage.\nSimple OpenAI Example‚Äã\n\nIn this simple example we use PromptLayerCallbackHandler with ChatOpenAI. We add a PromptLayer tag named chatopenai\n\nimport promptlayer  # Don't forget this üç∞\nfrom langchain.callbacks import PromptLayerCallbackHandler\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.schema import (\n    HumanMessage,\n)\n\nchat_llm = ChatOpenAI(\n    temperature=0,\n    callbacks=[PromptLayerCallbackHandler(pl_tags=[\"chatopenai\"])],\n)\nllm_results = chat_llm(\n    [\n        HumanMessage(content=\"What comes after 1,2,3 ?\"),\n        HumanMessage(content=\"Tell me another joke?\"),\n    ]\n)\nprint(llm_results)\n\nGPT4All Example‚Äã\nimport promptlayer  # Don't forget this üç∞\nfrom langchain.callbacks import PromptLayerCallbackHandler\nfrom langchain.llms import GPT4All\n\nmodel = GPT4All(model=\"./models/gpt4all-model.bin\", n_ctx=512, n_threads=8)\n\nresponse = model(\n    \"Once upon a time, \",\n    callbacks=[PromptLayerCallbackHandler(pl_tags=[\"langchain\", \"gpt4all\"])],\n)\n\nFull Featured Example‚Äã\n\nIn this example we unlock more of the power of PromptLayer.\n\nPromptLayer allows you to visually create, version, and track prompt templates. Using the Prompt Registry, we can programmatically fetch the prompt template called example.\n\nWe also define a pl_id_callback function which takes in the promptlayer_request_id and logs a score, metadata and links the prompt template used. Read more about tracking on our docs.\n\nimport promptlayer  # Don't forget this üç∞\nfrom langchain.callbacks import PromptLayerCallbackHandler\nfrom langchain.llms import OpenAI\n\n\ndef pl_id_callback(promptlayer_request_id):\n    print(\"prompt layer id \", promptlayer_request_id)\n    promptlayer.track.score(\n        request_id=promptlayer_request_id, score=100\n    )  # score is an integer 0-100\n    promptlayer.track.metadata(\n        request_id=promptlayer_request_id, metadata={\"foo\": \"bar\"}\n    )  # metadata is a dictionary of key value pairs that is tracked on PromptLayer\n    promptlayer.track.prompt(\n        request_id=promptlayer_request_id,\n        prompt_name=\"example\",\n        prompt_input_variables={\"product\": \"toasters\"},\n        version=1,\n    )  # link the request to a prompt template\n\n\nopenai_llm = OpenAI(\n    model_name=\"text-davinci-002\",\n    callbacks=[PromptLayerCallbackHandler(pl_id_callback=pl_id_callback)],\n)\n\nexample_prompt = promptlayer.prompts.get(\"example\", version=1, langchain=True)\nopenai_llm(example_prompt.format(product=\"toasters\"))\n\n\nThat is all it takes! After setup all your requests will show up on the PromptLayer dashboard. This callback also works with any LLM implemented on LangChain.\n\nPrevious\nLLMonitor\nNext\nSageMaker Tracking"
}