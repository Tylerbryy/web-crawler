{
	"title": "LangChain Decorators ✨ | 🦜️🔗 Langchain",
	"url": "https://python.langchain.com/docs/integrations/providers/langchain_decorators",
	"html": "ProvidersMoreLangChain Decorators ✨\nLangChain Decorators ✨\n\nlanchchain decorators is a layer on the top of LangChain that provides syntactic sugar 🍭 for writing custom langchain prompts and chains\n\nFor Feedback, Issues, Contributions - please raise an issue here: ju-bezdek/langchain-decorators\n\nMain principles and benefits:\n\nmore pythonic way of writing code\nwrite multiline prompts that won't break your code flow with indentation\nmaking use of IDE in-built support for hinting, type checking and popup with docs to quickly peek in the function to see the prompt, parameters it consumes etc.\nleverage all the power of 🦜🔗 LangChain ecosystem\nadding support for optional parameters\neasily share parameters between the prompts by binding them to one class\n\nHere is a simple example of a code written with LangChain Decorators ✨\n\n\n@llm_prompt\ndef write_me_short_post(topic:str, platform:str=\"twitter\", audience:str = \"developers\")->str:\n    \"\"\"\n    Write me a short header for my post about {topic} for {platform} platform. \n    It should be for {audience} audience.\n    (Max 15 words)\n    \"\"\"\n    return\n\n# run it naturally\nwrite_me_short_post(topic=\"starwars\")\n# or\nwrite_me_short_post(topic=\"starwars\", platform=\"redit\")\n\nQuick start\nInstallation​\npip install langchain_decorators\n\nExamples​\n\nGood idea on how to start is to review the examples here:\n\njupyter notebook\ncolab notebook\nDefining other parameters\n\nHere we are just marking a function as a prompt with llm_prompt decorator, turning it effectively into a LLMChain. Instead of running it\n\nStandard LLMchain takes much more init parameter than just inputs_variables and prompt... here is this implementation detail hidden in the decorator. Here is how it works:\n\nUsing Global settings:\n# define global settings for all prompty (if not set - chatGPT is the current default)\nfrom langchain_decorators import GlobalSettings\n\nGlobalSettings.define_settings(\n    default_llm=ChatOpenAI(temperature=0.0), this is default... can change it here globally\n    default_streaming_llm=ChatOpenAI(temperature=0.0,streaming=True), this is default... can change it here for all ... will be used for streaming\n)\n\nUsing predefined prompt types\n#You can change the default prompt types\nfrom langchain_decorators import PromptTypes, PromptTypeSettings\n\nPromptTypes.AGENT_REASONING.llm = ChatOpenAI()\n\n# Or you can just define your own ones:\nclass MyCustomPromptTypes(PromptTypes):\n    GPT4=PromptTypeSettings(llm=ChatOpenAI(model=\"gpt-4\"))\n\n@llm_prompt(prompt_type=MyCustomPromptTypes.GPT4) \ndef write_a_complicated_code(app_idea:str)->str:\n    ...\n\n\nDefine the settings directly in the decorator\nfrom langchain.llms import OpenAI\n\n@llm_prompt(\n    llm=OpenAI(temperature=0.7),\n    stop_tokens=[\"\\nObservation\"],\n    ...\n    )\ndef creative_writer(book_title:str)->str:\n    ...\n\nPassing a memory and/or callbacks:​\n\nTo pass any of these, just declare them in the function (or use kwargs to pass anything)\n\n\n@llm_prompt()\nasync def write_me_short_post(topic:str, platform:str=\"twitter\", memory:SimpleMemory = None):\n    \"\"\"\n    {history_key}\n    Write me a short header for my post about {topic} for {platform} platform. \n    It should be for {audience} audience.\n    (Max 15 words)\n    \"\"\"\n    pass\n\nawait write_me_short_post(topic=\"old movies\")\n\n\nSimplified streaming\n\nIf we want to leverage streaming:\n\nwe need to define prompt as async function\nturn on the streaming on the decorator, or we can define PromptType with streaming on\ncapture the stream using StreamingContext\n\nThis way we just mark which prompt should be streamed, not needing to tinker with what LLM should we use, passing around the creating and distribute streaming handler into particular part of our chain... just turn the streaming on/off on prompt/prompt type...\n\nThe streaming will happen only if we call it in streaming context ... there we can define a simple function to handle the stream\n\n# this code example is complete and should run as it is\n\nfrom langchain_decorators import StreamingContext, llm_prompt\n\n# this will mark the prompt for streaming (useful if we want stream just some prompts in our app... but don't want to pass distribute the callback handlers)\n# note that only async functions can be streamed (will get an error if it's not)\n@llm_prompt(capture_stream=True) \nasync def write_me_short_post(topic:str, platform:str=\"twitter\", audience:str = \"developers\"):\n    \"\"\"\n    Write me a short header for my post about {topic} for {platform} platform. \n    It should be for {audience} audience.\n    (Max 15 words)\n    \"\"\"\n    pass\n\n\n\n# just an arbitrary  function to demonstrate the streaming... will be some websockets code in the real world\ntokens=[]\ndef capture_stream_func(new_token:str):\n    tokens.append(new_token)\n\n# if we want to capture the stream, we need to wrap the execution into StreamingContext... \n# this will allow us to capture the stream even if the prompt call is hidden inside higher level method\n# only the prompts marked with capture_stream will be captured here\nwith StreamingContext(stream_to_stdout=True, callback=capture_stream_func):\n    result = await run_prompt()\n    print(\"Stream finished ... we can distinguish tokens thanks to alternating colors\")\n\n\nprint(\"\\nWe've captured\",len(tokens),\"tokens🎉\\n\")\nprint(\"Here is the result:\")\nprint(result)\n\nPrompt declarations\n\nBy default the prompt is is the whole function docs, unless you mark your prompt\n\nDocumenting your prompt​\n\nWe can specify what part of our docs is the prompt definition, by specifying a code block with <prompt> language tag\n\n@llm_prompt\ndef write_me_short_post(topic:str, platform:str=\"twitter\", audience:str = \"developers\"):\n    \"\"\"\n    Here is a good way to write a prompt as part of a function docstring, with additional documentation for devs.\n\n    It needs to be a code block, marked as a `<prompt>` language\n    ```<prompt>\n    Write me a short header for my post about {topic} for {platform} platform. \n    It should be for {audience} audience.\n    (Max 15 words)\n    ```\n\n    Now only to code block above will be used as a prompt, and the rest of the docstring will be used as a description for developers.\n    (It has also a nice benefit that IDE (like VS code) will display the prompt properly (not trying to parse it as markdown, and thus not showing new lines properly))\n    \"\"\"\n    return \n\nChat messages prompt​\n\nFor chat models is very useful to define prompt as a set of message templates... here is how to do it:\n\n@llm_prompt\ndef simulate_conversation(human_input:str, agent_role:str=\"a pirate\"):\n    \"\"\"\n    ## System message\n     - note the `:system` sufix inside the <prompt:_role_> tag\n     \n\n    ```<prompt:system>\n    You are a {agent_role} hacker. You mus act like one.\n    You reply always in code, using python or javascript code block...\n    for example:\n    \n    ... do not reply with anything else.. just with code - respecting your role.\n    ```\n\n    # human message \n    (we are using the real role that are enforced by the LLM - GPT supports system, assistant, user)\n    ``` <prompt:user>\n    Helo, who are you\n    ```\n    a reply:\n    \n\n    ``` <prompt:assistant>\n    \\``` python <<- escaping inner code block with \\ that should be part of the prompt\n    def hello():\n        print(\"Argh... hello you pesky pirate\")\n    \\```\n    ```\n    \n    we can also add some history using placeholder\n    ```<prompt:placeholder>\n    {history}\n    ```\n    ```<prompt:user>\n    {human_input}\n    ```\n\n    Now only to code block above will be used as a prompt, and the rest of the docstring will be used as a description for developers.\n    (It has also a nice benefit that IDE (like VS code) will display the prompt properly (not trying to parse it as markdown, and thus not showing new lines properly))\n    \"\"\"\n    pass\n\n\n\nthe roles here are model native roles (assistant, user, system for chatGPT)\n\nOptional sections\nyou can define a whole sections of your prompt that should be optional\nif any input in the section is missing, the whole section won't be rendered\n\nthe syntax for this is as follows:\n\n@llm_prompt\ndef prompt_with_optional_partials():\n    \"\"\"\n    this text will be rendered always, but\n\n    {? anything inside this block will be rendered only if all the {value}s parameters are not empty (None | \"\")   ?}\n\n    you can also place it in between the words\n    this too will be rendered{? , but\n        this  block will be rendered only if {this_value} and {this_value}\n        is not empty?} !\n    \"\"\"\n\nOutput parsers\nllm_prompt decorator natively tries to detect the best output parser based on the output type. (if not set, it returns the raw string)\nlist, dict and pydantic outputs are also supported natively (automatically)\n# this code example is complete and should run as it is\n\nfrom langchain_decorators import llm_prompt\n\n@llm_prompt\ndef write_name_suggestions(company_business:str, count:int)->list:\n    \"\"\" Write me {count} good name suggestions for company that {company_business}\n    \"\"\"\n    pass\n\nwrite_name_suggestions(company_business=\"sells cookies\", count=5)\n\nMore complex structures​\n\nfor dict / pydantic you need to specify the formatting instructions... this can be tedious, that's why you can let the output parser gegnerate you the instructions based on the model (pydantic)\n\nfrom langchain_decorators import llm_prompt\nfrom pydantic import BaseModel, Field\n\n\nclass TheOutputStructureWeExpect(BaseModel):\n    name:str = Field (description=\"The name of the company\")\n    headline:str = Field( description=\"The description of the company (for landing page)\")\n    employees:list[str] = Field(description=\"5-8 fake employee names with their positions\")\n\n@llm_prompt()\ndef fake_company_generator(company_business:str)->TheOutputStructureWeExpect:\n    \"\"\" Generate a fake company that {company_business}\n    {FORMAT_INSTRUCTIONS}\n    \"\"\"\n    return\n\ncompany = fake_company_generator(company_business=\"sells cookies\")\n\n# print the result nicely formatted\nprint(\"Company name: \",company.name)\nprint(\"company headline: \",company.headline)\nprint(\"company employees: \",company.employees)\n\n\nBinding the prompt to an object\nfrom pydantic import BaseModel\nfrom langchain_decorators import llm_prompt\n\nclass AssistantPersonality(BaseModel):\n    assistant_name:str\n    assistant_role:str\n    field:str\n\n    @property\n    def a_property(self):\n        return \"whatever\"\n\n    def hello_world(self, function_kwarg:str=None):\n        \"\"\"\n        We can reference any {field} or {a_property} inside our prompt... and combine it with {function_kwarg} in the method\n        \"\"\"\n\n    \n    @llm_prompt\n    def introduce_your_self(self)->str:\n        \"\"\"\n        ``` <prompt:system>\n        You are an assistant named {assistant_name}. \n        Your role is to act as {assistant_role}\n        ```\n        ```<prompt:user>\n        Introduce your self (in less than 20 words)\n        ```\n        \"\"\"\n\n    \n\npersonality = AssistantPersonality(assistant_name=\"John\", assistant_role=\"a pirate\")\n\nprint(personality.introduce_your_self(personality))\n\nMore examples:\nthese and few more examples are also available in the colab notebook here\nincluding the ReAct Agent re-implementation using purely langchain decorators\nPrevious\nLanceDB\nNext\nLlama.cpp"
}