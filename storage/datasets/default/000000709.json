{
	"title": "MLflow AI Gateway | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/integrations/providers/mlflow_ai_gateway",
	"html": "ProvidersMoreMLflow AI Gateway\nMLflow AI Gateway\n\nThe MLflow AI Gateway service is a powerful tool designed to streamline the usage and management of various large language model (LLM) providers, such as OpenAI and Anthropic, within an organization. It offers a high-level interface that simplifies the interaction with these services by providing a unified endpoint to handle specific LLM related requests. See the MLflow AI Gateway documentation for more details.\n\nInstallation and Setup‚Äã\n\nInstall mlflow with MLflow AI Gateway dependencies:\n\npip install 'mlflow[gateway]'\n\n\nSet the OpenAI API key as an environment variable:\n\nexport OPENAI_API_KEY=...\n\n\nCreate a configuration file:\n\nroutes:\n  - name: completions\n    route_type: llm/v1/completions\n    model:\n      provider: openai\n      name: text-davinci-003\n      config:\n        openai_api_key: $OPENAI_API_KEY\n\n  - name: embeddings\n    route_type: llm/v1/embeddings\n    model:\n      provider: openai\n      name: text-embedding-ada-002\n      config:\n        openai_api_key: $OPENAI_API_KEY\n\n\nStart the Gateway server:\n\nmlflow gateway start --config-path /path/to/config.yaml\n\nExample provided by MLflow‚Äã\n\nThe mlflow.langchain module provides an API for logging and loading LangChain models. This module exports multivariate LangChain models in the langchain flavor and univariate LangChain models in the pyfunc flavor.\n\nSee the API documentation and examples.\n\nCompletions Example‚Äã\nimport mlflow\nfrom langchain.chains import LLMChain, PromptTemplate\nfrom langchain.llms import MlflowAIGateway\n\ngateway = MlflowAIGateway(\n    gateway_uri=\"http://127.0.0.1:5000\",\n    route=\"completions\",\n    params={\n        \"temperature\": 0.0,\n        \"top_p\": 0.1,\n    },\n)\n\nllm_chain = LLMChain(\n    llm=gateway,\n    prompt=PromptTemplate(\n        input_variables=[\"adjective\"],\n        template=\"Tell me a {adjective} joke\",\n    ),\n)\nresult = llm_chain.run(adjective=\"funny\")\nprint(result)\n\nwith mlflow.start_run():\n    model_info = mlflow.langchain.log_model(chain, \"model\")\n\nmodel = mlflow.pyfunc.load_model(model_info.model_uri)\nprint(model.predict([{\"adjective\": \"funny\"}]))\n\nEmbeddings Example‚Äã\nfrom langchain.embeddings import MlflowAIGatewayEmbeddings\n\nembeddings = MlflowAIGatewayEmbeddings(\n    gateway_uri=\"http://127.0.0.1:5000\",\n    route=\"embeddings\",\n)\n\nprint(embeddings.embed_query(\"hello\"))\nprint(embeddings.embed_documents([\"hello\"]))\n\nChat Example‚Äã\nfrom langchain.chat_models import ChatMLflowAIGateway\nfrom langchain.schema import HumanMessage, SystemMessage\n\nchat = ChatMLflowAIGateway(\n    gateway_uri=\"http://127.0.0.1:5000\",\n    route=\"chat\",\n    params={\n        \"temperature\": 0.1\n    }\n)\n\nmessages = [\n    SystemMessage(\n        content=\"You are a helpful assistant that translates English to French.\"\n    ),\n    HumanMessage(\n        content=\"Translate this sentence from English to French: I love programming.\"\n    ),\n]\nprint(chat(messages))\n\nDatabricks MLflow AI Gateway‚Äã\n\nDatabricks MLflow AI Gateway is in private preview. Please contact a Databricks representative to enroll in the preview.\n\nfrom langchain.chains import LLMChain\nfrom langchain.prompts import PromptTemplate\nfrom langchain.llms import MlflowAIGateway\n\ngateway = MlflowAIGateway(\n    gateway_uri=\"databricks\",\n    route=\"completions\",\n)\n\nllm_chain = LLMChain(\n    llm=gateway,\n    prompt=PromptTemplate(\n        input_variables=[\"adjective\"],\n        template=\"Tell me a {adjective} joke\",\n    ),\n)\nresult = llm_chain.run(adjective=\"funny\")\nprint(result)\n\nPrevious\nMinimax\nNext\nMLflow"
}