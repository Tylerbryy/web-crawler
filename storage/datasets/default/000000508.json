{
	"title": "ğŸš… LiteLLM | ğŸ¦œï¸ğŸ”— Langchain",
	"url": "https://python.langchain.com/docs/integrations/chat/litellm",
	"html": "ComponentsChat modelsğŸš… LiteLLM\nğŸš… LiteLLM\n\nLiteLLM is a library that simplifies calling Anthropic, Azure, Huggingface, Replicate, etc.\n\nThis notebook covers how to get started with using Langchain + the LiteLLM I/O library.\n\nfrom langchain.chat_models import ChatLiteLLM\nfrom langchain.schema import HumanMessage\n\nchat = ChatLiteLLM(model=\"gpt-3.5-turbo\")\n\nmessages = [\n    HumanMessage(\n        content=\"Translate this sentence from English to French. I love programming.\"\n    )\n]\nchat(messages)\n\n    AIMessage(content=\" J'aime la programmation.\", additional_kwargs={}, example=False)\n\nChatLiteLLM also supports async and streaming functionality:â€‹\nfrom langchain.callbacks.manager import CallbackManager\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n\nawait chat.agenerate([messages])\n\n    LLMResult(generations=[[ChatGeneration(text=\" J'aime programmer.\", generation_info=None, message=AIMessage(content=\" J'aime programmer.\", additional_kwargs={}, example=False))]], llm_output={}, run=[RunInfo(run_id=UUID('8cc8fb68-1c35-439c-96a0-695036a93652'))])\n\nchat = ChatLiteLLM(\n    streaming=True,\n    verbose=True,\n    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n)\nchat(messages)\n\n     J'aime la programmation.\n\n\n\n\n    AIMessage(content=\" J'aime la programmation.\", additional_kwargs={}, example=False)\n\nPrevious\nKonko\nNext\nLlama-2 Chat"
}