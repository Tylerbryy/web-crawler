{
	"title": "Document transformers | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/modules/data_connection/document_transformers/",
	"html": "ModulesRetrievalDocument transformers\nDocument transformers\nINFO\n\nHead to Integrations for documentation on built-in document transformer integrations with 3rd-party tools.\n\nOnce you've loaded documents, you'll often want to transform them to better suit your application. The simplest example is you may want to split a long document into smaller chunks that can fit into your model's context window. LangChain has a number of built-in document transformers that make it easy to split, combine, filter, and otherwise manipulate documents.\n\nText splitters‚Äã\n\nWhen you want to deal with long pieces of text, it is necessary to split up that text into chunks. As simple as this sounds, there is a lot of potential complexity here. Ideally, you want to keep the semantically related pieces of text together. What \"semantically related\" means could depend on the type of text. This notebook showcases several ways to do that.\n\nAt a high level, text splitters work as following:\n\nSplit the text up into small, semantically meaningful chunks (often sentences).\nStart combining these small chunks into a larger chunk until you reach a certain size (as measured by some function).\nOnce you reach that size, make that chunk its own piece of text and then start creating a new chunk of text with some overlap (to keep context between chunks).\n\nThat means there are two different axes along which you can customize your text splitter:\n\nHow the text is split\nHow the chunk size is measured\nGet started with text splitters‚Äã\n\nThe default recommended text splitter is the RecursiveCharacterTextSplitter. This text splitter takes a list of characters. It tries to create chunks based on splitting on the first character, but if any chunks are too large it then moves onto the next character, and so forth. By default the characters it tries to split on are [\"\\n\\n\", \"\\n\", \" \", \"\"]\n\nIn addition to controlling which characters you can split on, you can also control a few other things:\n\nlength_function: how the length of chunks is calculated. Defaults to just counting number of characters, but it's pretty common to pass a token counter here.\nchunk_size: the maximum size of your chunks (as measured by the length function).\nchunk_overlap: the maximum overlap between chunks. It can be nice to have some overlap to maintain some continuity between chunks (e.g. do a sliding window).\nadd_start_index: whether to include the starting position of each chunk within the original document in the metadata.\n# This is a long document we can split up.\nwith open('../../state_of_the_union.txt') as f:\n    state_of_the_union = f.read()\n\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    # Set a really small chunk size, just to show.\n    chunk_size = 100,\n    chunk_overlap  = 20,\n    length_function = len,\n    add_start_index = True,\n)\n\ntexts = text_splitter.create_documents([state_of_the_union])\nprint(texts[0])\nprint(texts[1])\n\n    page_content='Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and' metadata={'start_index': 0}\n    page_content='of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.' metadata={'start_index': 82}\n\nOther transformations:‚Äã\nFilter redundant docs, translate docs, extract metadata, and more‚Äã\n\nWe can do perform a number of transformations on docs which are not simply splitting the text. With the EmbeddingsRedundantFilter we can identify similar documents and filter out redundancies. With integrations like doctran we can do things like translate documents from one language to another, extract desired properties and add them to metadata, and convert conversational dialogue into a Q/A format set of documents.\n\nPrevious\nPDF\nNext\nHTMLHeaderTextSplitter"
}