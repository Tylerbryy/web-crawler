{
	"title": "Anyscale | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/integrations/llms/anyscale",
	"html": "ComponentsLLMsAnyscale\nAnyscale\n\nAnyscale is a fully-managed Ray platform, on which you can build, deploy, and manage scalable AI and Python applications\n\nThis example goes over how to use LangChain to interact with Anyscale Endpoint.\n\nimport os\n\nos.environ[\"ANYSCALE_API_BASE\"] = ANYSCALE_API_BASE\nos.environ[\"ANYSCALE_API_KEY\"] = ANYSCALE_API_KEY\n\nfrom langchain.chains import LLMChain\nfrom langchain.llms import Anyscale\nfrom langchain.prompts import PromptTemplate\n\ntemplate = \"\"\"Question: {question}\n\nAnswer: Let's think step by step.\"\"\"\n\nprompt = PromptTemplate(template=template, input_variables=[\"question\"])\n\nllm = Anyscale(model_name=ANYSCALE_MODEL_NAME)\n\nllm_chain = LLMChain(prompt=prompt, llm=llm)\n\nquestion = \"When was George Washington president?\"\n\nllm_chain.run(question)\n\n\nWith Ray, we can distribute the queries without asynchronized implementation. This not only applies to Anyscale LLM model, but to any other Langchain LLM models which do not have _acall or _agenerate implemented\n\nprompt_list = [\n    \"When was George Washington president?\",\n    \"Explain to me the difference between nuclear fission and fusion.\",\n    \"Give me a list of 5 science fiction books I should read next.\",\n    \"Explain the difference between Spark and Ray.\",\n    \"Suggest some fun holiday ideas.\",\n    \"Tell a joke.\",\n    \"What is 2+2?\",\n    \"Explain what is machine learning like I am five years old.\",\n    \"Explain what is artifical intelligence.\",\n]\n\nimport ray\n\n\n@ray.remote(num_cpus=0.1)\ndef send_query(llm, prompt):\n    resp = llm(prompt)\n    return resp\n\n\nfutures = [send_query.remote(llm, prompt) for prompt in prompt_list]\nresults = ray.get(futures)\n\nPrevious\nAmazon API Gateway\nNext\nArcee"
}