{
	"title": "Token counting | ğŸ¦œï¸ğŸ”— Langchain",
	"url": "https://python.langchain.com/docs/modules/callbacks/token_counting",
	"html": "ModulesMoreCallbacksToken counting\nToken counting\n\nLangChain offers a context manager that allows you to count tokens.\n\nimport asyncio\n\nfrom langchain.callbacks import get_openai_callback\nfrom langchain.llms import OpenAI\n\nllm = OpenAI(temperature=0)\nwith get_openai_callback() as cb:\n    llm(\"What is the square root of 4?\")\n\ntotal_tokens = cb.total_tokens\nassert total_tokens > 0\n\nwith get_openai_callback() as cb:\n    llm(\"What is the square root of 4?\")\n    llm(\"What is the square root of 4?\")\n\nassert cb.total_tokens == total_tokens * 2\n\n# You can kick off concurrent runs from within the context manager\nwith get_openai_callback() as cb:\n    await asyncio.gather(\n        *[llm.agenerate([\"What is the square root of 4?\"]) for _ in range(3)]\n    )\n\nassert cb.total_tokens == total_tokens * 3\n\n# The context manager is concurrency safe\ntask = asyncio.create_task(llm.agenerate([\"What is the square root of 4?\"]))\nwith get_openai_callback() as cb:\n    await llm.agenerate([\"What is the square root of 4?\"])\n\nawait task\nassert cb.total_tokens == total_tokens\n\nPrevious\nTags\nNext\nğŸ¦œï¸ğŸ“ LangServe"
}