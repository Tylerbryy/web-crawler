{
	"title": "RWKV-4 | ðŸ¦œï¸ðŸ”— Langchain",
	"url": "https://python.langchain.com/docs/integrations/providers/rwkv",
	"html": "ProvidersMoreRWKV-4\nRWKV-4\n\nThis page covers how to use the RWKV-4 wrapper within LangChain. It is broken into two parts: installation and setup, and then usage with an example.\n\nInstallation and Setupâ€‹\nInstall the Python package with pip install rwkv\nInstall the tokenizer Python package with pip install tokenizer\nDownload a RWKV model and place it in your desired directory\nDownload the tokens file\nUsageâ€‹\nRWKVâ€‹\n\nTo use the RWKV wrapper, you need to provide the path to the pre-trained model file and the tokenizer's configuration.\n\nfrom langchain.llms import RWKV\n\n# Test the model\n\n```python\n\ndef generate_prompt(instruction, input=None):\n    if input:\n        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n# Instruction:\n{instruction}\n\n# Input:\n{input}\n\n# Response:\n\"\"\"\n    else:\n        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n# Instruction:\n{instruction}\n\n# Response:\n\"\"\"\n\n\nmodel = RWKV(model=\"./models/RWKV-4-Raven-3B-v7-Eng-20230404-ctx4096.pth\", strategy=\"cpu fp32\", tokens_path=\"./rwkv/20B_tokenizer.json\")\nresponse = model(generate_prompt(\"Once upon a time, \"))\n\nModel Fileâ€‹\n\nYou can find links to model file downloads at the RWKV-4-Raven repository.\n\nRwkv-4 models -> recommended VRAMâ€‹\nRWKV VRAM\nModel | 8bit | bf16/fp16 | fp32\n14B   | 16GB | 28GB      | >50GB\n7B    | 8GB  | 14GB      | 28GB\n3B    | 2.8GB| 6GB       | 12GB\n1b5   | 1.3GB| 3GB       | 6GB\n\n\nSee the rwkv pip page for more information about strategies, including streaming and cuda support.\n\nPrevious\nRunhouse\nNext\nSalute Devices"
}