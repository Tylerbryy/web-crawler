{
	"title": "rag-pinecone-multi-query | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/templates/rag-pinecone-multi-query",
	"html": "Templatesrag-pinecone-multi-query\nrag-pinecone-multi-query\n\nThis template performs RAG using Pinecone and OpenAI with a multi-query retriever.\n\nIt uses an LLM to generate multiple queries from different perspectives based on the user's input query.\n\nFor each query, it retrieves a set of relevant documents and takes the unique union across all queries for answer synthesis.\n\nEnvironment Setup‚Äã\n\nThis template uses Pinecone as a vectorstore and requires that PINECONE_API_KEY, PINECONE_ENVIRONMENT, and PINECONE_INDEX are set.\n\nSet the OPENAI_API_KEY environment variable to access the OpenAI models.\n\nUsage‚Äã\n\nTo use this package, you should first install the LangChain CLI:\n\npip install -U langchain-cli\n\n\nTo create a new LangChain project and install this package, do:\n\nlangchain app new my-app --package rag-pinecone-multi-query\n\n\nTo add this package to an existing project, run:\n\nlangchain app add rag-pinecone-multi-query\n\n\nAnd add the following code to your server.py file:\n\nfrom rag_pinecone_multi_query import chain as rag_pinecone_multi_query_chain\n\nadd_routes(app, rag_pinecone_multi_query_chain, path=\"/rag-pinecone-multi-query\")\n\n\n(Optional) Now, let's configure LangSmith. LangSmith will help us trace, monitor, and debug LangChain applications. LangSmith is currently in private beta, you can sign up here. If you don't have access, you can skip this section\n\nexport LANGCHAIN_TRACING_V2=true\nexport LANGCHAIN_API_KEY=<your-api-key>\nexport LANGCHAIN_PROJECT=<your-project>  # if not specified, defaults to \"default\"\n\n\nIf you are inside this directory, then you can spin up a LangServe instance directly by:\n\nlangchain serve\n\n\nThis will start the FastAPI app with a server running locally at http://localhost:8000\n\nYou can see all templates at http://127.0.0.1:8000/docs You can access the playground at http://127.0.0.1:8000/rag-pinecone-multi-query/playground\n\nTo access the template from code, use:\n\nfrom langserve.client import RemoteRunnable\n\nrunnable = RemoteRunnable(\"http://localhost:8000/rag-pinecone-multi-query\")\n\nPrevious\nRAG with Multiple Indexes (Routing)\nNext\nrag-pinecone-rerank"
}