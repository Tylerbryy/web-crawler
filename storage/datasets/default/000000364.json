{
	"title": "Amazon Comprehend Moderation Chain | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/guides/safety/amazon_comprehend_chain",
	"html": "SafetyAmazon Comprehend Moderation Chain\nAmazon Comprehend Moderation Chain\n\nThis notebook shows how to use Amazon Comprehend to detect and handle Personally Identifiable Information (PII) and toxicity.\n\nSetting up‚Äã\n%pip install boto3 nltk\n\n%pip install -U langchain_experimental\n\n%pip install -U langchain pydantic\n\nimport os\n\nimport boto3\n\ncomprehend_client = boto3.client(\"comprehend\", region_name=\"us-east-1\")\n\nfrom langchain_experimental.comprehend_moderation import AmazonComprehendModerationChain\n\ncomprehend_moderation = AmazonComprehendModerationChain(\n    client=comprehend_client,\n    verbose=True,  # optional\n)\n\nUsing AmazonComprehendModerationChain with LLM chain‚Äã\n\nNote: The example below uses the Fake LLM from LangChain, but the same concept could be applied to other LLMs.\n\nfrom langchain.llms.fake import FakeListLLM\nfrom langchain.prompts import PromptTemplate\nfrom langchain_experimental.comprehend_moderation.base_moderation_exceptions import (\n    ModerationPiiError,\n)\n\ntemplate = \"\"\"Question: {question}\n\nAnswer:\"\"\"\n\nprompt = PromptTemplate(template=template, input_variables=[\"question\"])\n\nresponses = [\n    \"Final Answer: A credit card number looks like 1289-2321-1123-2387. A fake SSN number looks like 323-22-9980. John Doe's phone number is (999)253-9876.\",\n    # replace with your own expletive\n    \"Final Answer: This is a really <expletive> way of constructing a birdhouse. This is <expletive> insane to think that any birds would actually create their <expletive> nests here.\",\n]\nllm = FakeListLLM(responses=responses)\n\nchain = (\n    prompt\n    | comprehend_moderation\n    | {\"input\": (lambda x: x[\"output\"]) | llm}\n    | comprehend_moderation\n)\n\ntry:\n    response = chain.invoke(\n        {\n            \"question\": \"A sample SSN number looks like this 123-22-3345. Can you give me some more samples?\"\n        }\n    )\nexcept ModerationPiiError as e:\n    print(str(e))\nelse:\n    print(response[\"output\"])\n\nUsing moderation_config to customize your moderation‚Äã\n\nUse Amazon Comprehend Moderation with a configuration to control what moderations you wish to perform and what actions should be taken for each of them. There are three different moderations that happen when no configuration is passed as demonstrated above. These moderations are:\n\nPII (Personally Identifiable Information) checks\nToxicity content detection\nPrompt Safety detection\n\nHere is an example of a moderation config.\n\nfrom langchain_experimental.comprehend_moderation import (\n    BaseModerationConfig,\n    ModerationPiiConfig,\n    ModerationPromptSafetyConfig,\n    ModerationToxicityConfig,\n)\n\npii_config = ModerationPiiConfig(labels=[\"SSN\"], redact=True, mask_character=\"X\")\n\ntoxicity_config = ModerationToxicityConfig(threshold=0.5)\n\nprompt_safety_config = ModerationPromptSafetyConfig(threshold=0.5)\n\nmoderation_config = BaseModerationConfig(\n    filters=[pii_config, toxicity_config, prompt_safety_config]\n)\n\n\nAt the core of the the configuration there are three configuration models to be used\n\nModerationPiiConfig used for configuring the behavior of the PII validations. Following are the parameters it can be initialized with\nlabels the PII entity labels. Defaults to an empty list which means that the PII validation will consider all PII entities.\nthreshold the confidence threshold for the detected entities, defaults to 0.5 or 50%\nredact a boolean flag to enforce whether redaction should be performed on the text, defaults to False. When False, the PII validation will error out when it detects any PII entity, when set to True it simply redacts the PII values in the text.\nmask_character the character used for masking, defaults to asterisk (*)\nModerationToxicityConfig used for configuring the behavior of the toxicity validations. Following are the parameters it can be initialized with\nlabels the Toxic entity labels. Defaults to an empty list which means that the toxicity validation will consider all toxic entities. all\nthreshold the confidence threshold for the detected entities, defaults to 0.5 or 50%\nModerationPromptSafetyConfig used for configuring the behavior of the prompt safety validation\nthreshold the confidence threshold for the the prompt safety classification, defaults to 0.5 or 50%\n\nFinally, you use the BaseModerationConfig to define the order in which each of these checks are to be performed. The BaseModerationConfig takes an optional filters parameter which can be a list of one or more than one of the above validation checks, as seen in the previous code block. The BaseModerationConfig can also be initialized with any filters in which case it will use all the checks with default configuration (more on this explained later).\n\nUsing the configuration in the previous cell will perform PII checks and will allow the prompt to pass through however it will mask any SSN numbers present in either the prompt or the LLM output.\n\ncomp_moderation_with_config = AmazonComprehendModerationChain(\n    moderation_config=moderation_config,  # specify the configuration\n    client=comprehend_client,  # optionally pass the Boto3 Client\n    verbose=True,\n)\n\nfrom langchain.llms.fake import FakeListLLM\nfrom langchain.prompts import PromptTemplate\n\ntemplate = \"\"\"Question: {question}\n\nAnswer:\"\"\"\n\nprompt = PromptTemplate(template=template, input_variables=[\"question\"])\n\nresponses = [\n    \"Final Answer: A credit card number looks like 1289-2321-1123-2387. A fake SSN number looks like 323-22-9980. John Doe's phone number is (999)253-9876.\",\n    # replace with your own expletive\n    \"Final Answer: This is a really <expletive> way of constructing a birdhouse. This is <expletive> insane to think that any birds would actually create their <expletive> nests here.\",\n]\nllm = FakeListLLM(responses=responses)\n\nchain = (\n    prompt\n    | comp_moderation_with_config\n    | {\"input\": (lambda x: x[\"output\"]) | llm}\n    | comp_moderation_with_config\n)\n\n\ntry:\n    response = chain.invoke(\n        {\n            \"question\": \"A sample SSN number looks like this 123-45-7890. Can you give me some more samples?\"\n        }\n    )\nexcept Exception as e:\n    print(str(e))\nelse:\n    print(response[\"output\"])\n\nUnique ID, and Moderation Callbacks‚Äã\n\nWhen Amazon Comprehend moderation action identifies any of the configugred entity, the chain will raise one of the following exceptions-\n\n- `ModerationPiiError`, for PII checks\n- `ModerationToxicityError`, for Toxicity checks \n- `ModerationPromptSafetyError` for Prompt Safety checks\n\n\nIn addition to the moderation configuration, the AmazonComprehendModerationChain can also be initialized with the following parameters\n\nunique_id [Optional] a string parameter. This parameter can be used to pass any string value or ID. For example, in a chat application, you may want to keep track of abusive users, in this case, you can pass the user's username/email ID etc. This defaults to None.\n\nmoderation_callback [Optional] the BaseModerationCallbackHandler that will be called asynchronously (non-blocking to the chain). Callback functions are useful when you want to perform additional actions when the moderation functions are executed, for example logging into a database, or writing a log file. You can override three functions by subclassing BaseModerationCallbackHandler - on_after_pii(), on_after_toxicity(), and on_after_prompt_safety(). Note that all three functions must be async functions. These callback functions receive two arguments:\n\nmoderation_beacon a dictionary that will contain information about the moderation function, the full response from Amazon Comprehend model, a unique chain id, the moderation status, and the input string which was validated. The dictionary is of the following schema-\n\n{ \n    'moderation_chain_id': 'xxx-xxx-xxx', # Unique chain ID\n    'moderation_type': 'Toxicity' | 'PII' | 'PromptSafety', \n    'moderation_status': 'LABELS_FOUND' | 'LABELS_NOT_FOUND',\n    'moderation_input': 'A sample SSN number looks like this 123-456-7890. Can you give me some more samples?',\n    'moderation_output': {...} #Full Amazon Comprehend PII, Toxicity, or Prompt Safety Model Output\n}\n\n\nunique_id if passed to the AmazonComprehendModerationChain\n\nNOTE: moderation_callback is different from LangChain Chain Callbacks. You can still use LangChain Chain callbacks with AmazonComprehendModerationChain via the callbacks parameter. Example:\n\nfrom langchain.callbacks.stdout import StdOutCallbackHandler comp_moderation_with_config = AmazonComprehendModerationChain(verbose=True, callbacks=[StdOutCallbackHandler()])\n\nfrom langchain_experimental.comprehend_moderation import BaseModerationCallbackHandler\n\n# Define callback handlers by subclassing BaseModerationCallbackHandler\n\n\nclass MyModCallback(BaseModerationCallbackHandler):\n    async def on_after_pii(self, output_beacon, unique_id):\n        import json\n\n        moderation_type = output_beacon[\"moderation_type\"]\n        chain_id = output_beacon[\"moderation_chain_id\"]\n        with open(f\"output-{moderation_type}-{chain_id}.json\", \"w\") as file:\n            data = {\"beacon_data\": output_beacon, \"unique_id\": unique_id}\n            json.dump(data, file)\n\n    \"\"\"\n    async def on_after_toxicity(self, output_beacon, unique_id):\n        pass\n    \n    async def on_after_prompt_safety(self, output_beacon, unique_id):\n        pass\n    \"\"\"\n\n\nmy_callback = MyModCallback()\n\npii_config = ModerationPiiConfig(labels=[\"SSN\"], redact=True, mask_character=\"X\")\n\ntoxicity_config = ModerationToxicityConfig(threshold=0.5)\n\nmoderation_config = BaseModerationConfig(filters=[pii_config, toxicity_config])\n\ncomp_moderation_with_config = AmazonComprehendModerationChain(\n    moderation_config=moderation_config,  # specify the configuration\n    client=comprehend_client,  # optionally pass the Boto3 Client\n    unique_id=\"john.doe@email.com\",  # A unique ID\n    moderation_callback=my_callback,  # BaseModerationCallbackHandler\n    verbose=True,\n)\n\nfrom langchain.llms.fake import FakeListLLM\nfrom langchain.prompts import PromptTemplate\n\ntemplate = \"\"\"Question: {question}\n\nAnswer:\"\"\"\n\nprompt = PromptTemplate(template=template, input_variables=[\"question\"])\n\nresponses = [\n    \"Final Answer: A credit card number looks like 1289-2321-1123-2387. A fake SSN number looks like 323-22-9980. John Doe's phone number is (999)253-9876.\",\n    # replace with your own expletive\n    \"Final Answer: This is a really <expletive> way of constructing a birdhouse. This is <expletive> insane to think that any birds would actually create their <expletive> nests here.\",\n]\n\nllm = FakeListLLM(responses=responses)\n\nchain = (\n    prompt\n    | comp_moderation_with_config\n    | {\"input\": (lambda x: x[\"output\"]) | llm}\n    | comp_moderation_with_config\n)\n\ntry:\n    response = chain.invoke(\n        {\n            \"question\": \"A sample SSN number looks like this 123-456-7890. Can you give me some more samples?\"\n        }\n    )\nexcept Exception as e:\n    print(str(e))\nelse:\n    print(response[\"output\"])\n\nmoderation_config and moderation execution order‚Äã\n\nIf AmazonComprehendModerationChain is not initialized with any moderation_config then it is initialized with the default values of BaseModerationConfig. If no filters are used then the sequence of moderation check is as follows.\n\nAmazonComprehendModerationChain\n‚îÇ\n‚îî‚îÄ‚îÄCheck PII with Stop Action\n    ‚îú‚îÄ‚îÄ Callback (if available)\n    ‚îú‚îÄ‚îÄ Label Found ‚ü∂ [Error Stop]\n    ‚îî‚îÄ‚îÄ No Label Found \n        ‚îî‚îÄ‚îÄCheck Toxicity with Stop Action\n            ‚îú‚îÄ‚îÄ Callback (if available)\n            ‚îú‚îÄ‚îÄ Label Found ‚ü∂ [Error Stop]\n            ‚îî‚îÄ‚îÄ No Label Found\n                ‚îî‚îÄ‚îÄCheck Prompt Safety with Stop Action\n                    ‚îú‚îÄ‚îÄ Callback (if available)\n                    ‚îú‚îÄ‚îÄ Label Found ‚ü∂ [Error Stop]\n                    ‚îî‚îÄ‚îÄ No Label Found\n                        ‚îî‚îÄ‚îÄ Return Prompt\n\n\nIf any of the check raises a validation exception then the subsequent checks will not be performed. If a callback is provided in this case, then it will be called for each of the checks that have been performed. For example, in the case above, if the Chain fails due to presence of PII then the Toxicity and Prompt Safety checks will not be performed.\n\nYou can override the execution order by passing moderation_config and simply specifying the desired order in the filters parameter of the BaseModerationConfig. In case you specify the filters, then the order of the checks as specified in the filters parameter will be maintained. For example, in the configuration below, first Toxicity check will be performed, then PII, and finally Prompt Safety validation will be performed. In this case, AmazonComprehendModerationChain will perform the desired checks in the specified order with default values of each model kwargs.\n\npii_check = ModerationPiiConfig()\ntoxicity_check = ModerationToxicityConfig()\nprompt_safety_check = ModerationPromptSafetyConfig()\n\nmoderation_config = BaseModerationConfig(filters=[toxicity_check, pii_check, prompt_safety_check])\n\n\nYou can have also use more than one configuration for a specific moderation check, for example in the sample below, two consecutive PII checks are performed. First the configuration checks for any SSN, if found it would raise an error. If any SSN isn't found then it will next check if any NAME and CREDIT_DEBIT_NUMBER is present in the prompt and will mask it.\n\npii_check_1 = ModerationPiiConfig(labels=[\"SSN\"])\npii_check_2 = ModerationPiiConfig(labels=[\"NAME\", \"CREDIT_DEBIT_NUMBER\"], redact=True)\n\nmoderation_config = BaseModerationConfig(filters=[pii_check_1, pii_check_2])\n\nFor a list of PII labels see Amazon Comprehend Universal PII entity types - https://docs.aws.amazon.com/comprehend/latest/dg/how-pii.html#how-pii-types\nFollowing are the list of available Toxicity labels-\nHATE_SPEECH: Speech that criticizes, insults, denounces or dehumanizes a person or a group on the basis of an identity, be it race, ethnicity, gender identity, religion, sexual orientation, ability, national origin, or another identity-group.\nGRAPHIC: Speech that uses visually descriptive, detailed and unpleasantly vivid imagery is considered as graphic. Such language is often made verbose so as to amplify an insult, discomfort or harm to the recipient.\nHARASSMENT_OR_ABUSE: Speech that imposes disruptive power dynamics between the speaker and hearer, regardless of intent, seeks to affect the psychological well-being of the recipient, or objectifies a person should be classified as Harassment.\nSEXUAL: Speech that indicates sexual interest, activity or arousal by using direct or indirect references to body parts or physical traits or sex is considered as toxic with toxicityType \"sexual\".\nVIOLENCE_OR_THREAT: Speech that includes threats which seek to inflict pain, injury or hostility towards a person or group.\nINSULT: Speech that includes demeaning, humiliating, mocking, insulting, or belittling language.\nPROFANITY: Speech that contains words, phrases or acronyms that are impolite, vulgar, or offensive is considered as profane.\nFor a list of Prompt Safety labels refer to documentation [link here]\nExamples‚Äã\nWith Hugging Face Hub Models‚Äã\n\nGet your API Key from Hugging Face hub\n\n%pip install huggingface_hub\n\nimport os\n\nos.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"<YOUR HF TOKEN HERE>\"\n\n# See https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads for some other options\nrepo_id = \"google/flan-t5-xxl\"\n\nfrom langchain.llms import HuggingFaceHub\nfrom langchain.prompts import PromptTemplate\n\ntemplate = \"\"\"{question}\"\"\"\n\nprompt = PromptTemplate(template=template, input_variables=[\"question\"])\nllm = HuggingFaceHub(\n    repo_id=repo_id, model_kwargs={\"temperature\": 0.5, \"max_length\": 256}\n)\n\n\nCreate a configuration and initialize an Amazon Comprehend Moderation chain\n\n# define filter configs\npii_config = ModerationPiiConfig(\n    labels=[\"SSN\", \"CREDIT_DEBIT_NUMBER\"], redact=True, mask_character=\"X\"\n)\n\ntoxicity_config = ModerationToxicityConfig(threshold=0.5)\n\nprompt_safety_config = ModerationPromptSafetyConfig(threshold=0.8)\n\n# define different moderation configs using the filter configs above\nmoderation_config_1 = BaseModerationConfig(\n    filters=[pii_config, toxicity_config, prompt_safety_config]\n)\n\nmoderation_config_2 = BaseModerationConfig(filters=[pii_config])\n\n\n# input prompt moderation chain with callback\namazon_comp_moderation = AmazonComprehendModerationChain(\n    moderation_config=moderation_config_1,\n    client=comprehend_client,\n    moderation_callback=my_callback,\n    verbose=True,\n)\n\n# Output from LLM moderation chain without callback\namazon_comp_moderation_out = AmazonComprehendModerationChain(\n    moderation_config=moderation_config_2, client=comprehend_client, verbose=True\n)\n\n\nThe moderation_config will now prevent any inputs containing obscene words or sentences, bad intent, or PII with entities other than SSN with score above threshold or 0.5 or 50%. If it finds Pii entities - SSN - it will redact them before allowing the call to proceed. It will also mask any SSN or credit card numbers from the model's response.\n\nchain = (\n    prompt\n    | amazon_comp_moderation\n    | {\"input\": (lambda x: x[\"output\"]) | llm}\n    | amazon_comp_moderation_out\n)\n\ntry:\n    response = chain.invoke(\n        {\n            \"question\": \"\"\"What is John Doe's address, phone number and SSN from the following text?\n\nJohn Doe, a resident of 1234 Elm Street in Springfield, recently celebrated his birthday on January 1st. Turning 43 this year, John reflected on the years gone by. He often shares memories of his younger days with his close friends through calls on his phone, (555) 123-4567. Meanwhile, during a casual evening, he received an email at johndoe@example.com reminding him of an old acquaintance's reunion. As he navigated through some old documents, he stumbled upon a paper that listed his SSN as 123-45-6789, reminding him to store it in a safer place.\n\"\"\"\n        }\n    )\nexcept Exception as e:\n    print(str(e))\nelse:\n    print(response[\"output\"])\n\nWith Amazon SageMaker Jumpstart‚Äã\n\nThe exmaple below shows how to use Amazon Comprehend Moderation chain with an Amazon SageMaker Jumpstart hosted LLM. You should have an Amazon SageMaker Jumpstart hosted LLM endpoint within your AWS Account. Refer to this notebook for more on how to deploy an LLM with Amazon SageMaker Jumpstart hosted endpoints.\n\nendpoint_name = \"<SAGEMAKER_ENDPOINT_NAME>\"  # replace with your SageMaker Endpoint name\nregion = \"<REGION>\"  # replace with your SageMaker Endpoint region\n\nimport json\n\nfrom langchain.llms import SagemakerEndpoint\nfrom langchain.llms.sagemaker_endpoint import LLMContentHandler\nfrom langchain.prompts import PromptTemplate\n\n\nclass ContentHandler(LLMContentHandler):\n    content_type = \"application/json\"\n    accepts = \"application/json\"\n\n    def transform_input(self, prompt: str, model_kwargs: dict) -> bytes:\n        input_str = json.dumps({\"text_inputs\": prompt, **model_kwargs})\n        return input_str.encode(\"utf-8\")\n\n    def transform_output(self, output: bytes) -> str:\n        response_json = json.loads(output.read().decode(\"utf-8\"))\n        return response_json[\"generated_texts\"][0]\n\n\ncontent_handler = ContentHandler()\n\ntemplate = \"\"\"From the following 'Document', precisely answer the 'Question'. Do not add any spurious information in your answer.\n\nDocument: John Doe, a resident of 1234 Elm Street in Springfield, recently celebrated his birthday on January 1st. Turning 43 this year, John reflected on the years gone by. He often shares memories of his younger days with his close friends through calls on his phone, (555) 123-4567. Meanwhile, during a casual evening, he received an email at johndoe@example.com reminding him of an old acquaintance's reunion. As he navigated through some old documents, he stumbled upon a paper that listed his SSN as 123-45-6789, reminding him to store it in a safer place.\nQuestion: {question}\nAnswer:\n\"\"\"\n\n# prompt template for input text\nllm_prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n\nllm = SagemakerEndpoint(\n    endpoint_name=endpoint_name,\n    region_name=region,\n    model_kwargs={\n        \"temperature\": 0.95,\n        \"max_length\": 200,\n        \"num_return_sequences\": 3,\n        \"top_k\": 50,\n        \"top_p\": 0.95,\n        \"do_sample\": True,\n    },\n    content_handler=content_handler,\n)\n\n\nCreate a configuration and initialize an Amazon Comprehend Moderation chain\n\n# define filter configs\npii_config = ModerationPiiConfig(labels=[\"SSN\"], redact=True, mask_character=\"X\")\n\ntoxicity_config = ModerationToxicityConfig(threshold=0.5)\n\n\n# define different moderation configs using the filter configs above\nmoderation_config_1 = BaseModerationConfig(filters=[pii_config, toxicity_config])\n\nmoderation_config_2 = BaseModerationConfig(filters=[pii_config])\n\n\n# input prompt moderation chain with callback\namazon_comp_moderation = AmazonComprehendModerationChain(\n    moderation_config=moderation_config_1,\n    client=comprehend_client,\n    moderation_callback=my_callback,\n    verbose=True,\n)\n\n# Output from LLM moderation chain without callback\namazon_comp_moderation_out = AmazonComprehendModerationChain(\n    moderation_config=moderation_config_2, client=comprehend_client, verbose=True\n)\n\n\nThe moderation_config will now prevent any inputs and model outputs containing obscene words or sentences, bad intent, or Pii with entities other than SSN with score above threshold or 0.5 or 50%. If it finds Pii entities - SSN - it will redact them before allowing the call to proceed.\n\nchain = (\n    prompt\n    | amazon_comp_moderation\n    | {\"input\": (lambda x: x[\"output\"]) | llm}\n    | amazon_comp_moderation_out\n)\n\ntry:\n    response = chain.invoke(\n        {\"question\": \"What is John Doe's address, phone number and SSN?\"}\n    )\nexcept Exception as e:\n    print(str(e))\nelse:\n    print(response[\"output\"])\n\nPrevious\nSafety\nNext\nConstitutional chain"
}