{
	"title": "Chat Bot Feedback Template | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/templates/chat-bot-feedback",
	"html": "TemplatesChat Bot Feedback Template\nChat Bot Feedback Template\n\nThis template shows how to evaluate your chat bot without explicit user feedback. It defines a simple chat bot in chain.py and custom evaluator that scores bot response effectiveness based on the subsequent user response. You can apply this run evaluator to your own chat bot by calling with_config on the chat bot before serving. You can also directly deploy your chat app using this template.\n\nChat bots are one of the most common interfaces for deploying LLMs. The quality of chat bots varies, making continuous development important. But users are wont to leave explicit feedback through mechanisms like thumbs-up or thumbs-down buttons. Furthermore, traditional analytics such as \"session length\" or \"conversation length\" often lack clarity. However, multi-turn conversations with a chat bot can provide a wealth of information, which we can transform into metrics for fine-tuning, evaluation, and product analytics.\n\nTaking Chat Langchain as a case study, only about 0.04% of all queries receive explicit feedback. Yet, approximately 70% of the queries are follow-ups to previous questions. A significant portion of these follow-up queries continue useful information we can use to infer the quality of the previous AI response.\n\nThis template helps solve this \"feedback scarcity\" problem. Below is an example invocation of this chat bot:\n\nWhen the user responds to this (link), the response evaluator is invoked, resulting in the following evaluationrun:\n\nAs shown, the evaluator sees that the user is increasingly frustrated, indicating that the prior response was not effective\n\nLangSmith Feedback‚Äã\n\nLangSmith is a platform for building production-grade LLM applications. Beyond its debugging and offline evaluation features, LangSmith helps you capture both user and model-assisted feedback to refine your LLM application. This template uses an LLM to generate feedback for your application, which you can use to continuously improve your service. For more examples on collecting feedback using LangSmith, consult the documentation.\n\nEvaluator Implementation‚Äã\n\nThe user feedback is inferred by custom RunEvaluator. This evaluator is called using the EvaluatorCallbackHandler, which run it in a separate thread to avoid interfering with the chat bot's runtime. You can use this custom evaluator on any compatible chat bot by calling the following function on your LangChain object:\n\nmy_chain\n.with_config(\n        callbacks=[\n            EvaluatorCallbackHandler(\n                evaluators=[\n                    ResponseEffectivenessEvaluator(evaluate_response_effectiveness)\n                ]\n            )\n        ],\n    )\n\n\nThe evaluator instructs an LLM, specifically gpt-3.5-turbo, to evaluate the AI's most recent chat message based on the user's followup response. It generates a score and accompanying reasoning that is converted to feedback in LangSmith, applied to the value provided as the last_run_id.\n\nThe prompt used within the LLM is available on the hub. Feel free to customize it with things like additional app context (such as the goal of the app or the types of questions it should respond to) or \"symptoms\" you'd like the LLM to focus on. This evaluator also utilizes OpenAI's function-calling API to ensure a more consistent, structured output for the grade.\n\nEnvironment Variables‚Äã\n\nEnsure that OPENAI_API_KEY is set to use OpenAI models. Also, configure LangSmith by setting your LANGSMITH_API_KEY.\n\nexport OPENAI_API_KEY=sk-...\nexport LANGSMITH_API_KEY=...\nexport LANGCHAIN_TRACING_V2=true\nexport LANGCHAIN_PROJECT=my-project # Set to the project you want to save to\n\nUsage‚Äã\n\nIf deploying via LangServe, we recommend configuring the server to return callback events as well. This will ensure the backend traces are included in whatever traces you generate using the RemoteRunnable.\n\nfrom chat_bot_feedback.chain import chain\n\nadd_routes(app, chain, path=\"/chat-bot-feedback\", include_callback_events=True)\n\n\nWith the server running, you can use the following code snippet to stream the chat bot responses for a 2 turn conversation.\n\nfrom functools import partial\nfrom typing import Dict, Optional, Callable, List\nfrom langserve import RemoteRunnable\nfrom langchain.callbacks.manager import tracing_v2_enabled\nfrom langchain.schema import BaseMessage, AIMessage, HumanMessage\n\n# Update with the URL provided by your LangServe server\nchain = RemoteRunnable(\"http://127.0.0.1:8031/chat-bot-feedback\")\n\ndef stream_content(\n    text: str,\n    chat_history: Optional[List[BaseMessage]] = None,\n    last_run_id: Optional[str] = None,\n    on_chunk: Callable = None,\n):\n    results = []\n    with tracing_v2_enabled() as cb:\n        for chunk in chain.stream(\n            {\"text\": text, \"chat_history\": chat_history, \"last_run_id\": last_run_id},\n        ):\n            on_chunk(chunk)\n            results.append(chunk)\n        last_run_id = cb.latest_run.id if cb.latest_run else None\n    return last_run_id, \"\".join(results)\n\nchat_history = []\ntext = \"Where are my keys?\"\nlast_run_id, response_message = stream_content(text, on_chunk=partial(print, end=\"\"))\nprint()\nchat_history.extend([HumanMessage(content=text), AIMessage(content=response_message)])\ntext = \"I CAN'T FIND THEM ANYWHERE\"  # The previous response will likely receive a low score,\n# as the user's frustration appears to be escalating.\nlast_run_id, response_message = stream_content(\n    text,\n    chat_history=chat_history,\n    last_run_id=str(last_run_id),\n    on_chunk=partial(print, end=\"\"),\n)\nprint()\nchat_history.extend([HumanMessage(content=text), AIMessage(content=response_message)])\n\n\nThis uses the tracing_v2_enabled callback manager to get the run ID of the call, which we provide in subsequent calls in the same chat thread, so the evaluator can assign feedback to the appropriate trace.\n\nConclusion‚Äã\n\nThis template provides a simple chat bot definition you can directly deploy using LangServe. It defines a custom evaluator to log evaluation feedback for the bot without any explicit user ratings. This is an effective way to augment your analytics and to better select data points for fine-tuning and evaluation.\n\nPrevious\nChain-of-Note (Wikipedia)\nNext\ncsv-agent"
}