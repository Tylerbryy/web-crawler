{
	"title": "Vectara | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/integrations/providers/vectara/",
	"html": "ProvidersMoreVectara\nVectara\n\nVectara is a GenAI platform for developers. It provides a simple API to build Grounded Generation (aka Retrieval-augmented-generation or RAG) applications.\n\nVectara Overview:\n\nVectara is developer-first API platform for building GenAI applications\nTo use Vectara - first sign up and create an account. Then create a corpus and an API key for indexing and searching.\nYou can use Vectara's indexing API to add documents into Vectara's index\nYou can use Vectara's Search API to query Vectara's index (which also supports Hybrid search implicitly).\nYou can use Vectara's integration with LangChain as a Vector store or using the Retriever abstraction.\nInstallation and Setup‚Äã\n\nTo use Vectara with LangChain no special installation steps are required. To get started, sign up and follow our quickstart guide to create a corpus and an API key. Once you have these, you can provide them as arguments to the Vectara vectorstore, or you can set them as environment variables.\n\nexport VECTARA_CUSTOMER_ID=\"your_customer_id\"\nexport VECTARA_CORPUS_ID=\"your_corpus_id\"\nexport VECTARA_API_KEY=\"your-vectara-api-key\"\nVector Store‚Äã\n\nThere exists a wrapper around the Vectara platform, allowing you to use it as a vectorstore, whether for semantic search or example selection.\n\nTo import this vectorstore:\n\nfrom langchain.vectorstores import Vectara\n\n\nTo create an instance of the Vectara vectorstore:\n\nvectara = Vectara(\n    vectara_customer_id=customer_id, \n    vectara_corpus_id=corpus_id, \n    vectara_api_key=api_key\n)\n\n\nThe customer_id, corpus_id and api_key are optional, and if they are not supplied will be read from the environment variables VECTARA_CUSTOMER_ID, VECTARA_CORPUS_ID and VECTARA_API_KEY, respectively.\n\nAfter you have the vectorstore, you can add_texts or add_documents as per the standard VectorStore interface, for example:\n\nvectara.add_texts([\"to be or not to be\", \"that is the question\"])\n\n\nSince Vectara supports file-upload, we also added the ability to upload files (PDF, TXT, HTML, PPT, DOC, etc) directly as file. When using this method, the file is uploaded directly to the Vectara backend, processed and chunked optimally there, so you don't have to use the LangChain document loader or chunking mechanism.\n\nAs an example:\n\nvectara.add_files([\"path/to/file1.pdf\", \"path/to/file2.pdf\",...])\n\n\nTo query the vectorstore, you can use the similarity_search method (or similarity_search_with_score), which takes a query string and returns a list of results:\n\nresults = vectara.similarity_score(\"what is LangChain?\")\n\n\nsimilarity_search_with_score also supports the following additional arguments:\n\nk: number of results to return (defaults to 5)\nlambda_val: the lexical matching factor for hybrid search (defaults to 0.025)\nfilter: a filter to apply to the results (default None)\nn_sentence_context: number of sentences to include before/after the actual matching segment when returning results. This defaults to 2.\n\nThe results are returned as a list of relevant documents, and a relevance score of each document.\n\nFor a more detailed examples of using the Vectara wrapper, see one of these two sample notebooks:\n\nChat Over Documents with Vectara\nVectara Text Generation\nPrevious\nVearch\nNext\nChat Over Documents with Vectara"
}