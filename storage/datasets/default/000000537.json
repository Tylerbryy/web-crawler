{
	"title": "Databricks | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/integrations/llms/databricks",
	"html": "ComponentsLLMsDatabricks\nDatabricks\n\nThe Databricks Lakehouse Platform unifies data, analytics, and AI on one platform.\n\nThis example notebook shows how to wrap Databricks endpoints as LLMs in LangChain. It supports two endpoint types:\n\nServing endpoint, recommended for production and development,\nCluster driver proxy app, recommended for iteractive development.\nfrom langchain.llms import Databricks\n\nWrapping a serving endpoint‚Äã\n\nPrerequisites:\n\nAn LLM was registered and deployed to a Databricks serving endpoint.\nYou have \"Can Query\" permission to the endpoint.\n\nThe expected MLflow model signature is:\n\ninputs: [{\"name\": \"prompt\", \"type\": \"string\"}, {\"name\": \"stop\", \"type\": \"list[string]\"}]\noutputs: [{\"type\": \"string\"}]\n\nIf the model signature is incompatible or you want to insert extra configs, you can set transform_input_fn and transform_output_fn accordingly.\n\n# If running a Databricks notebook attached to an interactive cluster in \"single user\"\n# or \"no isolation shared\" mode, you only need to specify the endpoint name to create\n# a `Databricks` instance to query a serving endpoint in the same workspace.\nllm = Databricks(endpoint_name=\"dolly\")\n\nllm(\"How are you?\")\n\n    'I am happy to hear that you are in good health and as always, you are appreciated.'\n\nllm(\"How are you?\", stop=[\".\"])\n\n    'Good'\n\n# Otherwise, you can manually specify the Databricks workspace hostname and personal access token\n# or set `DATABRICKS_HOST` and `DATABRICKS_TOKEN` environment variables, respectively.\n# See https://docs.databricks.com/dev-tools/auth.html#databricks-personal-access-tokens\n# We strongly recommend not exposing the API token explicitly inside a notebook.\n# You can use Databricks secret manager to store your API token securely.\n# See https://docs.databricks.com/dev-tools/databricks-utils.html#secrets-utility-dbutilssecrets\n\nimport os\n\nos.environ[\"DATABRICKS_TOKEN\"] = dbutils.secrets.get(\"myworkspace\", \"api_token\")\n\nllm = Databricks(host=\"myworkspace.cloud.databricks.com\", endpoint_name=\"dolly\")\n\nllm(\"How are you?\")\n\n    'I am fine. Thank you!'\n\n# If the serving endpoint accepts extra parameters like `temperature`,\n# you can set them in `model_kwargs`.\nllm = Databricks(endpoint_name=\"dolly\", model_kwargs={\"temperature\": 0.1})\n\nllm(\"How are you?\")\n\n    'I am fine.'\n\n# Use `transform_input_fn` and `transform_output_fn` if the serving endpoint\n# expects a different input schema and does not return a JSON string,\n# respectively, or you want to apply a prompt template on top.\n\n\ndef transform_input(**request):\n    full_prompt = f\"\"\"{request[\"prompt\"]}\n    Be Concise.\n    \"\"\"\n    request[\"prompt\"] = full_prompt\n    return request\n\n\nllm = Databricks(endpoint_name=\"dolly\", transform_input_fn=transform_input)\n\nllm(\"How are you?\")\n\n    'I‚Äôm Excellent. You?'\n\nWrapping a cluster driver proxy app‚Äã\n\nPrerequisites:\n\nAn LLM loaded on a Databricks interactive cluster in \"single user\" or \"no isolation shared\" mode.\nA local HTTP server running on the driver node to serve the model at \"/\" using HTTP POST with JSON input/output.\nIt uses a port number between [3000, 8000] and listens to the driver IP address or simply 0.0.0.0 instead of localhost only.\nYou have \"Can Attach To\" permission to the cluster.\n\nThe expected server schema (using JSON schema) is:\n\ninputs:\n{\"type\": \"object\",\n \"properties\": {\n    \"prompt\": {\"type\": \"string\"},\n     \"stop\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}},\n  \"required\": [\"prompt\"]}\n\noutputs: {\"type\": \"string\"}\n\nIf the server schema is incompatible or you want to insert extra configs, you can use transform_input_fn and transform_output_fn accordingly.\n\nThe following is a minimal example for running a driver proxy app to serve an LLM:\n\nfrom flask import Flask, request, jsonify\nimport torch\nfrom transformers import pipeline, AutoTokenizer, StoppingCriteria\n\nmodel = \"databricks/dolly-v2-3b\"\ntokenizer = AutoTokenizer.from_pretrained(model, padding_side=\"left\")\ndolly = pipeline(model=model, tokenizer=tokenizer, trust_remote_code=True, device_map=\"auto\")\ndevice = dolly.device\n\nclass CheckStop(StoppingCriteria):\n    def __init__(self, stop=None):\n        super().__init__()\n        self.stop = stop or []\n        self.matched = \"\"\n        self.stop_ids = [tokenizer.encode(s, return_tensors='pt').to(device) for s in self.stop]\n    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs):\n        for i, s in enumerate(self.stop_ids):\n            if torch.all((s == input_ids[0][-s.shape[1]:])).item():\n                self.matched = self.stop[i]\n                return True\n        return False\n\ndef llm(prompt, stop=None, **kwargs):\n  check_stop = CheckStop(stop)\n  result = dolly(prompt, stopping_criteria=[check_stop], **kwargs)\n  return result[0][\"generated_text\"].rstrip(check_stop.matched)\n\napp = Flask(\"dolly\")\n\n@app.route('/', methods=['POST'])\ndef serve_llm():\n  resp = llm(**request.json)\n  return jsonify(resp)\n\napp.run(host=\"0.0.0.0\", port=\"7777\")\n\n\nOnce the server is running, you can create a Databricks instance to wrap it as an LLM.\n\n# If running a Databricks notebook attached to the same cluster that runs the app,\n# you only need to specify the driver port to create a `Databricks` instance.\nllm = Databricks(cluster_driver_port=\"7777\")\n\nllm(\"How are you?\")\n\n    'Hello, thank you for asking. It is wonderful to hear that you are well.'\n\n# Otherwise, you can manually specify the cluster ID to use,\n# as well as Databricks workspace hostname and personal access token.\n\nllm = Databricks(cluster_id=\"0000-000000-xxxxxxxx\", cluster_driver_port=\"7777\")\n\nllm(\"How are you?\")\n\n    'I am well. You?'\n\n# If the app accepts extra parameters like `temperature`,\n# you can set them in `model_kwargs`.\nllm = Databricks(cluster_driver_port=\"7777\", model_kwargs={\"temperature\": 0.1})\n\nllm(\"How are you?\")\n\n    'I am very well. It is a pleasure to meet you.'\n\n# Use `transform_input_fn` and `transform_output_fn` if the app\n# expects a different input schema and does not return a JSON string,\n# respectively, or you want to apply a prompt template on top.\n\n\ndef transform_input(**request):\n    full_prompt = f\"\"\"{request[\"prompt\"]}\n    Be Concise.\n    \"\"\"\n    request[\"prompt\"] = full_prompt\n    return request\n\n\ndef transform_output(response):\n    return response.upper()\n\n\nllm = Databricks(\n    cluster_driver_port=\"7777\",\n    transform_input_fn=transform_input,\n    transform_output_fn=transform_output,\n)\n\nllm(\"How are you?\")\n\n    'I AM DOING GREAT THANK YOU.'\n\nPrevious\nCTranslate2\nNext\nDeepInfra"
}