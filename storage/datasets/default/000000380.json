{
	"title": "Prompt templates | 🦜️🔗 Langchain",
	"url": "https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/",
	"html": "ModulesModel I/OPromptsPrompt templates\nPrompt templates\n\nPrompt templates are pre-defined recipes for generating prompts for language models.\n\nA template may include instructions, few-shot examples, and specific context and questions appropriate for a given task.\n\nLangChain provides tooling to create and work with prompt templates.\n\nLangChain strives to create model agnostic templates to make it easy to reuse existing templates across different language models.\n\nTypically, language models expect the prompt to either be a string or else a list of chat messages.\n\nPromptTemplate​\n\nUse PromptTemplate to create a template for a string prompt.\n\nBy default, PromptTemplate uses Python's str.format syntax for templating.\n\nfrom langchain.prompts import PromptTemplate\n\nprompt_template = PromptTemplate.from_template(\n    \"Tell me a {adjective} joke about {content}.\"\n)\nprompt_template.format(adjective=\"funny\", content=\"chickens\")\n\n    'Tell me a funny joke about chickens.'\n\n\nThe template supports any number of variables, including no variables:\n\nfrom langchain.prompts import PromptTemplate\n\nprompt_template = PromptTemplate.from_template(\"Tell me a joke\")\nprompt_template.format()\n\n    'Tell me a joke'\n\n\nFor additional validation, specify input_variables explicitly. These variables will be compared against the variables present in the template string during instantiation, raising an exception if there is a mismatch. For example:\n\nfrom langchain.prompts import PromptTemplate\n\ninvalid_prompt = PromptTemplate(\n    input_variables=[\"adjective\"],\n    template=\"Tell me a {adjective} joke about {content}.\",\n)\n\n    ---------------------------------------------------------------------------\n\n    ValidationError                           Traceback (most recent call last)\n\n    Cell In[19], line 3\n          1 from langchain.prompts import PromptTemplate\n    ----> 3 invalid_prompt = PromptTemplate(\n          4     input_variables=[\"adjective\"],\n          5     template=\"Tell me a {adjective} joke about {content}.\"\n          6 )\n\n\n    File ~/langchain/libs/langchain/langchain/load/serializable.py:97, in Serializable.__init__(self, **kwargs)\n         96 def __init__(self, **kwargs: Any) -> None:\n    ---> 97     super().__init__(**kwargs)\n         98     self._lc_kwargs = kwargs\n\n\n    File ~/langchain/.venv/lib/python3.9/site-packages/pydantic/main.py:341, in pydantic.main.BaseModel.__init__()\n\n\n    ValidationError: 1 validation error for PromptTemplate\n    __root__\n      Invalid prompt schema; check for mismatched or missing input parameters. 'content' (type=value_error)\n\n\nYou can create custom prompt templates that format the prompt in any way you want. For more information, see Custom Prompt Templates.\n\nChatPromptTemplate​\n\nThe prompt to chat models is a list of chat messages.\n\nEach chat message is associated with content, and an additional parameter called role. For example, in the OpenAI Chat Completions API, a chat message can be associated with an AI assistant, a human or a system role.\n\nCreate a chat prompt template like this:\n\nfrom langchain.prompts import ChatPromptTemplate\n\nchat_template = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n        (\"human\", \"Hello, how are you doing?\"),\n        (\"ai\", \"I'm doing well, thanks!\"),\n        (\"human\", \"{user_input}\"),\n    ]\n)\n\nmessages = chat_template.format_messages(name=\"Bob\", user_input=\"What is your name?\")\n\n\nChatPromptTemplate.from_messages accepts a variety of message representations.\n\nFor example, in addition to using the 2-tuple representation of (type, content) used above, you could pass in an instance of MessagePromptTemplate or BaseMessage.\n\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.prompts import HumanMessagePromptTemplate\nfrom langchain.schema.messages import SystemMessage\n\nchat_template = ChatPromptTemplate.from_messages(\n    [\n        SystemMessage(\n            content=(\n                \"You are a helpful assistant that re-writes the user's text to \"\n                \"sound more upbeat.\"\n            )\n        ),\n        HumanMessagePromptTemplate.from_template(\"{text}\"),\n    ]\n)\n\nllm = ChatOpenAI()\nllm(chat_template.format_messages(text=\"i dont like eating tasty things.\"))\n\n    AIMessage(content='I absolutely love indulging in delicious treats!')\n\n\nThis provides you with a lot of flexibility in how you construct your chat prompts.\n\nLCEL​\n\nPromptTemplate and ChatPromptTemplate implement the Runnable interface, the basic building block of the LangChain Expression Language (LCEL). This means they support invoke, ainvoke, stream, astream, batch, abatch, astream_log calls.\n\nPromptTemplate accepts a dictionary (of the prompt variables) and returns a StringPromptValue. A ChatPromptTemplate accepts a dictionary and returns a ChatPromptValue.\n\nprompt_val = prompt_template.invoke({\"adjective\": \"funny\", \"content\": \"chickens\"})\nprompt_val\n\n    StringPromptValue(text='Tell me a joke')\n\nprompt_val.to_string()\n\n    'Tell me a joke'\n\nprompt_val.to_messages()\n\n    [HumanMessage(content='Tell me a joke')]\n\nchat_val = chat_template.invoke({\"text\": \"i dont like eating tasty things.\"})\n\nchat_val.to_messages()\n\n    [SystemMessage(content=\"You are a helpful assistant that re-writes the user's text to sound more upbeat.\"),\n     HumanMessage(content='i dont like eating tasty things.')]\n\nchat_val.to_string()\n\n    \"System: You are a helpful assistant that re-writes the user's text to sound more upbeat.\\nHuman: i dont like eating tasty things.\"\n\nPrevious\nPrompts\nNext\nConnecting to a Feature Store"
}