{
	"title": "Google Speech-to-Text Audio Transcripts | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/integrations/document_loaders/google_speech_to_text",
	"html": "ComponentsDocument loadersGoogle Speech-to-Text Audio Transcripts\nGoogle Speech-to-Text Audio Transcripts\n\nThe GoogleSpeechToTextLoader allows to transcribe audio files with the Google Cloud Speech-to-Text API and loads the transcribed text into documents.\n\nTo use it, you should have the google-cloud-speech python package installed, and a Google Cloud project with the Speech-to-Text API enabled.\n\nBringing the power of large models to Google Cloud‚Äôs Speech API\nInstallation & setup‚Äã\n\nFirst, you need to install the google-cloud-speech python package.\n\nYou can find more info about it on the Speech-to-Text client libraries page.\n\nFollow the quickstart guide in the Google Cloud documentation to create a project and enable the API.\n\n%pip install google-cloud-speech\n\nExample‚Äã\n\nThe GoogleSpeechToTextLoader must include the project_id and file_path arguments. Audio files can be specified as a Google Cloud Storage URI (gs://...) or a local file path.\n\nOnly synchronous requests are supported by the loader, which has a limit of 60 seconds or 10MB per audio file.\n\nfrom langchain.document_loaders import GoogleSpeechToTextLoader\n\nproject_id = \"<PROJECT_ID>\"\nfile_path = \"gs://cloud-samples-data/speech/audio.flac\"\n# or a local file path: file_path = \"./audio.wav\"\n\nloader = GoogleSpeechToTextLoader(project_id=project_id, file_path=file_path)\n\ndocs = loader.load()\n\n\nNote: Calling loader.load() blocks until the transcription is finished.\n\nThe transcribed text is available in the page_content:\n\ndocs[0].page_content\n\n\"How old is the Brooklyn Bridge?\"\n\n\nThe metadata contains the full JSON response with more meta information:\n\ndocs[0].metadata\n\n{\n  'language_code': 'en-US',\n  'result_end_offset': datetime.timedelta(seconds=1)\n}\n\nRecognition Config‚Äã\n\nYou can specify the config argument to use different speech recognition models and enable specific features.\n\nRefer to the Speech-to-Text recognizers documentation and the RecognizeRequest API reference for information on how to set a custom configuation.\n\nIf you don't specify a config, the following options will be selected automatically:\n\nModel: Chirp Universal Speech Model\nLanguage: en-US\nAudio Encoding: Automatically Detected\nAutomatic Punctuation: Enabled\nfrom google.cloud.speech_v2 import (\n    AutoDetectDecodingConfig,\n    RecognitionConfig,\n    RecognitionFeatures,\n)\nfrom langchain.document_loaders import GoogleSpeechToTextLoader\n\nproject_id = \"<PROJECT_ID>\"\nlocation = \"global\"\nrecognizer_id = \"<RECOGNIZER_ID>\"\nfile_path = \"./audio.wav\"\n\nconfig = RecognitionConfig(\n    auto_decoding_config=AutoDetectDecodingConfig(),\n    language_codes=[\"en-US\"],\n    model=\"long\",\n    features=RecognitionFeatures(\n        enable_automatic_punctuation=False,\n        profanity_filter=True,\n        enable_spoken_punctuation=True,\n        enable_spoken_emojis=True,\n    ),\n)\n\nloader = GoogleSpeechToTextLoader(\n    project_id=project_id,\n    location=location,\n    recognizer_id=recognizer_id,\n    file_path=file_path,\n    config=config,\n)\n\nPrevious\nGoogle Drive\nNext\nGrobid"
}