{
	"title": "Gradient | 🦜️🔗 Langchain",
	"url": "https://python.langchain.com/docs/integrations/llms/gradient",
	"html": "ComponentsLLMsGradient\nGradient\n\nGradient allows to fine tune and get completions on LLMs with a simple web API.\n\nThis notebook goes over how to use Langchain with Gradient.\n\nImports​\nfrom langchain.chains import LLMChain\nfrom langchain.llms import GradientLLM\nfrom langchain.prompts import PromptTemplate\n\nSet the Environment API Key​\n\nMake sure to get your API key from Gradient AI. You are given $10 in free credits to test and fine-tune different models.\n\nimport os\nfrom getpass import getpass\n\nif not os.environ.get(\"GRADIENT_ACCESS_TOKEN\", None):\n    # Access token under https://auth.gradient.ai/select-workspace\n    os.environ[\"GRADIENT_ACCESS_TOKEN\"] = getpass(\"gradient.ai access token:\")\nif not os.environ.get(\"GRADIENT_WORKSPACE_ID\", None):\n    # `ID` listed in `$ gradient workspace list`\n    # also displayed after login at at https://auth.gradient.ai/select-workspace\n    os.environ[\"GRADIENT_WORKSPACE_ID\"] = getpass(\"gradient.ai workspace id:\")\n\n\nOptional: Validate your Enviroment variables GRADIENT_ACCESS_TOKEN and GRADIENT_WORKSPACE_ID to get currently deployed models. Using the gradientai Python package.\n\npip install gradientai\n\n    Requirement already satisfied: gradientai in /home/michi/.venv/lib/python3.10/site-packages (1.0.0)\n    Requirement already satisfied: aenum>=3.1.11 in /home/michi/.venv/lib/python3.10/site-packages (from gradientai) (3.1.15)\n    Requirement already satisfied: pydantic<2.0.0,>=1.10.5 in /home/michi/.venv/lib/python3.10/site-packages (from gradientai) (1.10.12)\n    Requirement already satisfied: python-dateutil>=2.8.2 in /home/michi/.venv/lib/python3.10/site-packages (from gradientai) (2.8.2)\n    Requirement already satisfied: urllib3>=1.25.3 in /home/michi/.venv/lib/python3.10/site-packages (from gradientai) (1.26.16)\n    Requirement already satisfied: typing-extensions>=4.2.0 in /home/michi/.venv/lib/python3.10/site-packages (from pydantic<2.0.0,>=1.10.5->gradientai) (4.5.0)\n    Requirement already satisfied: six>=1.5 in /home/michi/.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->gradientai) (1.16.0)\n\nimport gradientai\n\nclient = gradientai.Gradient()\n\nmodels = client.list_models(only_base=True)\nfor model in models:\n    print(model.id)\n\n    99148c6d-c2a0-4fbe-a4a7-e7c05bdb8a09_base_ml_model\n    f0b97d96-51a8-4040-8b22-7940ee1fa24e_base_ml_model\n    cc2dafce-9e6e-4a23-a918-cad6ba89e42e_base_ml_model\n\nnew_model = models[-1].create_model_adapter(name=\"my_model_adapter\")\nnew_model.id, new_model.name\n\n    ('674119b5-f19e-4856-add2-767ae7f7d7ef_model_adapter', 'my_model_adapter')\n\nCreate the Gradient instance​\n\nYou can specify different parameters such as the model, max_tokens generated, temperature, etc.\n\nAs we later want to fine-tune out model, we select the model_adapter with the id 674119b5-f19e-4856-add2-767ae7f7d7ef_model_adapter, but you can use any base or fine-tunable model.\n\nllm = GradientLLM(\n    # `ID` listed in `$ gradient model list`\n    model=\"674119b5-f19e-4856-add2-767ae7f7d7ef_model_adapter\",\n    # # optional: set new credentials, they default to environment variables\n    # gradient_workspace_id=os.environ[\"GRADIENT_WORKSPACE_ID\"],\n    # gradient_access_token=os.environ[\"GRADIENT_ACCESS_TOKEN\"],\n    model_kwargs=dict(max_generated_token_count=128),\n)\n\nCreate a Prompt Template​\n\nWe will create a prompt template for Question and Answer.\n\ntemplate = \"\"\"Question: {question}\n\nAnswer: \"\"\"\n\nprompt = PromptTemplate(template=template, input_variables=[\"question\"])\n\nInitiate the LLMChain​\nllm_chain = LLMChain(prompt=prompt, llm=llm)\n\nRun the LLMChain​\n\nProvide a question and run the LLMChain.\n\nquestion = \"What NFL team won the Super Bowl in 1994?\"\n\nllm_chain.run(question=question)\n\n    '\\nThe San Francisco 49ers won the Super Bowl in 1994.'\n\nImprove the results by fine-tuning (optional)\n\nWell - that is wrong - the San Francisco 49ers did not win. The correct answer to the question would be The Dallas Cowboys!.\n\nLet's increase the odds for the correct answer, by fine-tuning on the correct answer using the PromptTemplate.\n\ndataset = [\n    {\n        \"inputs\": template.format(question=\"What NFL team won the Super Bowl in 1994?\")\n        + \" The Dallas Cowboys!\"\n    }\n]\ndataset\n\n    [{'inputs': 'Question: What NFL team won the Super Bowl in 1994?\\n\\nAnswer:  The Dallas Cowboys!'}]\n\nnew_model.fine_tune(samples=dataset)\n\n    FineTuneResponse(number_of_trainable_tokens=27, sum_loss=78.17996)\n\n# we can keep the llm_chain, as the registered model just got refreshed on the gradient.ai servers.\nllm_chain.run(question=question)\n\n    'The Dallas Cowboys'\n\nPrevious\nGPT4All\nNext\nHugging Face Hub"
}