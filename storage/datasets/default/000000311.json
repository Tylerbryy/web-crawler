{
	"title": "TensorFlow Datasets | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/integrations/document_loaders/tensorflow_datasets",
	"html": "ComponentsDocument loadersTensorFlow Datasets\nTensorFlow Datasets\n\nTensorFlow Datasets is a collection of datasets ready to use, with TensorFlow or other Python ML frameworks, such as Jax. All datasets are exposed as tf.data.Datasets, enabling easy-to-use and high-performance input pipelines. To get started see the guide and the list of datasets.\n\nThis notebook shows how to load TensorFlow Datasets into a Document format that we can use downstream.\n\nInstallation‚Äã\n\nYou need to install tensorflow and tensorflow-datasets python packages.\n\npip install tensorflow\n\npip install tensorflow-datasets\n\nExample‚Äã\n\nAs an example, we use the mlqa/en dataset.\n\nMLQA (Multilingual Question Answering Dataset) is a benchmark dataset for evaluating multilingual question answering performance. The dataset consists of 7 languages: Arabic, German, Spanish, English, Hindi, Vietnamese, Chinese.\n\nHomepage: https://github.com/facebookresearch/MLQA\nSource code: tfds.datasets.mlqa.Builder\nDownload size: 72.21 MiB\n# Feature structure of `mlqa/en` dataset:\n\nFeaturesDict(\n    {\n        \"answers\": Sequence(\n            {\n                \"answer_start\": int32,\n                \"text\": Text(shape=(), dtype=string),\n            }\n        ),\n        \"context\": Text(shape=(), dtype=string),\n        \"id\": string,\n        \"question\": Text(shape=(), dtype=string),\n        \"title\": Text(shape=(), dtype=string),\n    }\n)\n\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\n\n# try directly access this dataset:\nds = tfds.load(\"mlqa/en\", split=\"test\")\nds = ds.take(1)  # Only take a single example\nds\n\n    <_TakeDataset element_spec={'answers': {'answer_start': TensorSpec(shape=(None,), dtype=tf.int32, name=None), 'text': TensorSpec(shape=(None,), dtype=tf.string, name=None)}, 'context': TensorSpec(shape=(), dtype=tf.string, name=None), 'id': TensorSpec(shape=(), dtype=tf.string, name=None), 'question': TensorSpec(shape=(), dtype=tf.string, name=None), 'title': TensorSpec(shape=(), dtype=tf.string, name=None)}>\n\n\nNow we have to create a custom function to convert dataset sample into a Document.\n\nThis is a requirement. There is no standard format for the TF datasets that's why we need to make a custom transformation function.\n\nLet's use context field as the Document.page_content and place other fields in the Document.metadata.\n\ndef decode_to_str(item: tf.Tensor) -> str:\n    return item.numpy().decode(\"utf-8\")\n\n\ndef mlqaen_example_to_document(example: dict) -> Document:\n    return Document(\n        page_content=decode_to_str(example[\"context\"]),\n        metadata={\n            \"id\": decode_to_str(example[\"id\"]),\n            \"title\": decode_to_str(example[\"title\"]),\n            \"question\": decode_to_str(example[\"question\"]),\n            \"answer\": decode_to_str(example[\"answers\"][\"text\"][0]),\n        },\n    )\n\n\nfor example in ds:\n    doc = mlqaen_example_to_document(example)\n    print(doc)\n    break\n\n    page_content='After completing the journey around South America, on 23 February 2006, Queen Mary 2 met her namesake, the original RMS Queen Mary, which is permanently docked at Long Beach, California. Escorted by a flotilla of smaller ships, the two Queens exchanged a \"whistle salute\" which was heard throughout the city of Long Beach. Queen Mary 2 met the other serving Cunard liners Queen Victoria and Queen Elizabeth 2 on 13 January 2008 near the Statue of Liberty in New York City harbour, with a celebratory fireworks display; Queen Elizabeth 2 and Queen Victoria made a tandem crossing of the Atlantic for the meeting. This marked the first time three Cunard Queens have been present in the same location. Cunard stated this would be the last time these three ships would ever meet, due to Queen Elizabeth 2\\'s impending retirement from service in late 2008. However this would prove not to be the case, as the three Queens met in Southampton on 22 April 2008. Queen Mary 2 rendezvoused with Queen Elizabeth 2  in Dubai on Saturday 21 March 2009, after the latter ship\\'s retirement, while both ships were berthed at Port Rashid. With the withdrawal of Queen Elizabeth 2 from Cunard\\'s fleet and its docking in Dubai, Queen Mary 2 became the only ocean liner left in active passenger service.' metadata={'id': '5116f7cccdbf614d60bcd23498274ffd7b1e4ec7', 'title': 'RMS Queen Mary 2', 'question': 'What year did Queen Mary 2 complete her journey around South America?', 'answer': '2006'}\n\n\n    2023-08-03 14:27:08.482983: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n\nfrom langchain.document_loaders import TensorflowDatasetLoader\nfrom langchain.schema import Document\n\nloader = TensorflowDatasetLoader(\n    dataset_name=\"mlqa/en\",\n    split_name=\"test\",\n    load_max_docs=3,\n    sample_to_document_function=mlqaen_example_to_document,\n)\n\n\nTensorflowDatasetLoader has these parameters:\n\ndataset_name: the name of the dataset to load\nsplit_name: the name of the split to load. Defaults to \"train\".\nload_max_docs: a limit to the number of loaded documents. Defaults to 100.\nsample_to_document_function: a function that converts a dataset sample to a Document\ndocs = loader.load()\nlen(docs)\n\n    2023-08-03 14:27:22.998964: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n\n\n\n\n\n    3\n\ndocs[0].page_content\n\n    'After completing the journey around South America, on 23 February 2006, Queen Mary 2 met her namesake, the original RMS Queen Mary, which is permanently docked at Long Beach, California. Escorted by a flotilla of smaller ships, the two Queens exchanged a \"whistle salute\" which was heard throughout the city of Long Beach. Queen Mary 2 met the other serving Cunard liners Queen Victoria and Queen Elizabeth 2 on 13 January 2008 near the Statue of Liberty in New York City harbour, with a celebratory fireworks display; Queen Elizabeth 2 and Queen Victoria made a tandem crossing of the Atlantic for the meeting. This marked the first time three Cunard Queens have been present in the same location. Cunard stated this would be the last time these three ships would ever meet, due to Queen Elizabeth 2\\'s impending retirement from service in late 2008. However this would prove not to be the case, as the three Queens met in Southampton on 22 April 2008. Queen Mary 2 rendezvoused with Queen Elizabeth 2  in Dubai on Saturday 21 March 2009, after the latter ship\\'s retirement, while both ships were berthed at Port Rashid. With the withdrawal of Queen Elizabeth 2 from Cunard\\'s fleet and its docking in Dubai, Queen Mary 2 became the only ocean liner left in active passenger service.'\n\ndocs[0].metadata\n\n    {'id': '5116f7cccdbf614d60bcd23498274ffd7b1e4ec7',\n     'title': 'RMS Queen Mary 2',\n     'question': 'What year did Queen Mary 2 complete her journey around South America?',\n     'answer': '2006'}\n\nPrevious\nTencent COS File\nNext\n2Markdown"
}