{
	"title": "Flyte | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/integrations/providers/flyte",
	"html": "ProvidersMoreFlyte\nFlyte\n\nFlyte is an open-source orchestrator that facilitates building production-grade data and ML pipelines. It is built for scalability and reproducibility, leveraging Kubernetes as its underlying platform.\n\nThe purpose of this notebook is to demonstrate the integration of a FlyteCallback into your Flyte task, enabling you to effectively monitor and track your LangChain experiments.\n\nInstallation & Setup‚Äã\nInstall the Flytekit library by running the command pip install flytekit.\nInstall the Flytekit-Envd plugin by running the command pip install flytekitplugins-envd.\nInstall LangChain by running the command pip install langchain.\nInstall Docker on your system.\nFlyte Tasks‚Äã\n\nA Flyte task serves as the foundational building block of Flyte. To execute LangChain experiments, you need to write Flyte tasks that define the specific steps and operations involved.\n\nNOTE: The getting started guide offers detailed, step-by-step instructions on installing Flyte locally and running your initial Flyte pipeline.\n\nFirst, import the necessary dependencies to support your LangChain experiments.\n\nimport os\n\nfrom flytekit import ImageSpec, task\nfrom langchain.agents import AgentType, initialize_agent, load_tools\nfrom langchain.callbacks import FlyteCallbackHandler\nfrom langchain.chains import LLMChain\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.prompts import PromptTemplate\nfrom langchain.schema import HumanMessage\n\n\nSet up the necessary environment variables to utilize the OpenAI API and Serp API:\n\n# Set OpenAI API key\nos.environ[\"OPENAI_API_KEY\"] = \"<your_openai_api_key>\"\n\n# Set Serp API key\nos.environ[\"SERPAPI_API_KEY\"] = \"<your_serp_api_key>\"\n\n\nReplace <your_openai_api_key> and <your_serp_api_key> with your respective API keys obtained from OpenAI and Serp API.\n\nTo guarantee reproducibility of your pipelines, Flyte tasks are containerized. Each Flyte task must be associated with an image, which can either be shared across the entire Flyte workflow or provided separately for each task.\n\nTo streamline the process of supplying the required dependencies for each Flyte task, you can initialize an ImageSpec object. This approach automatically triggers a Docker build, alleviating the need for users to manually create a Docker image.\n\ncustom_image = ImageSpec(\n    name=\"langchain-flyte\",\n    packages=[\n        \"langchain\",\n        \"openai\",\n        \"spacy\",\n        \"https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0.tar.gz\",\n        \"textstat\",\n        \"google-search-results\",\n    ],\n    registry=\"<your-registry>\",\n)\n\n\nYou have the flexibility to push the Docker image to a registry of your preference. Docker Hub or GitHub Container Registry (GHCR) is a convenient option to begin with.\n\nOnce you have selected a registry, you can proceed to create Flyte tasks that log the LangChain metrics to Flyte Deck.\n\nThe following examples demonstrate tasks related to OpenAI LLM, chains and agent with tools:\n\nLLM‚Äã\n@task(disable_deck=False, container_image=custom_image)\ndef langchain_llm() -> str:\n    llm = ChatOpenAI(\n        model_name=\"gpt-3.5-turbo\",\n        temperature=0.2,\n        callbacks=[FlyteCallbackHandler()],\n    )\n    return llm([HumanMessage(content=\"Tell me a joke\")]).content\n\nChain‚Äã\n@task(disable_deck=False, container_image=custom_image)\ndef langchain_chain() -> list[dict[str, str]]:\n    template = \"\"\"You are a playwright. Given the title of play, it is your job to write a synopsis for that title.\nTitle: {title}\nPlaywright: This is a synopsis for the above play:\"\"\"\n    llm = ChatOpenAI(\n        model_name=\"gpt-3.5-turbo\",\n        temperature=0,\n        callbacks=[FlyteCallbackHandler()],\n    )\n    prompt_template = PromptTemplate(input_variables=[\"title\"], template=template)\n    synopsis_chain = LLMChain(\n        llm=llm, prompt=prompt_template, callbacks=[FlyteCallbackHandler()]\n    )\n    test_prompts = [\n        {\n            \"title\": \"documentary about good video games that push the boundary of game design\"\n        },\n    ]\n    return synopsis_chain.apply(test_prompts)\n\nAgent‚Äã\n@task(disable_deck=False, container_image=custom_image)\ndef langchain_agent() -> str:\n    llm = OpenAI(\n        model_name=\"gpt-3.5-turbo\",\n        temperature=0,\n        callbacks=[FlyteCallbackHandler()],\n    )\n    tools = load_tools(\n        [\"serpapi\", \"llm-math\"], llm=llm, callbacks=[FlyteCallbackHandler()]\n    )\n    agent = initialize_agent(\n        tools,\n        llm,\n        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n        callbacks=[FlyteCallbackHandler()],\n        verbose=True,\n    )\n    return agent.run(\n        \"Who is Leonardo DiCaprio's girlfriend? Could you calculate her current age and raise it to the power of 0.43?\"\n    )\n\n\nThese tasks serve as a starting point for running your LangChain experiments within Flyte.\n\nExecute the Flyte Tasks on Kubernetes‚Äã\n\nTo execute the Flyte tasks on the configured Flyte backend, use the following command:\n\npyflyte run --image <your-image> langchain_flyte.py langchain_llm\n\n\nThis command will initiate the execution of the langchain_llm task on the Flyte backend. You can trigger the remaining two tasks in a similar manner.\n\nThe metrics will be displayed on the Flyte UI as follows:\n\nPrevious\nFireworks\nNext\nForefrontAI"
}