{
	"title": "Google Cloud Document AI | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/integrations/document_transformers/docai",
	"html": "ComponentsDocument transformersGoogle Cloud Document AI\nGoogle Cloud Document AI\n\nDocument AI is a document understanding platform from Google Cloud to transform unstructured data from documents into structured data, making it easier to understand, analyze, and consume.\n\nLearn more:\n\nDocument AI overview\nDocument AI videos and labs\nTry it!\n\nThe module contains a PDF parser based on DocAI from Google Cloud.\n\nYou need to install two libraries to use this parser:\n\n%pip install google-cloud-documentai-toolbox\n\n\nFirst, you need to set up a Google Cloud Storage (GCS) bucket and create your own Optical Character Recognition (OCR) processor as described here: https://cloud.google.com/document-ai/docs/create-processor\n\nThe GCS_OUTPUT_PATH should be a path to a folder on GCS (starting with gs://) and a PROCESSOR_NAME should look like projects/PROJECT_NUMBER/locations/LOCATION/processors/PROCESSOR_ID or projects/PROJECT_NUMBER/locations/LOCATION/processors/PROCESSOR_ID/processorVersions/PROCESSOR_VERSION_ID. You can get it either programmatically or copy from the Prediction endpoint section of the Processor details tab in the Google Cloud Console.\n\nGCS_OUTPUT_PATH = \"gs://BUCKET_NAME/FOLDER_PATH\"\nPROCESSOR_NAME = \"projects/PROJECT_NUMBER/locations/LOCATION/processors/PROCESSOR_ID\"\n\nfrom langchain.document_loaders.blob_loaders import Blob\nfrom langchain.document_loaders.parsers import DocAIParser\n\n\nNow, create a DocAIParser.\n\nparser = DocAIParser(\n    location=\"us\", processor_name=PROCESSOR_NAME, gcs_output_path=GCS_OUTPUT_PATH\n)\n\n\nFor this example, you can use an Alphabet earnings report that's uploaded to a public GCS bucket.\n\n2022Q1_alphabet_earnings_release.pdf\n\nPass the document to the lazy_parse() method to\n\nblob = Blob(\n    path=\"gs://cloud-samples-data/gen-app-builder/search/alphabet-investor-pdfs/2022Q1_alphabet_earnings_release.pdf\"\n)\n\n\nWe'll get one document per page, 11 in total:\n\ndocs = list(parser.lazy_parse(blob))\nprint(len(docs))\n\n    11\n\n\nYou can run end-to-end parsing of a blob one-by-one. If you have many documents, it might be a better approach to batch them together and maybe even detach parsing from handling the results of parsing.\n\noperations = parser.docai_parse([blob])\nprint([op.operation.name for op in operations])\n\n    ['projects/543079149601/locations/us/operations/16447136779727347991']\n\n\nYou can check whether operations are finished:\n\nparser.is_running(operations)\n\n    True\n\n\nAnd when they're finished, you can parse the results:\n\nparser.is_running(operations)\n\n    False\n\nresults = parser.get_results(operations)\nprint(results[0])\n\n    DocAIParsingResults(source_path='gs://vertex-pgt/examples/goog-exhibit-99-1-q1-2023-19.pdf', parsed_path='gs://vertex-pgt/test/run1/16447136779727347991/0')\n\n\nAnd now we can finally generate Documents from parsed results:\n\ndocs = list(parser.parse_from_results(results))\n\nprint(len(docs))\n\n    11\n\nPrevious\nBeautiful Soup\nNext\nDoctran: extract properties"
}