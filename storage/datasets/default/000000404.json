{
	"title": "Custom callback handlers | ðŸ¦œï¸ðŸ”— Langchain",
	"url": "https://python.langchain.com/docs/modules/callbacks/custom_callbacks",
	"html": "ModulesMoreCallbacksCustom callback handlers\nCustom callback handlers\n\nYou can create a custom handler to set on the object as well. In the example below, we'll implement streaming with a custom handler.\n\nfrom langchain.callbacks.base import BaseCallbackHandler\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.schema import HumanMessage\n\n\nclass MyCustomHandler(BaseCallbackHandler):\n    def on_llm_new_token(self, token: str, **kwargs) -> None:\n        print(f\"My custom handler, token: {token}\")\n\n\n# To enable streaming, we pass in `streaming=True` to the ChatModel constructor\n# Additionally, we pass in a list with our custom handler\nchat = ChatOpenAI(max_tokens=25, streaming=True, callbacks=[MyCustomHandler()])\n\nchat([HumanMessage(content=\"Tell me a joke\")])\n\n    My custom handler, token: \n    My custom handler, token: Why\n    My custom handler, token:  don\n    My custom handler, token: 't\n    My custom handler, token:  scientists\n    My custom handler, token:  trust\n    My custom handler, token:  atoms\n    My custom handler, token: ?\n    My custom handler, token:  \n    \n    \n    My custom handler, token: Because\n    My custom handler, token:  they\n    My custom handler, token:  make\n    My custom handler, token:  up\n    My custom handler, token:  everything\n    My custom handler, token: .\n    My custom handler, token: \n\n\n\n\n\n    AIMessage(content=\"Why don't scientists trust atoms? \\n\\nBecause they make up everything.\", additional_kwargs={}, example=False)\n\nPrevious\nAsync callbacks\nNext\nCallbacks for custom chains"
}