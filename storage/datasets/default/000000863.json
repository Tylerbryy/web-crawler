{
	"title": "OpenClip | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/integrations/text_embedding/open_clip",
	"html": "ComponentsText embedding modelsOpenClip\nOpenClip\n\nOpenClip is an source implementation of OpenAI's CLIP.\n\nThese multi-modal embeddings can be used to embed images or text.\n\npip install -U langchain-experimental\n\npip install pillow open_clip_torch torch matplotlib\n\n\nWe can the list of available CLIP embedding models and checkpoints:\n\nimport open_clip\n\nopen_clip.list_pretrained()\n\n\nBelow, I test a larger but more performant model based on the table (here):\n\nmodel_name = \"ViT-g-14\"\ncheckpoint = \"laion2b_s34b_b88k\"\n\n\nBut, you can also opt for a smaller, less performant model:\n\nmodel_name = \"ViT-B-32\"\ncheckpoint = \"laion2b_s34b_b79k\"\n\n\nThe model model_name,checkpoint are set in langchain_experimental.open_clip.py.\n\nFor text, use the same method embed_documents as with other embedding models.\n\nFor images, use embed_image and simply pass a list of uris for the images.\n\nimport numpy as np\nfrom langchain_experimental.open_clip import OpenCLIPEmbeddings\nfrom PIL import Image\n\n# Image URIs\nuri_dog = \"/Users/rlm/Desktop/test/dog.jpg\"\nuri_house = \"/Users/rlm/Desktop/test/house.jpg\"\n\n# Embe images or text\nclip_embd = OpenCLIPEmbeddings()\nimg_feat_dog = clip_embd.embed_image([uri_dog])\nimg_feat_house = clip_embd.embed_image([uri_house])\ntext_feat_dog = clip_embd.embed_documents([\"dog\"])\ntext_feat_house = clip_embd.embed_documents([\"house\"])\n\nSanity Check‚Äã\n\nLet's reproduce results shown in the OpenClip Colab here.\n\nimport os\nfrom collections import OrderedDict\n\nimport IPython.display\nimport matplotlib.pyplot as plt\nimport skimage\n\ndescriptions = {\n    \"page\": \"a page of text about segmentation\",\n    \"chelsea\": \"a facial photo of a tabby cat\",\n    \"astronaut\": \"a portrait of an astronaut with the American flag\",\n    \"rocket\": \"a rocket standing on a launchpad\",\n    \"motorcycle_right\": \"a red motorcycle standing in a garage\",\n    \"camera\": \"a person looking at a camera on a tripod\",\n    \"horse\": \"a black-and-white silhouette of a horse\",\n    \"coffee\": \"a cup of coffee on a saucer\",\n}\n\noriginal_images = []\nimages = []\nimage_uris = []  # List to store image URIs\ntexts = []\nplt.figure(figsize=(16, 5))\n\n# Loop to display and prepare images and assemble URIs\nfor filename in [\n    filename\n    for filename in os.listdir(skimage.data_dir)\n    if filename.endswith(\".png\") or filename.endswith(\".jpg\")\n]:\n    name = os.path.splitext(filename)[0]\n    if name not in descriptions:\n        continue\n\n    image_path = os.path.join(skimage.data_dir, filename)\n    image = Image.open(image_path).convert(\"RGB\")\n\n    plt.subplot(2, 4, len(images) + 1)\n    plt.imshow(image)\n    plt.title(f\"{filename}\\n{descriptions[name]}\")\n    plt.xticks([])\n    plt.yticks([])\n\n    original_images.append(image)\n    images.append(image)  # Origional code does preprocessing here\n    texts.append(descriptions[name])\n    image_uris.append(image_path)  # Add the image URI to the list\n\nplt.tight_layout()\n\n    \n![png](_open_clip_files/output_8_0.png)\n    \n\n# Instantiate your model\nclip_embd = OpenCLIPEmbeddings()\n\n# Embed images and text\nimg_features = clip_embd.embed_image(image_uris)\ntext_features = clip_embd.embed_documents([\"This is \" + desc for desc in texts])\n\n# Convert the list of lists to numpy arrays for matrix operations\nimg_features_np = np.array(img_features)\ntext_features_np = np.array(text_features)\n\n# Calculate similarity\nsimilarity = np.matmul(text_features_np, img_features_np.T)\n\n# Plot\ncount = len(descriptions)\nplt.figure(figsize=(20, 14))\nplt.imshow(similarity, vmin=0.1, vmax=0.3)\n# plt.colorbar()\nplt.yticks(range(count), texts, fontsize=18)\nplt.xticks([])\nfor i, image in enumerate(original_images):\n    plt.imshow(image, extent=(i - 0.5, i + 0.5, -1.6, -0.6), origin=\"lower\")\nfor x in range(similarity.shape[1]):\n    for y in range(similarity.shape[0]):\n        plt.text(x, y, f\"{similarity[y, x]:.2f}\", ha=\"center\", va=\"center\", size=12)\n\nfor side in [\"left\", \"top\", \"right\", \"bottom\"]:\n    plt.gca().spines[side].set_visible(False)\n\nplt.xlim([-0.5, count - 0.5])\nplt.ylim([count + 0.5, -2])\n\nplt.title(\"Cosine similarity between text and image features\", size=20)\n\n    Text(0.5, 1.0, 'Cosine similarity between text and image features')\n\n\n\n\n    \n![png](_open_clip_files/output_9_1.png)\n    \n\nPrevious\nOllama\nNext\nOpenAI"
}