{
	"title": "Xorbits Inference (Xinference) | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/integrations/providers/xinference",
	"html": "ProvidersMoreXorbits Inference (Xinference)\nXorbits Inference (Xinference)\n\nThis page demonstrates how to use Xinference with LangChain.\n\nXinference is a powerful and versatile library designed to serve LLMs, speech recognition models, and multimodal models, even on your laptop. With Xorbits Inference, you can effortlessly deploy and serve your or state-of-the-art built-in models using just a single command.\n\nInstallation and Setup‚Äã\n\nXinference can be installed via pip from PyPI:\n\npip install \"xinference[all]\"\n\nLLM‚Äã\n\nXinference supports various models compatible with GGML, including chatglm, baichuan, whisper, vicuna, and orca. To view the builtin models, run the command:\n\nxinference list --all\n\nWrapper for Xinference‚Äã\n\nYou can start a local instance of Xinference by running:\n\nxinference\n\n\nYou can also deploy Xinference in a distributed cluster. To do so, first start an Xinference supervisor on the server you want to run it:\n\nxinference-supervisor -H \"${supervisor_host}\"\n\n\nThen, start the Xinference workers on each of the other servers where you want to run them on:\n\nxinference-worker -e \"http://${supervisor_host}:9997\"\n\n\nYou can also start a local instance of Xinference by running:\n\nxinference\n\n\nOnce Xinference is running, an endpoint will be accessible for model management via CLI or Xinference client.\n\nFor local deployment, the endpoint will be http://localhost:9997.\n\nFor cluster deployment, the endpoint will be http://${supervisor_host}:9997.\n\nThen, you need to launch a model. You can specify the model names and other attributes including model_size_in_billions and quantization. You can use command line interface (CLI) to do it. For example,\n\nxinference launch -n orca -s 3 -q q4_0\n\n\nA model uid will be returned.\n\nExample usage:\n\nfrom langchain.llms import Xinference\n\nllm = Xinference(\n    server_url=\"http://0.0.0.0:9997\",\n    model_uid = {model_uid} # replace model_uid with the model UID return from launching the model\n)\n\nllm(\n    prompt=\"Q: where can we visit in the capital of France? A:\",\n    generate_config={\"max_tokens\": 1024, \"stream\": True},\n)\n\n\nUsage‚Äã\n\nFor more information and detailed examples, refer to the example for xinference LLMs\n\nEmbeddings‚Äã\n\nXinference also supports embedding queries and documents. See example for xinference embeddings for a more detailed demo.\n\nPrevious\nXata\nNext\nYandex"
}